# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import os
import argparse
import json
import numpy as np
from time import perf_counter as pc
from collections import OrderedDict as ODict
import sys

from layer_packing import greedy_memory_packing, balanced_memory_packing, reuse_memory_packing, balanced_time_packing, balanced_time_packing_2, manual_pack
from task_graph_composor import compose
from task_graph_composor_2 import compose2
from simulator import simulate, sim_res_str

class TopK(object):
    def __init__(self, k=1, hash_fn=None, print_fn=None):
        """ hash_fn: if None, use counter as id (can have duplicate configs); 
                     else, use hash value as id (de-duplicated configs) """
        self.k = k if k > 0 else float('inf')
        # å€™é€‰é›†å­—å…¸
        self.roster = ODict() # { id: { "time" : 1., "config" : ODict } }
        self.cnt = 0
        self.hash_fn = hash_fn
        self.print_fn = print_fn 
        if self.print_fn is None: 
            self.print_fn = lambda r, t, c: print("Top{}: time {}, config {}".format(r + 1, t, c))

    # ç»´æŠ¤ä¸€ä¸ªå€™é€‰é›†ï¼Œå³self.roster
    # è‹¥å€™é€‰é›†æœªæ»¡(å€™é€‰é›†å¤§å°ä¸ºself.k)ï¼Œç›´æ¥å°†timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†
    # å¦åˆ™ï¼Œè‹¥ç»™å®šçš„timeå°äºå€™é€‰é›†ä¸­çš„æœ€å¤§æ—¶é—´ï¼Œåˆ æ‰å€™é€‰é›†ä¸­å…·æœ‰æœ€å¤§æ—¶é—´çš„å€™é€‰è€…ï¼Œå°†å½“å‰timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†
    def add(self, time, config):
        """ rank topK based on this argument time """
        id = self.cnt if self.hash_fn is None else self.hash_fn(config)
        # è‹¥å€™é€‰é›†æœªæ»¡ï¼Œç›´æ¥å°†å½“å‰timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†ä¸­
        if self.cnt < self.k: # topK not full yet
            if id not in self.roster:
                self.roster[id] = ODict({"time":time, "config":config})
                self.cnt += 1      

        # å¦‚æœå€™é€‰é›†å·²æ»¡ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹é€»è¾‘æ¥æ›´æ–°å€™é€‰é›†
        else: # topK is full
            # find the worst one
            # è·å–å€™é€‰é›†ä¸­æ‰€æœ‰å…ƒç´ çš„keyï¼Œå³id
            topk_id = [key for key in self.roster.keys()]
            topk_time = [v["time"] for v in self.roster.values()]
            # æ‰¾åˆ°å€™é€‰é›†ä¸­æ—¶é—´æœ€å¤§çš„å…ƒç´ çš„ä¸‹æ ‡
            worst_id = topk_id[ topk_time.index(max(topk_time)) ]
            # if in topK and unique, replace the worst one
            # è‹¥å½“å‰ç»™å®šçš„timeæ¯”å€™é€‰é›†ä¸­æœ€å·®çš„å€™é€‰è€…çš„timeå°ï¼Œä¸”idä¸å­˜åœ¨äºå€™é€‰é›†ä¸­ï¼Œåˆ æ‰å€™é€‰é›†ä¸­å…·æœ‰æœ€å¤§æ—¶é—´çš„
            # å€™é€‰è€…ï¼Œå°†å½“å‰çš„timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†ä¸­
            if time < self.roster[worst_id]['time'] and (id not in self.roster):
                del self.roster[worst_id]
                self.roster[id] = ODict({"time":time, "config":config})
                self.cnt += 1
    
    # æŒ‰ç…§æ‰§è¡Œæ—¶é—´å¯¹å€™é€‰è€…è¿›è¡Œæ’åºï¼Œå¹¶æŒ‰ç…§æ’åºå»ºç«‹å€™é€‰è€…çš„config listå’Œç›¸å…³ä¿¡æ¯å­—ç¬¦ä¸²list
    # è¿”å›è¿™ä¸¤ä¸ªlist
    def summary(self, title=""):
        print("%s" % title)
        
        topk_time = [v["time"] for v in self.roster.values()]
        topk_config = [v["config"] for v in self.roster.values()]
        # ä½¿ç”¨ numpy åº“ä¸­çš„ argsort å‡½æ•°å¯¹ topk_time è¿›è¡Œæ’åºï¼Œå¹¶è¿”å›æ’åºåçš„ç´¢å¼•
        # æ¯ä¸ªå…ƒç´ ï¼Œå³ç´¢å¼•çš„ä¸‹æ ‡ä»£è¡¨äº†æ’åºåçš„ä½ç½®ï¼Œç´¢å¼•åˆ™ä»£è¡¨äº†topk_timeä¸­å¯¹åº”çš„å…ƒç´ (time)çš„ä½ç½®
        indices = np.argsort(topk_time) # returning index of a sorted list (ascending)
        # print(f"indices:{indices}")

        sorted_config, sorted_print = [], []
        for r, i in enumerate(indices):
            # æŒ‰ç…§æ’å¥½åºçš„é¡ºåºå–å‡ºtopk_timeä¸­çš„å…ƒç´ 
            time = topk_time[i]
            config = topk_config[i]
            # æ‰“å°å¹¶è¿”å›å­—ç¬¦ä¸²ï¼Œæè¿°äº†å€™é€‰è€…çš„å„ç±»ä¿¡æ¯
            s = self.print_fn(r, time, config)
            sorted_config.append(config)
            sorted_print.append(s)
        
        # è¿”å›æŒ‰æ—¶é—´ä»å°åˆ°å¤§æ’åºçš„å€™é€‰è€…çš„configï¼Œå’Œç›¸å…³ä¿¡æ¯å­—ç¬¦ä¸²
        return sorted_config, sorted_print

def hash_fn(config):
    u_fwd = int(config['CONFIGS']['u_fwd'])
    pack_fwd = config['CONFIGS']['pack_fwd']
    u_bwd = int(config['CONFIGS']['u_bwd'])
    pack_bwd = config['CONFIGS']['pack_bwd']
    # e.g., pack_fwd = []
    # e.g., pack_fwd = [[0, 1, 2, 3, 4...12, 13, 14, 15, 16, 17, 18]]
    # e.g., pack_fwd = [[0, 1, 2, 3], ..., [16, 17, 18, 19, 20]]
    pack_fwd = tuple( tuple(p) for p in pack_fwd )
    pack_bwd = tuple( tuple(p) for p in pack_bwd )
    # hash the immutables
    
    return hash((u_fwd,pack_fwd,u_bwd,pack_bwd))

def print_fn(r, time, config):
    # print_str1 = print_global(config["end2end_times"], config["end2end_memories"], title="Top%d"%(r+1))
    print_str1 = sim_res_str(config["res"], title="Top%d"%(r+1))
    print(print_str1)
    
    u_fwd = config['CONFIGS']['u_fwd']
    pack_fwd = config['CONFIGS']['pack_fwd']
    u_bwd = config['CONFIGS']['u_bwd']
    pack_bwd = config['CONFIGS']['pack_bwd']
    print_str2  = "\tu_fwd   : {}\n".format(u_fwd)
    print_str2 += "\tpack_fwd: {} =\t{}\n".format(len(pack_fwd), pack_fwd)
    print_str2 += "\t{}\n".format(config["packing_method_fwd"])
    print_str2 += "\tu_bwd   : {}\n".format(u_bwd)
    print_str2 += "\tpack_bwd: {} =\t{}\n".format(len(pack_bwd), pack_bwd)
    print_str2 += "\t{}".format(config["packing_method_bwd"])
    print(print_str2)
    
    return print_str1 + "\n" + print_str2
    # print("\tu_fwd   : {}".format(u_fwd))
    # print("\tpack_fwd: {}x =\t{}".format(len(pack_fwd), pack_fwd))
    # print("\t{}".format(config["packing_method_fwd"]))
    # print("\tu_bwd   : {}".format(u_bwd))
    # print("\tpack_bwd: {}x =\t{}".format(len(pack_bwd), pack_bwd))
    # print("\t{}".format(config["packing_method_bwd"]))  

def is_equal_ubatchsize(args, ubatchsize):
    """ for both Ufwd and Ubwd """
    D, N = args.minibatchsize, args.num_gpus
    if args.mode == 'vDP':
        is_equal = True
        for n in range(N):
            # ----- find per-GPU microbatch sizes -----
            DD = int(float(D)/N)
            if D%N != 0: # uneven batch size across GPUs
                if n < D%N:
                    DD += 1
            assert DD >= ubatchsize
            if DD % ubatchsize != 0:
                is_equal = False
                break
        return is_equal
    
    # å¯¹vPPæ¥è¯´ï¼Œåªæœ‰minibatchsizeèƒ½æ•´é™¤microbatchsizeï¼Œè¿”å›çš„æ‰æ˜¯true
    elif args.mode == 'vPP':
        assert D >= ubatchsize
        return D % ubatchsize == 0
    else:
        raise ValueError

# åƒæ˜¯ä¸€ä¸ªæ£€æŸ¥å®Œæ•´æ€§çš„å‡½æ•°ï¼Œå³
# 1.ç¡®ä¿microbatchçš„å¤§å°ä¸è¶…è¿‡minibatchçš„å¤§å°
# 2.ç­›é€‰å‡ºèƒ½è¢«minibatchsizeæ•´é™¤çš„ubatchsize
# è¿”å›ï¼šsorted(ubatchsizes_fwd), sorted(ubatchsizes_bwd)
def find_ubatchsizes(args):
    """ find valid ubatch sizes to search """
    # ubatchsize = 1 ~ min(Umax,min(DD)) for vDP both fwd and bwd
    # ubatchsize = 1 ~ min(Umax,D) for vPP both fwd and bwd
    Umin_fwd, Umax_fwd = min(args.fwd_ubatchsizes), max(args.fwd_ubatchsizes)
    Umin_bwd, Umax_bwd = min(args.bwd_ubatchsizes), max(args.bwd_ubatchsizes)
    D, N = args.minibatchsize, args.num_gpus    
    if args.mode == 'vDP':
        D = int(float(D)/N) # namely, min(DD)
    elif args.mode == 'vPP':
        D = D
    else:
        raise ValueError
    # æœ€å¤§çš„ubatchsizeä¸ºï¼Œfwd_ubatchsizesçš„æœ€å¤§å€¼ï¼Œå’Œminibatchsizeä¹‹é—´çš„æœ€å°å€¼ã€‚å³microbatchçš„å¤§å°ä¸å…è®¸
    # è¶…è¿‡minibatchçš„å¤§å°
    ubatchsizes_fwd = list(range(Umin_fwd, min(Umax_fwd, D)+1, args.ubatchsize_step))
    ubatchsizes_bwd = list(range(Umin_bwd, min(Umax_bwd, D)+1, args.ubatchsize_step))
    
    # then select equal ubatchsize if needed (DD/D % ubatchsize == 0)
    # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œè¿™é‡Œæ‰§è¡Œ: ç­›é€‰å‡ºèƒ½è¢«minibatchsizeæ•´é™¤çš„ubatchsize
    if not args.inequal_ubatchsize:
        #                                                1.assert D >= ubatchsizeï¼Œç¡®ä¿ubatchsizeä¸è¶…è¿‡minibatchsize
        #                                                2.å¯¹vPPæ¥è¯´ï¼Œåªæœ‰minibatchsizeèƒ½æ•´é™¤uï¼Œè¿”å›çš„æ‰æ˜¯true
        #                                                  ğŸ“Œå› æ­¤å¾—åˆ°çš„microbatchsizeä¸€å®šæ˜¯å’Œminibatchsizeçš„æ¬¡å¹‚è¡¨è¾¾å¼çš„åº•æ•°ç›¸åŒ
        ubatchsizes_fwd = [u for u in ubatchsizes_fwd if is_equal_ubatchsize(args, u)]
        ubatchsizes_bwd = [u for u in ubatchsizes_bwd if is_equal_ubatchsize(args, u)]
    else:
        print("[WARNING] allow inequal microbatchsize will disable double buffering. Although we can still search, runtime is not supported currently.")
    
    return sorted(ubatchsizes_fwd), sorted(ubatchsizes_bwd)

# æ ¹æ®å‚æ•°æŒ‡å®šçš„æ–¹å¼ï¼Œå¯¹æ‰€æœ‰å±‚è¿›è¡Œæ‰“åŒ…
# blanced_timeï¼šæŒ‰ç…§æ¯ä¸ªpackæ‰§è¡Œæ—¶é—´å°½å¯èƒ½ç›¸ç­‰çš„åŸåˆ™è¿›è¡Œæ‰“åŒ…ã€‚æ­¤å¤–ï¼Œéœ€è¦æ³¨æ„éšå«çš„å¦ä¸€ä¸ªåŸåˆ™ï¼Œå³æ‰“åŒ…æ•°ä¼šå°½å¯èƒ½çš„å°‘ã€‚
# åœ¨é¦–ä¸ªç¬¦åˆGPUå®¹é‡çš„æ‰“åŒ…å®Œæˆåï¼Œä¸ä¼šå†è¿›è¡Œåç»­çš„æ‰“åŒ…
# è¿”å›ä¸€ä¸ªå­—å…¸ï¼š{ "balanced_time"(æ‰“åŒ…æ–¹æ³•)ï¼š[[0,1,2], [3,4,5]]ï¼ˆlayer_packsï¼‰ }
def find_layer_packs(args, ubatchsize, num_layers, type, reuse_packs=None, verbose=True):
    """ for an ubatchsize, find valid layer packs to search """
    assert type in ["FWD","BWD"]
    
    # build per_layer_memories list
    memory_list = []; x_list = []
    # ä¸ºæ¯ä¸€å±‚layerï¼Œåˆ†åˆ«ä¸ºä¸¤ä¸ªåˆ—è¡¨æ·»åŠ ä¸€ä¸ªå€¼ï¼Œåˆ†åˆ«ä¸ºå±‚çš„å ç”¨å¤§å°ï¼Œå’Œè¯¥å±‚è¾“å…¥çš„å¤§å°
    for l in range(num_layers):
        mem  = args.prof['MEMORY_FWDBWD'].get(type, ubatchsize, l, interp=True) # int
        xmem = args.prof["XMETA"].get_bytes(ubatchsize, l, interp=True)# int
        # MEMORY_FWDBWDæœ¬èº«å°±åŒ…å«äº†è¿™ä¸€å±‚çš„è¾“å…¥å’Œè¾“å‡ºï¼Œå‡xmemå³å‡å»è¿™ä¸€å±‚è¾“å…¥çš„å¤§å°
        # ğŸ“Œæ¯å±‚çš„ç©ºé—´å ç”¨ä¸ºï¼Œå‚æ•°+è¾“å‡º
        memory_list.append(mem - xmem) # bytes 
        # æ¯å±‚çš„è¾“å…¥å¤§å°
        x_list.append(xmem) # bytes
    
    # different packing methods
    layer_packs = ODict() # { "greedy" : [[0,1,2], [3,4,5]] }
    packing_method = args.packing_method_fwd if type == 'FWD' else \
                     args.packing_method_bwd
    print(f"packing_method:{packing_method}")

    tab = "\t\t\t" if type == 'FWD' else "\t"
    # æŒ‰ç…§ç»™å®šçš„å‚æ•°ï¼Œæ‰§è¡Œå¯¹åº”çš„packingç­–ç•¥
    for method in packing_method:
        if method == "greedy":
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, verbose=verbose, tab=tab)
        elif method == "greedy_addx":
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, per_layer_x=x_list, verbose=verbose, title="greedy_memory_packing (addx)", tab=tab)
        elif method == "greedy_reverse":
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, reverse=True, verbose=verbose,title="greedy_memory_packing (reverse)", tab=tab)
        elif method == "greedy_reverse_addx":
            # ä½¿ç”¨è´ªå©ªæ–¹æ³•æ‰“åŒ…å±‚ï¼Œå³åªè¦å½“å‰packä¸­æ‰€æœ‰çš„å±‚+é¦–å±‚çš„è¾“å…¥ä¸è¶…è¿‡GPUçš„å®¹é‡ï¼Œå°±ä¸€ç›´å¾€packä¸­æ·»åŠ æ–°çš„layerã€‚å¦åˆ™ï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªpack
            # å…¶å†…éƒ¨ä¼šé€†åºçš„è¿›è¡Œæ‰“åŒ…ï¼Œä½†æœ€ç»ˆåœ¨è¿”å›å‰ä¼šå¯¹æ¯ä¸ªlist(layer pack)è¿›è¡Œç¿»è½¬
            # è¿”å›ï¼šlayer_packï¼ˆlistï¼‰ï¼Œå³è¯¥listä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªlistï¼Œè¡¨ç¤ºä¸€ä¸ªlayer packã€‚layer_packä¸­çš„å…ƒç´ ä¸ºlayerçš„åºå·
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, reverse=True, per_layer_x=x_list, verbose=verbose,title="greedy_memory_packing (reverse,addx)", tab=tab)
        elif method == "balanced":
            layer_packs[method] = balanced_memory_packing(memory_list, args.memory_cap, verbose=verbose, tab=tab)
        elif method == "balanced_addx":
            layer_packs[method] = balanced_memory_packing(memory_list, args.memory_cap, per_layer_x=x_list, verbose=verbose, title="balanced_memory_packing (addx)", tab=tab)
        elif method == "reuse":
            # å¯¹packsä¸­çš„æ‰€æœ‰packè®¡ç®—ï¼Œè¯¥packæ‰€æœ‰å±‚çš„å¤§å°+è¯¥packé¦–å±‚çš„è¾“å…¥å¤§å°ã€‚é€‰å‡ºå…¶ä¸­çš„æœ€å¤§å€¼
            # è‹¥æœ€å¤§å€¼ < GPUå®¹é‡ï¼Œè¿”å›è¯¥packs
            # è‹¥æœ€å¤§çš„pack > GPUå®¹é‡ï¼Œè¿”å›Noneï¼Œè¡¨ç¤ºä¸èƒ½reuse packing
            layer_packs[method] = reuse_memory_packing(reuse_packs, memory_list, x_list, args.memory_cap, verbose=verbose, tab=tab)
        elif method == "balanced_time":
            # æ‰€æœ‰å±‚BWDçš„æ‰§è¡Œæ—¶é—´çš„list
            time_list = [ args.prof['TIME_FWDBWD'].get(type, ubatchsize, l, "GPU",interp=True) for l in range(num_layers) ] # sec (float)
            # æŒ‰ç…§æ¯ä¸ªpackæ‰§è¡Œæ—¶é—´å°½å¯èƒ½ç›¸ç­‰çš„åŸåˆ™è¿›è¡Œæ‰“åŒ…ã€‚æ­¤å¤–ï¼Œéœ€è¦æ³¨æ„éšå«çš„å¦ä¸€ä¸ªåŸåˆ™ï¼Œå³æ‰“åŒ…æ•°ä¼šå°½å¯èƒ½çš„å°‘ã€‚
            # åœ¨é¦–ä¸ªç¬¦åˆGPUå®¹é‡çš„æ‰“åŒ…å®Œæˆåï¼Œä¸ä¼šå†è¿›è¡Œåç»­çš„æ‰“åŒ…
            # å³ä»å°†æ•´ä¸ªmodelåˆ‡æˆä¸¤ä»½å¼€å§‹ï¼Œæ£€æµ‹æŒ‰æ—¶é—´å‡åŒ€æ–¹æ³•åˆ‡å‰²çš„layer packæœ‰æ²¡æœ‰è¶…è¿‡GPUçš„å®¹é‡ã€‚æ²¡è¶…è¿‡ç›´æ¥è¿”å›ï¼Œè¶…è¿‡äº†ç»§ç»­æµ‹è¯•åˆ‡æˆ
            # 3ä»½ï¼Œä»¥æ­¤ç±»æ¨
            # è¿”å›layer_packsï¼Œè¯¥listä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªlistï¼Œè¡¨ç¤ºä¸€ä¸ªlayer packã€‚layer_packä¸­çš„å…ƒç´ ä¸ºlayerçš„åºå·
            layer_packs[method] = balanced_time_packing(time_list, memory_list, x_list, args.memory_cap, verbose=verbose, tab=tab)
        # skip invalid packing
        # åˆ æ‰å­—å…¸ä¸­æ²¡æœ‰å€¼çš„é”®å€¼å¯¹
        if layer_packs[method] is None:
            del layer_packs[method]
    
    # { "balanced_time"(æ‰“åŒ…æ–¹æ³•)ï¼š[[0,1,2], [3,4,5]]ï¼ˆlayer_packsï¼‰ }
    return layer_packs

# å·²åºŸå¼ƒ
def find_layer_packs_2(args, ubatchsize, num_layers, type, reuse_packs=None, verbose=True):
    """ for an ubatchsize, find valid layer packs to search """
    assert type in ["FWD","BWD"]
    
    # build per_layer_memories list
    memory_list = []; x_list = []
    # ä¸ºæ¯ä¸€å±‚layerï¼Œåˆ†åˆ«ä¸ºä¸¤ä¸ªåˆ—è¡¨æ·»åŠ ä¸€ä¸ªå€¼ï¼Œåˆ†åˆ«ä¸ºå±‚çš„å ç”¨å¤§å°ï¼Œå’Œè¯¥å±‚è¾“å…¥çš„å¤§å°
    for l in range(num_layers):
        mem  = args.prof['MEMORY_FWDBWD'].get(type, ubatchsize, l, interp=True) # int
        xmem = args.prof["XMETA"].get_bytes(ubatchsize, l, interp=True)# int
        # MEMORY_FWDBWDæœ¬èº«å°±åŒ…å«äº†è¿™ä¸€å±‚çš„è¾“å…¥å’Œè¾“å‡ºï¼Œå‡xmemå³å‡å»è¿™ä¸€å±‚è¾“å…¥çš„å¤§å°
        # ğŸ“Œæ¯å±‚çš„ç©ºé—´å ç”¨ä¸ºï¼Œå‚æ•°+è¾“å‡º
        memory_list.append(mem - xmem) # bytes 
        # æ¯å±‚çš„è¾“å…¥å¤§å°
        x_list.append(xmem) # bytes
    
    # different packing methods
    layer_packs = ODict() # { "greedy" : [[0,1,2], [3,4,5]] }
    packing_method = args.packing_method_fwd if type == 'FWD' else \
                     args.packing_method_bwd
    print(f"packing_method:{packing_method}")

    tab = "\t\t\t" if type == 'FWD' else "\t"
    # æŒ‰ç…§ç»™å®šçš„å‚æ•°ï¼Œæ‰§è¡Œå¯¹åº”çš„packingç­–ç•¥
    for method in packing_method:
        if method == "greedy":
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, verbose=verbose, tab=tab)
        elif method == "greedy_addx":
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, per_layer_x=x_list, verbose=verbose, title="greedy_memory_packing (addx)", tab=tab)
        elif method == "greedy_reverse":
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, reverse=True, verbose=verbose,title="greedy_memory_packing (reverse)", tab=tab)
        elif method == "greedy_reverse_addx":
            # ä½¿ç”¨è´ªå©ªæ–¹æ³•æ‰“åŒ…å±‚ï¼Œå³åªè¦å½“å‰packä¸­æ‰€æœ‰çš„å±‚+é¦–å±‚çš„è¾“å…¥ä¸è¶…è¿‡GPUçš„å®¹é‡ï¼Œå°±ä¸€ç›´å¾€packä¸­æ·»åŠ æ–°çš„layerã€‚å¦åˆ™ï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªpack
            # å…¶å†…éƒ¨ä¼šé€†åºçš„è¿›è¡Œæ‰“åŒ…ï¼Œä½†æœ€ç»ˆåœ¨è¿”å›å‰ä¼šå¯¹æ¯ä¸ªlist(layer pack)è¿›è¡Œç¿»è½¬
            # è¿”å›ï¼šlayer_packï¼ˆlistï¼‰ï¼Œå³è¯¥listä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªlistï¼Œè¡¨ç¤ºä¸€ä¸ªlayer packã€‚layer_packä¸­çš„å…ƒç´ ä¸ºlayerçš„åºå·
            layer_packs[method] = greedy_memory_packing(memory_list, args.memory_cap, reverse=True, per_layer_x=x_list, verbose=verbose,title="greedy_memory_packing (reverse,addx)", tab=tab)
        elif method == "balanced":
            layer_packs[method] = balanced_memory_packing(memory_list, args.memory_cap, verbose=verbose, tab=tab)
        elif method == "balanced_addx":
            layer_packs[method] = balanced_memory_packing(memory_list, args.memory_cap, per_layer_x=x_list, verbose=verbose, title="balanced_memory_packing (addx)", tab=tab)
        elif method == "reuse":
            # å¯¹packsä¸­çš„æ‰€æœ‰packè®¡ç®—ï¼Œè¯¥packæ‰€æœ‰å±‚çš„å¤§å°+è¯¥packé¦–å±‚çš„è¾“å…¥å¤§å°ã€‚é€‰å‡ºå…¶ä¸­çš„æœ€å¤§å€¼
            # è‹¥æœ€å¤§å€¼ < GPUå®¹é‡ï¼Œè¿”å›è¯¥packs
            # è‹¥æœ€å¤§çš„pack > GPUå®¹é‡ï¼Œè¿”å›Noneï¼Œè¡¨ç¤ºä¸èƒ½reuse packing
            layer_packs[method] = reuse_memory_packing(reuse_packs, memory_list, x_list, args.memory_cap, verbose=verbose, tab=tab)
        elif method == "balanced_time":
            # æ‰€æœ‰å±‚BWDçš„æ‰§è¡Œæ—¶é—´çš„list
            time_list = [ args.prof['TIME_FWDBWD'].get(type, ubatchsize, l, "GPU",interp=True) for l in range(num_layers) ] # sec (float)
            # æŒ‰ç…§æ¯ä¸ªpackæ‰§è¡Œæ—¶é—´å°½å¯èƒ½ç›¸ç­‰çš„åŸåˆ™è¿›è¡Œæ‰“åŒ…ã€‚æ­¤å¤–ï¼Œéœ€è¦æ³¨æ„éšå«çš„å¦ä¸€ä¸ªåŸåˆ™ï¼Œå³æ‰“åŒ…æ•°ä¼šå°½å¯èƒ½çš„å°‘ã€‚
            # åœ¨é¦–ä¸ªç¬¦åˆGPUå®¹é‡çš„æ‰“åŒ…å®Œæˆåï¼Œä¸ä¼šå†è¿›è¡Œåç»­çš„æ‰“åŒ…
            # å³ä»å°†æ•´ä¸ªmodelåˆ‡æˆä¸¤ä»½å¼€å§‹ï¼Œæ£€æµ‹æŒ‰æ—¶é—´å‡åŒ€æ–¹æ³•åˆ‡å‰²çš„layer packæœ‰æ²¡æœ‰è¶…è¿‡GPUçš„å®¹é‡ã€‚æ²¡è¶…è¿‡ç›´æ¥è¿”å›ï¼Œè¶…è¿‡äº†ç»§ç»­æµ‹è¯•åˆ‡æˆ
            # 3ä»½ï¼Œä»¥æ­¤ç±»æ¨
            # è¿”å›layer_packsï¼Œè¯¥listä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªlistï¼Œè¡¨ç¤ºä¸€ä¸ªlayer packã€‚layer_packä¸­çš„å…ƒç´ ä¸ºlayerçš„åºå·
            layer_packs[method] = balanced_time_packing_2(time_list, memory_list, x_list, args.memory_cap, args.num_gpus, verbose=verbose, tab=tab)
        # skip invalid packing
        # åˆ æ‰å­—å…¸ä¸­æ²¡æœ‰å€¼çš„é”®å€¼å¯¹
        if layer_packs[method] is None:
            del layer_packs[method]
    
    # { "balanced_time"(æ‰“åŒ…æ–¹æ³•)ï¼š[[0,1,2], [3,4,5]]ï¼ˆlayer_packsï¼‰ }
    return layer_packs

# 
def search(args):
    """ top-level function """
    """ search for the best configuration (Ufwd, Pfwd, Ubwd, Pbwd) for min estimated runtime under memory capacity constraints. """
    
    ### find microbatch sizes to search
    # åƒæ˜¯ä¸€ä¸ªæ£€æŸ¥å®Œæ•´æ€§çš„å‡½æ•°ï¼Œå³å¯¹FWDçš„microbatchsizeå’ŒBWDçš„microbatchsizeåˆ—è¡¨ä¸­çš„æ¯ä¸ªmicrobatchsizeæ‰§è¡Œï¼š
    # 1.ç¡®ä¿microbatchçš„å¤§å°ä¸è¶…è¿‡minibatchçš„å¤§å°
    # 2.ğŸ“Œç­›é€‰å‡ºèƒ½è¢«minibatchsizeæ•´é™¤çš„ubatchsize (æ‰€ä»¥è¯´ä¸æ˜¯æ¯ä¸ªubatchsizeéƒ½èƒ½è¢«ç”¨)
    # è¿”å›ï¼šsorted(ubatchsizes_fwd), sorted(ubatchsizes_bwd)
    ubatchsizes_fwd, ubatchsizes_bwd = find_ubatchsizes(args)
    if args.verbose: 
        print("searchable ubatchsizes_fwd: {}".format(ubatchsizes_fwd)) 
        print("searchable ubatchsizes_bwd: {}".format(ubatchsizes_bwd)) 
    ubatchsizes_fwd = np.array(ubatchsizes_fwd, dtype=np.uint64)
    
    ### find valid ubatch size and layer packs
    valid_size_pack = [] # [(u_fwd, pack_fwd, u_bwd, pack_bwd, method_fwd, method_bwd)]
    t_start = pc()
    ## search BWD first
    # 
    for u_bwd in ubatchsizes_bwd: 
        if args.verbose: print("\nFor u_bwd: %d, find pack_bwd..."%u_bwd)

        # æ ¹æ®å‚æ•°æŒ‡å®šçš„æ–¹å¼ï¼Œå¯¹æ‰€æœ‰å±‚è¿›è¡Œæ‰“åŒ…
        # blanced_timeï¼šæŒ‰ç…§æ¯ä¸ªpackæ‰§è¡Œæ—¶é—´å°½å¯èƒ½ç›¸ç­‰çš„åŸåˆ™è¿›è¡Œæ‰“åŒ…ã€‚æ­¤å¤–ï¼Œéœ€è¦æ³¨æ„éšå«çš„å¦ä¸€ä¸ªåŸåˆ™ï¼Œå³æ‰“åŒ…æ•°ä¼šå°½å¯èƒ½çš„å°‘ã€‚
        # åœ¨é¦–ä¸ªç¬¦åˆGPUå®¹é‡çš„æ‰“åŒ…å®Œæˆåï¼Œä¸ä¼šå†è¿›è¡Œåç»­çš„æ‰“åŒ…
        # è¿”å›ä¸€ä¸ªå­—å…¸ï¼š{ "balanced_time"(æ‰“åŒ…æ–¹æ³•)ï¼š[[0,1,2], [3,4,5]]ï¼ˆlayer_packsï¼‰ }
        method_packs_bwd = find_layer_packs(args, u_bwd, args.num_layers, "BWD", verbose=args.verbose)
        # under each BWD packing, search FWD
        for method_bwd, pack_bwd in method_packs_bwd.items():
            if args.verbose: print("\tFor %s, find FWD..."%method_bwd)
            assert len(pack_bwd) > 0
            # è‹¥åªæ‰“äº†ä¸€ä¸ªåŒ…ï¼Œåˆ™ä»£è¡¨è¯¥ä»»åŠ¡å°±æ˜¯ä¸€ä¸ªå•ä¸ªBWDä»»åŠ¡ï¼Œä¸ç”¨æ‰¾FWDäº†
            if len(pack_bwd) == 1: # Empty (single BWD pack)
                if args.verbose: print("\t\tFWD is empty")
                u_fwd = u_bwd
                pack_fwd = []
                valid_size_pack.append((u_fwd, pack_fwd, u_bwd, pack_bwd, "", method_bwd))

            # 
            else: # search FWD
                # å–pack_bwdä¸­å€’æ•°ç¬¬äºŒä¸ªpackçš„æœ€åä¸€ä¸ªlayer_idï¼Œå¹¶+1
                # å³é™¤æœ€åä¸€ä¸ªpackå¤–ï¼Œå‰é¢çš„packæ€»å…±æœ‰å¤šå°‘å±‚
                num_layers_fwd = pack_bwd[:-1][-1][-1] + 1
                print(f"\tå»é™¤BWDçš„æœ€åä¸€ä¸ªpack, å‰é¢çš„packæ€»å…±æœ‰å¤šå°‘å±‚:{num_layers_fwd}")

                # assert num_layers_fwd == sum(len(p) for p in pack_bwd[:-1])
                # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œä¸æ‰§è¡Œ
                if args.smaller_ufwd: # allow u_fwd < u_bwd
                    idx = 0
                    print("[WARNING] allow microbatch size of forward be smaller than backward is still bleeding.")
                else: # make u_fwd starts from u_bwd
                    # æ‰¾åˆ° u_bwd åº”è¯¥åœ¨ ubatchsizes_fwd ä¸­æ’å…¥çš„ä½ç½®ï¼Œå³fwdä¸­å¯¹åº”u_bwdçš„ä½ç½®
                    idx = np.searchsorted(ubatchsizes_fwd,u_bwd,side='left') # works for non-valid u_fwd, valid u_fwd, even out of range u_fwd
                    print(idx)

                # ä»fwd micro batch sizeçš„èµ·å§‹ä½ç½®å¼€å§‹éå†ï¼ˆèµ·å§‹ä½ç½®å°±æ˜¯ä»å½“å‰u_bwdçš„å¤§å°å¼€å§‹ï¼‰
                for u_fwd in ubatchsizes_fwd[idx:]: 
                    u_fwd = int(u_fwd)
                    if args.verbose: print("\t\tFor u_fwd: %d, find pack_fwd..."%u_fwd)
                    # è¿”å›ä¸€ä¸ªå­—å…¸ï¼š{ "balanced_time"(æ‰“åŒ…æ–¹æ³•)ï¼š[[0,1,2], [3,4,5]]ï¼ˆlayer_packsï¼‰ï¼Œ"reuse"ï¼špack_bwd[:-1] }
                    method_packs_fwd = find_layer_packs(args, u_fwd, num_layers_fwd, "FWD", reuse_packs=pack_bwd[:-1], verbose=args.verbose)
                    print(f"method_packs_fwd:{method_packs_fwd}")
                    for method_fwd, pack_fwd in method_packs_fwd.items():
                        valid_size_pack.append((u_fwd, pack_fwd, u_bwd, pack_bwd, method_fwd, method_bwd))
    
    ### evaluate each valid one
    print("\nEvalute valid size and packs (%d points) ..."%len(valid_size_pack))
    if args.verbose: 
        print("< u_fwd, num_pack_fwd, u_bwd, num_pack_bwd, packing_method_fwd, packing_method_bwd >")
    # 
    top_k = TopK(args.topk, hash_fn if args.dedup else None, print_fn)
    # 
    for u_fwd, pack_fwd, u_bwd, pack_bwd, method_fwd, method_bwd in valid_size_pack:
        ## compose task graph
        if args.mode == 'vPP' and args.num_gpus > 1 and u_fwd != u_bwd:
            args.last_fwd_msg = True
        else:
            args.last_fwd_msg = False
        
        CONFIGS, TASKS, rTASKS = compose(args, u_fwd, pack_fwd, u_bwd, pack_bwd, verbose=True)
        ## estimate runtime 
        res = simulate(args, rTASKS, CONFIGS, TASKS=TASKS, prefetch_offload=args.prefetch_offload, verbose=False, view=False)
        if args.verbose: 
            print(sim_res_str(res, 
                              title="< %d, %d, %d, %d, %s, %s >"%
                              (u_fwd, len(pack_fwd), u_bwd, len(pack_bwd), 
                               method_fwd[:6].ljust(6,' '), 
                               method_bwd[:6].ljust(6,' ') ) ) )
        ## compare for the best
        global_time = res['global_endtime']
        # è‹¥åå‘çš„packåªæœ‰ä¸€ä¸ªï¼Œè¯´æ˜æ•´ä¸ªmodelèƒ½æ”¾è¿›ä¸€ä¸ªGPUä¸­ï¼Œæ‰“å°ä¸€æ¡è­¦å‘Šï¼Œå»ºè®®ä¸ä½¿ç”¨Harmonyæ–¹æ³•
        if len(pack_bwd) == 1:
            # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œæ‰§è¡Œ
            if not args.rank_fit_normally:
                print("[WARNING] The entire model can fit onto a single GPU. Not recommend to use Harmony. The fit config is now put as Top1.")
                global_time /= 1000.
            else:
                print("[WARNING] The entire model can fit onto a single GPU. Not recommend to use Harmony. The fit config is still ranked normally.")
        ## add to top k best
        # ç»´æŠ¤ä¸€ä¸ªå€™é€‰é›†ï¼Œå³self.roster
        # è‹¥å€™é€‰é›†æœªæ»¡(å€™é€‰é›†å¤§å°ä¸ºself.k)ï¼Œç›´æ¥å°†timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†
        # å¦åˆ™ï¼Œè‹¥ç»™å®šçš„timeå°äºå€™é€‰é›†ä¸­çš„æœ€å¤§æ—¶é—´ï¼Œåˆ æ‰å€™é€‰é›†ä¸­å…·æœ‰æœ€å¤§æ—¶é—´çš„å€™é€‰è€…ï¼Œå°†å½“å‰timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†
        top_k.add(  global_time, 
                    ODict({ 'res': res, 
                            'CONFIGS': CONFIGS,
                            'rTASKS': rTASKS,
                            'packing_method_fwd': method_fwd,
                            'packing_method_bwd': method_bwd })) 
    
    t_end = pc()
    print("\n--- Search done: %.3f sec ---"%(t_end-t_start))
    
    return top_k

def search2(args):
    """ top-level function """
    """ search for the best configuration (Ufwd, Pfwd, Ubwd, Pbwd) for min estimated runtime under memory capacity constraints. """
    
    ### find microbatch sizes to search
    # åƒæ˜¯ä¸€ä¸ªæ£€æŸ¥å®Œæ•´æ€§çš„å‡½æ•°ï¼Œå³å¯¹FWDçš„microbatchsizeå’ŒBWDçš„microbatchsizeåˆ—è¡¨ä¸­çš„æ¯ä¸ªmicrobatchsizeæ‰§è¡Œï¼š
    # 1.ç¡®ä¿microbatchçš„å¤§å°ä¸è¶…è¿‡minibatchçš„å¤§å°
    # 2.ğŸ“Œç­›é€‰å‡ºèƒ½è¢«minibatchsizeæ•´é™¤çš„ubatchsize (æ‰€ä»¥è¯´ä¸æ˜¯æ¯ä¸ªubatchsizeéƒ½èƒ½è¢«ç”¨)
    # è¿”å›ï¼šsorted(ubatchsizes_fwd), sorted(ubatchsizes_bwd)
    ubatchsizes_fwd, ubatchsizes_bwd = find_ubatchsizes(args)
    if args.verbose: 
        print("searchable ubatchsizes_fwd: {}".format(ubatchsizes_fwd)) 
        print("searchable ubatchsizes_bwd: {}".format(ubatchsizes_bwd)) 
    ubatchsizes_fwd = np.array(ubatchsizes_fwd, dtype=np.uint64)
    
    ### find valid ubatch size and layer packs
    valid_size_pack = [] # [(u_fwd, pack_fwd, u_bwd, pack_bwd, method_fwd, method_bwd)]
    t_start = pc()
    ## search BWD first
    # 
    for u_bwd in ubatchsizes_bwd: 
        if args.verbose: print("\nFor u_bwd: %d, find pack_bwd..."%u_bwd)

        # æ ¹æ®å‚æ•°æŒ‡å®šçš„æ–¹å¼ï¼Œå¯¹æ‰€æœ‰å±‚è¿›è¡Œæ‰“åŒ…
        # blanced_timeï¼šæŒ‰ç…§æ¯ä¸ªpackæ‰§è¡Œæ—¶é—´å°½å¯èƒ½ç›¸ç­‰çš„åŸåˆ™è¿›è¡Œæ‰“åŒ…ã€‚æ­¤å¤–ï¼Œéœ€è¦æ³¨æ„éšå«çš„å¦ä¸€ä¸ªåŸåˆ™ï¼Œå³æ‰“åŒ…æ•°ä¼šå°½å¯èƒ½çš„å°‘ã€‚
        # åœ¨é¦–ä¸ªç¬¦åˆGPUå®¹é‡çš„æ‰“åŒ…å®Œæˆåï¼Œä¸ä¼šå†è¿›è¡Œåç»­çš„æ‰“åŒ…
        # è¿”å›ä¸€ä¸ªå­—å…¸ï¼š{ "balanced_time"(æ‰“åŒ…æ–¹æ³•)ï¼š[[0,1,2], [3,4,5]]ï¼ˆlayer_packsï¼‰ }
        method_packs_bwd = find_layer_packs(args, u_bwd, args.num_layers, "BWD", verbose=args.verbose)
        # under each BWD packing, search FWD
        for method_bwd, pack_bwd in method_packs_bwd.items():
            if args.verbose: print("\tFor %s, find FWD..."%method_bwd)
            assert len(pack_bwd) > 0
            # è‹¥åªæ‰“äº†ä¸€ä¸ªåŒ…ï¼Œåˆ™ä»£è¡¨è¯¥ä»»åŠ¡å°±æ˜¯ä¸€ä¸ªå•ä¸ªBWDä»»åŠ¡ï¼Œä¸ç”¨æ‰¾FWDäº†
            if len(pack_bwd) == 1: # Empty (single BWD pack)
                if args.verbose: print("\t\tFWD is empty")
                u_fwd = u_bwd
                pack_fwd = []
                valid_size_pack.append((u_fwd, pack_fwd, u_bwd, pack_bwd, "", method_bwd))

            # 
            else: # search FWD
                # å–pack_bwdä¸­å€’æ•°ç¬¬äºŒä¸ªpackçš„æœ€åä¸€ä¸ªlayer_idï¼Œå¹¶+1
                # å³é™¤æœ€åä¸€ä¸ªpackå¤–ï¼Œå‰é¢çš„packæ€»å…±æœ‰å¤šå°‘å±‚
                num_layers_fwd = pack_bwd[:-1][-1][-1] + 1
                print(f"\tå»é™¤BWDçš„æœ€åä¸€ä¸ªpack, å‰é¢çš„packæ€»å…±æœ‰å¤šå°‘å±‚:{num_layers_fwd}")

                # assert num_layers_fwd == sum(len(p) for p in pack_bwd[:-1])
                # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œä¸æ‰§è¡Œ
                if args.smaller_ufwd: # allow u_fwd < u_bwd
                    idx = 0
                    print("[WARNING] allow microbatch size of forward be smaller than backward is still bleeding.")
                else: # make u_fwd starts from u_bwd
                    # æ‰¾åˆ° u_bwd åº”è¯¥åœ¨ ubatchsizes_fwd ä¸­æ’å…¥çš„ä½ç½®ï¼Œå³fwdä¸­å¯¹åº”u_bwdçš„ä½ç½®
                    idx = np.searchsorted(ubatchsizes_fwd,u_bwd,side='left') # works for non-valid u_fwd, valid u_fwd, even out of range u_fwd
                    print(idx)

                # ä»fwd micro batch sizeçš„èµ·å§‹ä½ç½®å¼€å§‹éå†ï¼ˆèµ·å§‹ä½ç½®å°±æ˜¯ä»å½“å‰u_bwdçš„å¤§å°å¼€å§‹ï¼‰
                for u_fwd in ubatchsizes_fwd[idx:]: 
                    u_fwd = int(u_fwd)
                    if args.verbose: print("\t\tFor u_fwd: %d, find pack_fwd..."%u_fwd)
                    # è¿”å›ä¸€ä¸ªå­—å…¸ï¼š{ "balanced_time"(æ‰“åŒ…æ–¹æ³•)ï¼š[[0,1,2], [3,4,5]]ï¼ˆlayer_packsï¼‰ï¼Œ"reuse"ï¼špack_bwd[:-1] }
                    method_packs_fwd = find_layer_packs(args, u_fwd, num_layers_fwd, "FWD", reuse_packs=pack_bwd[:-1], verbose=args.verbose)
                    print(f"method_packs_fwd:{method_packs_fwd}")
                    for method_fwd, pack_fwd in method_packs_fwd.items():
                        valid_size_pack.append((u_fwd, pack_fwd, u_bwd, pack_bwd, method_fwd, method_bwd))
    
    ### evaluate each valid one
    print("\nEvalute valid size and packs (%d points) ..."%len(valid_size_pack))
    if args.verbose: 
        print("< u_fwd, num_pack_fwd, u_bwd, num_pack_bwd, packing_method_fwd, packing_method_bwd >")
    # 
    top_k = TopK(args.topk, hash_fn if args.dedup else None, print_fn)
    # 
    for u_fwd, pack_fwd, u_bwd, pack_bwd, method_fwd, method_bwd in valid_size_pack:
        ## compose task graph
        if args.mode == 'vPP' and args.num_gpus > 1 and u_fwd != u_bwd:
            args.last_fwd_msg = True
        else:
            args.last_fwd_msg = False
        
        CONFIGS, TASKS, rTASKS = compose2(args, u_fwd, pack_fwd, u_bwd, pack_bwd, verbose=True)
        ## estimate runtime 
        res = simulate(args, rTASKS, CONFIGS, TASKS=TASKS, prefetch_offload=args.prefetch_offload, verbose=False, view=False)
        if args.verbose: 
            print(sim_res_str(res, 
                              title="< %d, %d, %d, %d, %s, %s >"%
                              (u_fwd, len(pack_fwd), u_bwd, len(pack_bwd), 
                               method_fwd[:6].ljust(6,' '), 
                               method_bwd[:6].ljust(6,' ') ) ) )
        ## compare for the best
        global_time = res['global_endtime']
        # è‹¥åå‘çš„packåªæœ‰ä¸€ä¸ªï¼Œè¯´æ˜æ•´ä¸ªmodelèƒ½æ”¾è¿›ä¸€ä¸ªGPUä¸­ï¼Œæ‰“å°ä¸€æ¡è­¦å‘Šï¼Œå»ºè®®ä¸ä½¿ç”¨Harmonyæ–¹æ³•
        if len(pack_bwd) == 1:
            # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œæ‰§è¡Œ
            if not args.rank_fit_normally:
                print("[WARNING] The entire model can fit onto a single GPU. Not recommend to use Harmony. The fit config is now put as Top1.")
                global_time /= 1000.
            else:
                print("[WARNING] The entire model can fit onto a single GPU. Not recommend to use Harmony. The fit config is still ranked normally.")
        ## add to top k best
        # ç»´æŠ¤ä¸€ä¸ªå€™é€‰é›†ï¼Œå³self.roster
        # è‹¥å€™é€‰é›†æœªæ»¡(å€™é€‰é›†å¤§å°ä¸ºself.k)ï¼Œç›´æ¥å°†timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†
        # å¦åˆ™ï¼Œè‹¥ç»™å®šçš„timeå°äºå€™é€‰é›†ä¸­çš„æœ€å¤§æ—¶é—´ï¼Œåˆ æ‰å€™é€‰é›†ä¸­å…·æœ‰æœ€å¤§æ—¶é—´çš„å€™é€‰è€…ï¼Œå°†å½“å‰timeå’Œconfigæ·»åŠ åˆ°å€™é€‰é›†
        top_k.add(  global_time, 
                    ODict({ 'res': res, 
                            'CONFIGS': CONFIGS,
                            'rTASKS': rTASKS,
                            'packing_method_fwd': method_fwd,
                            'packing_method_bwd': method_bwd })) 
    
    t_end = pc()
    print("\n--- Search done: %.3f sec ---"%(t_end-t_start))
    
    return top_k

    
