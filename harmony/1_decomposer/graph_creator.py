# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import copy
import functools
import numpy as np
import os
import torch
from torch.autograd import Variable
from torch.autograd import Function

import graph

import inspect
# å¾ˆæ˜æ˜¾ï¼Œè¾“å…¥å‚æ•°æ˜¯module
def description(obj):
    """ Get one-line description of an Torch Module object.
        Format: <ClassName>(Arg1=Val1, Arg2=Val2) 
        E.g.  : Linear(in_features=512, out_features=1000, bias=True)
    """
    assert isinstance(obj, torch.nn.Module)
    if obj.extra_repr(): # most torch Modules
        return obj.__repr__()
    else: # customized Modules
        # å¦‚æœ obj æ˜¯è‡ªå®šä¹‰çš„æ¨¡å—ï¼Œåˆ™è·å–å…¶ __init__ æ–¹æ³•çš„å‚æ•°ç­¾åï¼ˆsignatureï¼‰
        sig = inspect.signature(obj.__class__.__init__)
        # ä»å‚æ•°ç­¾åä¸­è·å–æ‰€æœ‰å‚æ•°çš„åç§°
        names = [param.name for param in sig.parameters.values()]
        if "self" in names:
            names.remove("self")
        values = []
        for n in names: # in strict definition order
            # è‹¥å½“å‰å‚æ•°åæ˜¯'config'ï¼Œç›´æ¥å°†è¯¥å‚æ•°ååŠ å…¥åˆ°valuesåˆ—è¡¨ä¸­
            if n in ['config']: # BERT, GPT2
                values.append(n)
            else:
                assert hasattr(obj, n)
                # è·å–å¯¹è±¡ obj ä¸­åç§°ä¸º n çš„å±æ€§çš„å€¼
                v = getattr(obj, n)
                # è‹¥è·å–çš„å€¼æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œå°†è¯¥å€¼åŠ å…¥åˆ°valueç±»è¡¨ä¸­
                if isinstance(v, (int, float, bool, None)):
                    values.append(v)
                else:
                    print("[warning] untested argument type: {}.__init__({}={})".format(obj.__class__.__name__, n, v))
                    values.append(v)
        main_str = obj._get_name() + '('
        # make simple one-liner info as most builtin Modules will use
        # e.g.
        #       'in_features={}, out_features={}, bias={}'.format(
        #        self.in_features, self.out_features, self.bias is not None )
        main_str += ", ".join(["{}={}".format(n,v) for n,v in zip(names,values)])
        main_str += ')'
        return main_str
    # ref: 
    # - https://pytorch.org/docs/1.5.0/_modules/torch/nn/modules/module.html#Module
    # - https://pytorch.org/docs/1.5.0/_modules/torch/nn/modules/linear.html#Linear
    # - https://pytorch.org/docs/1.5.0/_modules/torch/nn/modules/conv.html#Conv2d
    # - https://docs.python.org/3.8/library/inspect.html#introspecting-callables-with-the-signature-object
    # - https://stackoverflow.com/questions/36849837/introspecting-arguments-from-the-constructor-function-init-in-python

object_id = 0

class TensorWrapper(object):
    def __init__(self, tensor, node_desc, graph_creator, activation_size=None):
        self.tensor = tensor
        global object_id
        self.object_id = object_id
        print(f"node id:{object_id}, node_desc:{node_desc}")
        object_id += 1
        self.node_desc = node_desc

        self._node = graph.Node("node%d" % object_id, node_desc=node_desc)
        self.graph_creator = graph_creator

    def size(self, dim=None):
        if dim is None:
            result = self.tensor.size()
            dim_str = ""
        else:
            result = self.tensor.size(dim)
            dim_str = "(%d)" % dim
        wrapped_result = TensorWrapper(result, "Size%s" % dim_str, self.graph_creator)
        self.graph_creator.graph.add_edge(self._node, wrapped_result.node())
        return wrapped_result

    def dim(self):
        return self.tensor.dim()

    def view(self, *wrapped_sizes):
        sizes = []
        in_edges = []
        for wrapped_size in wrapped_sizes:
            if isinstance(wrapped_size, TensorWrapper):
                sizes.append(wrapped_size.tensor)
                in_edges.append(wrapped_size)
            else:
                sizes.append(wrapped_size)
        result = self.tensor.view(*sizes)

        if len(sizes) == 1:
            wrapped_result = TensorWrapper(result, "View", self.graph_creator)
        else:
            wrapped_result = TensorWrapper(result,
                                           "View(%s)" % ", ".join([str(size) for size in sizes[1:]]),
                                           self.graph_creator)
        self.graph_creator.graph.add_edge(self._node, wrapped_result.node())
        for in_edge in in_edges:
            self.graph_creator.graph.add_edge(in_edge.node(), wrapped_result.node())
        return wrapped_result

    def __gt__(self, other):
        return self.tensor.__gt__(other)

    def __lt__(self, other):
        return self.tensor.__lt__(other)

    def __add__(self, other):
        
        if isinstance(other, TensorWrapper):
            result_tensor = self.tensor + other.tensor
        else:
            result_tensor = self.tensor + other
        wrapped_result = TensorWrapper(result_tensor, "Add", self.graph_creator)
        self.graph_creator.graph.add_edge(self._node, wrapped_result.node())
        if isinstance(other, TensorWrapper):
            self.graph_creator.graph.add_edge(other.node(), wrapped_result.node())
        return wrapped_result

    def __iadd__(self, other):
        wrapped_result = TensorWrapper(self.tensor, "Add(inplace)", self.graph_creator)
        self.tensor += other.tensor
        self.graph_creator.graph.add_edge(self._node, wrapped_result.node())
        self.graph_creator.graph.add_edge(other.node(), wrapped_result.node())
        return wrapped_result

    def __mul__(self, other):
        result = self.tensor * other.tensor
        wrapped_result = TensorWrapper(result, "Mul", self.graph_creator)
        self.graph_creator.graph.add_edge(self._node, wrapped_result.node())
        self.graph_creator.graph.add_edge(other.node(), wrapped_result.node())
        return wrapped_result

    def __getitem__(self, key):
        """ NOTE: slice is underdevelopment 
        see: 
            https://www.geeksforgeeks.org/implementing-slicing-in-__getitem__/
            https://stackoverflow.com/questions/43627405/understanding-getitem-method
            https://stackoverflow.com/questions/2936863/implementing-slicing-in-getitem
        """
        result_tensor = self.tensor[key]
        wrapped_result = TensorWrapper(result_tensor, "__getitem__(%s)" % key, self.graph_creator)
        self.graph_creator.graph.add_edge(self._node, wrapped_result.node())
        return wrapped_result

    def transpose(self, *args):
        result_tensor = self.tensor.transpose(*args)
        args_str = ", ".join([str(arg) for arg in args])
        wrapped_result = TensorWrapper(result_tensor, "Transpose(%s)" % args_str,
                                       self.graph_creator)
        self.graph_creator.graph.add_edge(self._node, wrapped_result.node())
        return wrapped_result

    def unsqueeze(self, *args):
        return self.tensor.unsqueeze(*args)

    def node(self):
        return self._node


def cat(wrapped_tensors, dim):
    tensors = []
    all_unwrapped_tensors = True
    graph_creator = None
    for wrapped_tensor in wrapped_tensors:
        if isinstance(wrapped_tensor, TensorWrapper):
            tensors.append(wrapped_tensor.tensor)
            graph_creator = wrapped_tensor.graph_creator
            all_unwrapped_tensors = False
        else:
            tensors.append(wrapped_tensor)
    # Simplifying assumption: if all tensors are "unwrapped", then we're not profiling,
    # and default to torch implementation.
    if all_unwrapped_tensors:
        return torch.cat(tensors, dim)
    result = torch.cat(tensors, dim)
    wrapped_result = TensorWrapper(result, "Concat(%d)" % dim, graph_creator)
    for wrapped_tensor in wrapped_tensors:
        if not isinstance(wrapped_tensor, TensorWrapper):
            wrapped_tensor = TensorWrapper(wrapped_tensor, "Input", graph_creator)
        graph_creator.graph.add_edge(wrapped_tensor.node(), wrapped_result.node())
    return wrapped_result


class GraphCreator(object):
    def __init__(self, model, module_whitelist=[], summary=[]):
        """
        Recursively create graph nodes from top to leaf module.
        Args:
            model: the top-level module
            module_whitelist: optional, contains module class names, if recursion hits this whitelist, just build the hit module as a graph node, without further recursion to leaf modules. Otherwise (miss this whitelist), recursion to leafs. 
            summary: optional, contains size and time of each module node
        """
        if isinstance(model, torch.nn.Module) is False:
            raise Exception("Not a valid model, please provide a 'nn.Module' instance.")

        self.model = model
        self.module_whitelist = module_whitelist
        self.summary = copy.deepcopy(summary)
        self.forward_original_methods = {}
        self.graph = graph.Graph()
        self.inputs = {} # {åŸå§‹tensorï¼šåŒ…è£…åçš„tensor}

    # å°†ç»™å®šmoduleçš„æ¯ä¸€å±‚ï¼Œè‹¥å…¶æ²¡æœ‰å­å±‚æˆ–å…¶åå­—åœ¨ç™½åå•ä¸­ï¼ˆå°±æ˜¯å„ç§GPT2layerï¼‰ï¼Œæ›¿æ¢å…¶å‰å‘ä¼ æ’­æ–¹æ³•
    # è¿™æ ·åœ¨æ‰§è¡Œå‰å‘ä¼ æ’­æœŸé—´ï¼Œä¼šè‡ªåŠ¨çš„å»ºç«‹æ¯å±‚å¯¹åº”çš„nodeï¼Œå¹¶åœ¨nodeä¹‹é—´å»ºç«‹è¾¹çš„å…³ç³»ï¼ˆé€šè¿‡é‚»æ¥è¡¨å»ºç«‹ï¼‰
    def hook_modules(self, module, root=False):
        this_creator = self
        # __dict__ å±æ€§æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«äº†ä¸€ä¸ªå¯¹è±¡çš„æ‰€æœ‰å±æ€§å’Œæ–¹æ³•
        sub_modules = module.__dict__['_modules']

        # Wrapper function to "forward()", keeping track of dependencies.

        # å¤‡æ³¨ï¼šâ€œ*â€è¡¨ç¤ºå‚æ•°å¯ä»¥æ¥å—ä»»æ„æ•°é‡çš„ä½ç½®å‚æ•°ï¼Œå¹¶ä¸”åœ¨å‡½æ•°å†…éƒ¨å¯ä»¥å°†å®ƒä»¬å½“ä½œä¸€ä¸ªå…ƒç»„è¿›è¡Œå¤„ç†
        #
        # forward()çš„åŒ…è£…å‡½æ•°ï¼ŒåŠŸèƒ½ä¸ºè¿½è¸ªä¾èµ–ï¼Œå³æ‰§è¡ŒåŸæœ¬å‰å‘è®¡ç®—åŠŸèƒ½çš„åŒæ—¶ï¼Œé€šè¿‡TensorWrapperä¸­ä¿å­˜çš„nodeæ„å»ºå±‚(node)ä¹‹é—´
        # çš„ä¾èµ–å…³ç³»ã€‚ï¼ˆæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½ä¼šè¢«åŒ…è£¹æˆTensorWrapperï¼Œè¯¥ç±»ä¸­çš„ä¸€ä¸ªæˆå‘˜å˜é‡å°±æ˜¯nodeï¼Œå³äº§ç”Ÿè¯¥è¾“å‡ºçš„å±‚ï¼‰
        # ä¸¤ç§æƒ…å†µ
        # 1.æ¨¡å‹åˆšå¼€å§‹æ‰§è¡Œæ—¶ï¼Œè¾“å…¥å½“å‰forwardçš„æ•°æ®æ˜¯tensorï¼Œåœ¨å¯¹æ¨¡å‹çš„è¾“å…¥è¿›è¡Œ TensorWrapper çš„å®ä¾‹åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œåˆ›å»ºå¯¹åº”
        #   è¾“å…¥çš„nodeï¼Œå³â€œè¾“å…¥å±‚â€ã€‚è€Œåå¾—åˆ°å½“å‰å±‚çš„è¾“å‡ºï¼Œå¹¶å¯¹è¾“å‡ºè¿›è¡ŒTensorWrapperçš„å®ä¾‹åŒ–ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­å»ºç«‹äº†å¯¹åº”æ‰§è¡Œforward
        #   çš„moduleçš„nodeã€‚è€Œåå»ºç«‹è¾“å…¥nodeå’Œå½“å‰nodeä¹‹é—´çš„è¿æ¥ã€‚è¿”å›çš„ TensorWrapper å®ä¾‹å°†ç›´æ¥ç”¨äºåç»­moduleæ‰§è¡Œ
        #   forward_wrapperå‡½æ•°æ—¶nodeä¹‹é—´çš„è¿æ¥
        # 2.è‹¥è¾“å…¥çš„tensorå·²ç»æ˜¯åŒ…è£¹åçš„tesnorï¼ŒTensorWrapperï¼Œè¯´æ˜è¾“å…¥å½“å‰å±‚çš„æ•°æ®æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œå…¶ä¸­çš„nodeæˆå‘˜å˜é‡
        #   å¿…ç„¶ä¿å­˜äº†è¾“å‡ºè¯¥è¾“å‡ºçš„nodeã€‚ç›´æ¥æ‰§è¡Œå½“å‰moduleçš„forwardå‡½æ•°å¾—åˆ°è¾“å‡ºï¼Œå»ºç«‹å½“å‰moduleçš„nodeï¼Œå¹¶åœ¨ä¸¤ä¸ªmoduleä¹‹é—´
        #   å»ºç«‹è¿æ¥
        #
        # ğŸ“Œåˆ†æï¼šåˆšçœ‹ä¼šæœ‰ç‚¹ç»•ï¼Œå› ä¸ºä¼šæŠŠTensorWrapperå®ä¾‹åŒ–çš„è¿‡ç¨‹ä¸tensorè”ç³»åœ¨ä¸€èµ·ï¼Œæ„Ÿè§‰å°±æ˜¯ä¸ºäº†å­˜æ”¾tensorã€‚å…¶å®å®ä¾‹åŒ–çš„ç›®çš„
        # æ˜¯ä¸ºäº†åˆ›å»ºnodeï¼Œç«™åœ¨nodeçš„è§’åº¦å°±å¥½ç†è§£äº†ã€‚ç»•çš„åŸå› åœ¨äºï¼Œå¹¶ä¸æ˜¯éå¸¸æ˜æ˜¾çš„ç›´æ¥å¯¹å½“å‰Moduleåˆ›å»ºnodeï¼Œè€Œæ˜¯åœ¨å½“å‰Moduleè¾“å‡º
        # ä¸€ä¸ªç»“æœåï¼Œä»¥è¯¥ç»“æœåšä¸ºå‚æ•°ä¹‹ä¸€è¿›è¡Œå®ä¾‹åŒ–ï¼Œè¿™æ ·çœ‹èµ·æ¥å®ä¾‹åŒ–çš„è¿‡ç¨‹è²Œä¼¼åªæ˜¯è·Ÿtensorç›¸å…³çš„ï¼Œå»ºç«‹nodeçš„è¿‡ç¨‹å¾ˆéšæ™¦ã€‚
        def forward_wrapper(self, *wrapped_inputs):
            input = []
            wrapped_inputs_list = list(wrapped_inputs)
            # å°†wrapped_inputs_listä¸­çš„tesnorå…¨éƒ¨æ›¿æ¢ä¸ºåŒ…è£¹åçš„tensorï¼Œæ‰§è¡Œå®Œæ¯•åï¼Œinputè¿™ä¸ªåˆ—è¡¨åŒ…å«æ‰€æœ‰åŸå§‹çš„tesnor
            #
            # åˆ†æï¼šç»è¿‡module forwardæ‰§è¡Œåçš„ç»“æœä¸€å®šæ˜¯TensorWrapperç±»å‹çš„ï¼Œå› æ­¤åœ¨forå¾ªç¯ä¸­åªä¼šæ‰§è¡Œifè¯­å¥ã€‚æ¢å¥è¯è¯´ï¼Œåªæœ‰æ¨¡å‹
            # åˆšå¼€å§‹æ‰§è¡Œæ—¶ä¼šè¿›å…¥elseï¼Œæ˜¾å¼çš„ä¼ å…¥nodeæè¿°ï¼Œå³åœ¨å®ä¾‹åŒ–TensorWrapperçš„è¿‡ç¨‹ä¸­åˆ›å»ºâ€œè¾“å…¥å±‚â€
            for i in range(len(wrapped_inputs_list)):
                # 2.è‹¥è¾“å…¥çš„tensorå·²ç»æ˜¯åŒ…è£¹åçš„tesnorï¼ŒTensorWrapperï¼Œè¯´æ˜è¾“å…¥å½“å‰å±‚çš„æ•°æ®æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œå…¶ä¸­çš„nodeæˆå‘˜å˜é‡
                #   å¿…ç„¶ä¿å­˜äº†è¾“å‡ºè¯¥è¾“å‡ºçš„nodeï¼Œå› æ­¤æ— éœ€è¿›å…¥elseè¯­å¥é€šè¿‡å®ä¾‹åŒ–TensorWrapperæ¥åˆ›å»ºnodeã€‚
                #   æ•…å¯ç›´æ¥å°†è¯¥ç±»ä¸­ä¿å­˜çš„tesnoråŠ å…¥åˆ°inputåˆ—è¡¨ä¸­ï¼Œç›´æ¥ç”¨äºè¾“å…¥ç»™å½“å‰å±‚çš„forwardå‡½æ•°
                if isinstance(wrapped_inputs_list[i], TensorWrapper):
                    input.append(wrapped_inputs_list[i].tensor)
                else:
                    # è¯¥keyå³ wrapped_inputs ä¸­çš„ä¸€ä¸ªtensor
                    key = wrapped_inputs_list[i]

                    # æ„Ÿè§‰è¿™å°±æ˜¯ä¸ªä¿é™©æ“ä½œï¼Œæš‚æ—¶ç†è§£ä¸ºåºŸæ“ä½œ
                    # è‹¥inputsè¿™ä¸ªå­—å…¸ä¸­å­˜åœ¨è¿™ä¸ªkeyï¼Œç›´æ¥æŠŠå¯¹åº”çš„å€¼æ‹¿å‡ºæ¥åŸåœ°æ›¿æ¢æ‰wrapped_inputs_liståœ¨å½“å‰ä½ç½®çš„tensor
                    if key in this_creator.inputs:
                        wrapped_inputs_list[i] = this_creator.inputs[key]

                    # 1.è‹¥ä¸å­˜åœ¨ï¼Œè¯´æ˜è¾“å…¥çš„tensoræ˜¯æ¨¡å‹çš„è¾“å…¥ï¼Œå³è®­ç»ƒæ•°æ®ã€‚æ˜¾ç„¶ï¼Œè¯¥é€»è¾‘ä»…åœ¨æ•°æ®åˆšè¿›å…¥æ¨¡å‹æ—¶æ‰§è¡Œä¸€æ¬¡ã€‚
                    #   å³åœ¨å¯¹æ¨¡å‹çš„è¾“å…¥è¿›è¡ŒTensorWrapperçš„å®ä¾‹åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œåˆ›å»ºå¯¹åº”è¯¥è¾“å…¥çš„â€œè¾“å…¥å±‚ï¼ˆè¾“å…¥nodeï¼‰â€
                    else:
                        # æ¨¡å‹åˆšå¼€å§‹æ‰§è¡Œï¼Œj=0. æ‰€ä»¥è¿™ä¸ªæè¿°ï¼ˆç¬¬2ä¸ªå‚æ•°ï¼‰æ˜¯ Input0ã€‚å¯è§åœ¨å¯¹æ¨¡å‹çš„è¾“å…¥è¿›è¡ŒTensorWrapper
                        # çš„å®ä¾‹åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œåˆ›å»ºçš„å¯¹åº”è¾“å…¥çš„nodeä¸ºâ€œè¾“å…¥å±‚â€
                        j = len(this_creator.inputs)
                        wrapped_inputs_list[i] = TensorWrapper(wrapped_inputs_list[i],
                                                               "Input%d" % j, this_creator)
                        # æŠŠ {åŸå§‹tensorï¼šåŒ…è£…åçš„tensor} åŠ å…¥åˆ° inputs è¿™ä¸ªå­—å…¸ä¸­
                        this_creator.inputs[key] = wrapped_inputs_list[i]

                    # inputä¸­è£…çš„æ˜¯åŸå§‹çš„tensor
                    input.append(wrapped_inputs_list[i].tensor)

            # å¤‡æ³¨ï¼š* æ“ä½œç¬¦ï¼Œè¡¨ç¤ºå°† input ä¸­çš„å…ƒç´ è§£åŒ…å¹¶ä½œä¸ºå‚æ•°ä¼ é€’ç»™å‡½æ•°è°ƒç”¨
            # forward_original_methods[self] å°±æ˜¯å¾—åˆ°ä¸€ä¸ªforwardæ–¹æ³•ï¼Œè¿™ä¸ªselfå°±æ˜¯sub_moduleï¼Œå¹¶éhook_modulesçš„self
            # ä½¿ç”¨è°ƒç”¨è¯¥æ–¹æ³•çš„sub_moduleåŸæœ¬çš„forwardæ–¹æ³•å¤„ç†è¾“å…¥çš„tensor
            result = this_creator.forward_original_methods[self](*input)
            
            # å¯¹ä¸Šé¢è¾“å‡ºçš„ç»“æœï¼Œå³å½“å‰module forwardåçš„ç»“æœè¿›è¡ŒåŒ…è£¹ï¼Œå³åˆ›å»ºå¯¹åº”å½“å‰moduleçš„nodeã€‚
            # æ³¨æ„ï¼Œç¬¬äºŒä¸ªå‚æ•°çš„selfæŒ‡çš„æ˜¯moduleï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹è¯¥è¾“å‡ºçš„æè¿°æ˜¯é’ˆå¯¹è¿™ä¸€å±‚çš„æè¿°
            #
            # ğŸ“Œåˆ†æï¼šåˆšçœ‹å¯èƒ½è§‰å¾—æ˜¯å¯¹å±‚çš„è¾“å‡ºçš„æè¿°ï¼Œè¿™æ˜¯ä¸å¯¹çš„ï¼Œè¯¥æè¿°ç”¨æ¥åˆ›å»ºä¸€ä¸ªnodeï¼Œå› æ­¤å½“å‰å‡½æ•°å»ºç«‹çš„èŠ‚ç‚¹å°±æ˜¯è°ƒ
            # ç”¨è¯¥forward_wrapperå‡½æ•°çš„å±‚æœ¬èº«ã€‚åªæ˜¯åœ¨è®¡ç®—è¯¥å±‚è¾“å‡ºçš„åŒæ—¶å»ºç«‹äº†è¯¥å±‚çš„èŠ‚ç‚¹ï¼Œè¿™ä¸ªæè¿°å’Œè¯¥å±‚çš„è¾“å‡ºä¹‹é—´å¹¶æ— å…³ç³»ï¼Œ
            # å°±æ˜¯ç”¨æ¥æè¿°å½“å‰å±‚çš„
            wrapped_result = TensorWrapper(result, description(self), this_creator) 
            
            # å¯¹æ¯ä¸€ä¸ªè¾“å…¥ï¼ˆTensorWrapperï¼‰ï¼š
            for wrapped_input in wrapped_inputs_list:
                # ä½¿ç”¨é‚»æ¥è¡¨å»ºç«‹èŠ‚ç‚¹ä¹‹é—´çš„æŒ‡å‘å…³ç³»
                # def add_edge(self, node1, node2): # node1 -> node2
                # ğŸ“Œå¯è§ï¼Œåªæ˜¯é€šè¿‡è¾“å…¥æ¥è·å–è¾“å‡ºè¯¥è¾“å…¥çš„nodeï¼ˆå±‚ï¼‰ï¼Œå»ºç«‹çš„è¿˜æ˜¯å±‚ä¹‹é—´çš„å…³ç³»ï¼Œä¸æ˜¯å»ºç«‹è¾“å…¥è¾“å‡ºä¹‹é—´çš„å…³ç³»
                # ğŸ“Œæ­¤å¤–ï¼Œå°½ç®¡åœ¨å®ä¾‹åŒ–TensorWrapperçš„è¿‡ç¨‹ä¸­åˆ›å»ºäº†nodeï¼Œä½†åœ¨æ‰§è¡Œadd_edgeå‡½æ•°æ—¶ï¼Œnodeæ‰è¢«åŠ å…¥åˆ°graphä¸­
                this_creator.graph.add_edge(wrapped_input.node(), wrapped_result.node())

            return wrapped_result

        # Wrapper function to "forward()", keeping track of dependencies.
        # (without creating self node)
        def forward_wrapper_root(self, *wrapped_inputs):
            input = []
            wrapped_inputs_list = list(wrapped_inputs)
            for i in range(len(wrapped_inputs_list)):
                if isinstance(wrapped_inputs_list[i], TensorWrapper):
                    input.append(wrapped_inputs_list[i].tensor)
                else:
                    key = wrapped_inputs_list[i]
                    if key in this_creator.inputs:
                        wrapped_inputs_list[i] = this_creator.inputs[key]
                    else:
                        j = len(this_creator.inputs)
                        wrapped_inputs_list[i] = TensorWrapper(wrapped_inputs_list[i],
                                                               "Input%d" % j, this_creator)
                        this_creator.inputs[key] = wrapped_inputs_list[i]
                    input.append(wrapped_inputs_list[i].tensor)
            result = this_creator.forward_original_methods[self](*input)

            return result

        # name æ˜¯æ¨¡å—åç§°ï¼Œsub_module æ˜¯å¯¹åº”çš„æ¨¡å—å¯¹è±¡
        for name, sub_module in sub_modules.items():
            # nn.Module is the only thing we care about.
            if sub_module is None or isinstance(sub_module, torch.nn.Module) is False:
                break

            sub_module_name = sub_module.__class__.__name__
            print("sub_module_name: ",sub_module_name)
            sub_sub_modules = sub_module.__dict__['_modules']
            print("len of sub_sub_modules", len(sub_sub_modules))
            print("------")
            # å¦‚æœå½“å‰å­æ¨¡å—æ²¡æœ‰å­æ¨¡å—ï¼Œæˆ–è€…å½“å‰å­æ¨¡å—åœ¨ç™½åå•ä¸­ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
            if len(sub_sub_modules) == 0 or sub_module_name in self.module_whitelist:
                """
                In 'pre_hook.patch' for the 'torchprofiler'
                +    def reset_hooks(self):
                +        self._backward_hooks = OrderedDict()
                +        self._backward_pre_hooks = OrderedDict() # profiler specific
                +        self._forward_hooks = OrderedDict()
                +        self._backward_hooks = OrderedDict()
                """
                """
                To be patch free, we manually reset hooks:
                    sub_module._backward_hooks = OrderedDict()
                    sub_module._forward_hooks = OrderedDict()
                [optional] sub_module._forward_pre_hooks = OrderedDict()
                """
                # sub_module.reset_hooks()
                if hasattr(sub_module, 'reset_hooks'): # patched for 'torchprofiler'
                    sub_module.reset_hooks()
                else: # patch free
                    from collections import OrderedDict
                    sub_module._backward_hooks = OrderedDict()
                    sub_module._forward_hooks = OrderedDict()    
                #
                # Hook leaf nn.Module with no descendants, or just in whitelist.
                #

                # Replace "forward" with "wrapped_forward".
                # è‹¥ forward_original_methods è¿™ä¸ªå­—å…¸ä¸­ä¸å­˜åœ¨ sub_module è¿™ä¸ªkeyï¼Œè¯´æ˜è¯¥å±‚è¿˜æ²¡å¤„ç†è¿‡ï¼Œè¿›è¡Œä»¥ä¸‹é€»è¾‘ï¼š
                if sub_module not in this_creator.forward_original_methods:
                    # å°†å½“å‰å­æ¨¡å—åŠå…¶ forward æ–¹æ³•æ·»åŠ åˆ° forward_original_methods è¿™ä¸ªå­—å…¸ä¸­ï¼Œå³ä¿å­˜å¥½åŸå§‹çš„forwardæ–¹æ³•
                    this_creator.forward_original_methods.update({sub_module:
                                                                   sub_module.forward})
                    # __get__ ï¼šä¸€ä¸ªæè¿°ç¬¦æ–¹æ³•ï¼Œç”¨äºç»‘å®šæ–¹æ³•æˆ–å±æ€§åˆ°ä¸€ä¸ªå¯¹è±¡ä¸Š
                    # å°†å½“å‰å­æ¨¡å—çš„ forward æ–¹æ³•æ›¿æ¢ä¸º forward_wrapper æ–¹æ³•
                    sub_module.forward = forward_wrapper.__get__(sub_module, sub_module.__class__)

            # è‹¥å½“å‰å±‚æœ‰å­å±‚ä¸”å½“å‰å­å±‚çš„åå­—ä¸åœ¨ç™½åå•ä¸­,å…ˆé€’å½’çš„ä¸ºå­å±‚æ‰§è¡Œä¸Šé¢çš„é€»è¾‘
            if len(sub_sub_modules) > 0 and sub_module_name not in self.module_whitelist:
                #
                # Recursively visit this module's descendants, if not in whitelist
                #
                self.hook_modules(sub_module)
        
        if root:
            this_creator.forward_original_methods.update({module: module.forward})
            module.forward = forward_wrapper_root.__get__(module, module.__class__)

    def unhook_modules(self):
        for sub_module in self.forward_original_methods:
            sub_module.forward = self.forward_original_methods[sub_module]

    def persist_graph(self, directory): 
        # 1.æ¸²æŸ“ä¸€ä¸ªgraphvizå›¾
        # 2.å°†å½“å‰ä¿å­˜äº†nodeå’Œnodeå…¥è¾¹å‡ºè¾¹æ–°çš„graphå®ä¾‹å­—ç¬¦ä¸²åŒ–å¹¶å†™å…¥åˆ°txtæ–‡ä»¶
        graph.save_graph(self.graph, directory, graph.GRAPH_FILENAME)
