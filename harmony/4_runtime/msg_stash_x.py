# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import os
import threading
from collections import OrderedDict as ODict

import torch
import torch.distributed as dist
from torch.autograd import Variable

from torch.cuda.nvtx import range_push as nvtx_range_push 
from torch.cuda.nvtx import range_pop as nvtx_range_pop 

from prof_data_struct import ConstMeta, TensorMeta, XMeta, TMeta
from task_data_struct import Medium, vTask
import threadsafe_data_struct

class MSGStashX(object):
    """ Handles gloo send/recv of stashing X between cpu processes. 
        Assumption:
            0) distributed environment already initialized
            1) uses task graph and profiled tensor metas
            2) only CPU tensor and no grad
            3) equal microbatchsize from FWD to BWD (after UBatchSizeConverter)
            4) only FWD(non-criterion) to BWD(non-criterion) has stashX
    """
    def __init__(self, rank, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering='pack-by-pack', pin_memory=True, verbose=False, nvprof=False): 
        self.rank = rank
        assert dist.get_rank() == rank
        self.group = dist.new_group(ranks=None, backend='gloo')
        self.pin_memory = pin_memory
        self.verbose = verbose
        self.nvprof = nvprof
        # This function requires that 1) all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group; 2) groups should be created in the same order in all processes.
        self._initialize(rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering)
        # 1.åˆ›å»ºä¸€ä¸ªå‘é€æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _send_helper_thread
        #   ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨
        #   send_dictä¸­ï¼Œä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
        #   è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
        #   æ–°çš„tesnor
        # 2.ä¸ºæ¯ä¸ª src_rankï¼Œå³ç»™å½“å‰rankå‘é€Xçš„rankï¼Œåˆ›å»ºä¸€ä¸ªæ¥æ”¶æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _recv_helper_thread
        #   ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
        #   å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥Xã€‚ğŸ“Œè‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
        self._start_helper_threads()

    # åœ¨å½“å‰rankçš„taskä¸­å¯»æ‰¾ï¼šç±»å‹ä¸ºFWDã€éæœ€åä¸€ä¸ªlossè®¡ç®—ä»»åŠ¡ã€ä¸”éœ€è¦è¾“å‡ºè¾“å…¥çš„ä»»åŠ¡
    # è‹¥å­˜åœ¨è¿™æ ·çš„ä»»åŠ¡ï¼Œä¸”è¾“å‡ºåª’ä»‹ä¸ºMSGï¼Œå°†ç›®æ ‡rankæ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xæ˜¯è¾“å…¥åˆ°å“ªä¸€å±‚çš„)ï¼šrank(dst_rank)}
    def _find_send_ranks(self, rtasks):
        send_ranks = ODict()
        # åœ¨å½“å‰rankçš„taskä¸­å¯»æ‰¾ï¼šç±»å‹ä¸ºFWDã€éæœ€åä¸€ä¸ªlossè®¡ç®—ä»»åŠ¡ã€ä¸”éœ€è¦è¾“å‡ºè¾“å…¥çš„ä»»åŠ¡
        # è‹¥å­˜åœ¨è¿™æ ·çš„ä»»åŠ¡ï¼Œä¸”è¾“å‡ºåª’ä»‹ä¸ºMSGï¼Œå°†ç›®æ ‡rankæ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œ{l(å±‚å·)ï¼šrank(dst_rank)}
        for vt in rtasks[self.rank]:
            # 
            if vt.type == 'FWD' and (not vt.has_criterion) and vt.Out['X']: # FIX 4)
                for l,m in vt.Out['X'].items(): 
                    # è‹¥åª’ä»‹ä¸ºMSGï¼Œ
                    if m.medium == "MSG":
                        send_ranks[l] = m.rank # dst_rank
        return send_ranks

    # å¯¹å½“å‰rankä¸­é™¤ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ä»¥å¤–çš„BWDä»»åŠ¡ï¼Œè‹¥æ¥æ”¶çš„Xçš„åª’ä»‹ä¸ºMSGï¼Œå°†æ¥æ”¶è¯¥Xçš„layer_idå’Œsrc_rankäº’ç›¸ä½œä¸ºé”®å€¼å¯¹
    # æ·»åŠ åˆ°ä¸¤ä¸ªå­—å…¸ä¸­å¹¶è¿”å›
    # è¿”å›ä¸¤ä¸ªå­—å…¸ï¼š{ layer_id: src_rank }ï¼Œ{ src_rank: [layer_id] }
    def _find_recv_ranks_layers(self, rtasks):
        recv_ranks = ODict() # { layer_id: src_rank } # can include self.rank
        recv_layers = ODict() # { src_rank: [layer_id] } # can include self.rank
        for vt in rtasks[self.rank]:
            if vt.type == 'BWD' and (not vt.has_criterion) and vt.In['X']: # FIX 4)
                for l,m in vt.In['X'].items(): 
                    if m.medium == "MSG":
                        recv_ranks[l] = m.rank # src_rank
                        if m.rank not in recv_layers:
                            recv_layers[m.rank] = []
                        recv_layers[m.rank].append(l)
        return recv_ranks, recv_layers

    # åœ¨å½“å‰rankä¸Šå¯»æ‰¾é™¤æœ€åä¸€ä¸ªfwdä»»åŠ¡å¤–çš„å…¶ä»–fwdä»»åŠ¡
    # å°†è¦å‘é€çš„Xå¯¹åº”çš„layerå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„ï¼Œæ”¾åˆ°ä¸€ä¸ªlistä¸­è¿”å›ï¼Œ[(layer_id,bwd_ubsize), ...]
    # 
    def _find_send_order(self, rtasks):
        """ find the send order of [(layer_id,ubatchsize)] from self rank. """
        order = []
        # å¯¹å½“å‰rankä¸­çš„æ‰€æœ‰taskï¼Œ
        for vt in rtasks[self.rank]: # pre-run task graph at self rank
            # 
            if vt.type == 'FWD' and (not vt.has_criterion) and vt.is_gpu: # FIX 4)
                # éå†BWDçš„microbatch sizeåˆ—è¡¨
                for u in self.ubatchszs:
                    # å¯¹è¯¥vtä¸Šæ¯ä¸€ä¸ªè¦å‘é€çš„Xï¼Œè‹¥åª’ä»‹ä¸ºMSGï¼Œå°†(layer_id,bwd_ubsize)ä½œä¸ºå…ƒç»„åŠ å…¥åˆ°order list
                    for l,m in vt.Out['X'].items(): 
                        if m.medium == "MSG":
                            order.append((l,u)) # can include self queue
        return order
    
    # æ‰¾åˆ°å‘é€è¾“å…¥Xåˆ°å½“å‰rankçš„ä»»åŠ¡ï¼Œæ”¾è¿›å­—å…¸ä¸­ï¼Œå­—å…¸çš„keyä¸ºsrc_rankï¼Œvalä¸ºä¸€ä¸ªlistï¼Œè£…ç€è¯¥rankä¸Šå¯¹åº”Xçš„layer_idå’Œ
    # ubatchsize_bwdã€‚{src_rankï¼š[(l,u),...], ...}
    # è¯¥å­—å…¸æ˜¯æŒ‰rankå·æ’åºçš„
    def _find_recv_order(self, rtasks):
        """ find the recv orders of { src_rank : (layer_id,ubatchsize) } to self rank. """
        orders = ODict() # { src_rank: order }
        for src_rank, vts in rtasks.items(): # pre-run task graph at all ranks
            for vt in vts:
                if vt.type == 'FWD' and (not vt.has_criterion) and vt.is_gpu: # FIX 4)
                    for u in self.ubatchszs:
                        for l,m in vt.Out['X'].items(): 
                            if m.medium == "MSG":
                                # è‹¥src_rankçš„å‘é€Xçš„taskçš„ç›®æ ‡rankç­‰äºå½“å‰rankï¼Œå°†src_rankä¸Šå¯¹åº”è¯¥Xçš„layer_idå’Œubatchsize_bwd
                                # åŠ å…¥åˆ°è¯¥src_rankçš„åˆ—è¡¨ä¸­ã€‚src_rankï¼š[(l,u),...]
                                if m.rank == self.rank: # can include self queue # and vt.rank != self.rank: 
                                    if src_rank not in orders:
                                        orders[src_rank] = []
                                    orders[src_rank].append((l,u))
        return orders
    
    # 1.æ‰¾åˆ°å½“å‰rankä¸Šå‘é€Xçš„ä»»åŠ¡ï¼ˆåª’ä»‹ä¸ºMSGï¼‰ï¼Œæ„å»ºä¸€ä¸ªå­—å…¸ï¼šï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xå¯¹åº”çš„é‚£ä¸€å±‚)ï¼šrank(dst_rank)}
    # 2.è‹¥ä¸Šä¸€æ­¥ç”Ÿæˆçš„å­—å…¸ä¸ä¸ºç©ºï¼Œå³å­˜åœ¨MSGXä»»åŠ¡ã€‚å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸ï¼Œç”¨äºåœ¨çº¿ç¨‹é—´å‘é€æ¥æ”¶æ•°æ®
    #   2.1.åˆå§‹åŒ–æœ‰åºå­—å…¸ï¼Œå³ä¸ºä¼ è¿›æ¥çš„ layer_ids è¿™ä¸ªlistä¸­çš„ layer_id æ‰§è¡Œï¼šself.odict[id] = []
    #   2.2.åˆå§‹åŒ–ä¸€ä¸ªæˆå‘˜å˜é‡ï¼Œlayer_idsï¼Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¼ è¿›æ¥çš„layer_idï¼Œä¸”æ˜¯æœ‰åºçš„
    # 3.ä»å½“å‰rankçš„BWDä»»åŠ¡ä¸­ï¼ŒæŠŠé‚£äº›æ¥æ”¶è¾“å…¥Xçš„ä»»åŠ¡çš„ä¿¡æ¯æå–å‡ºæ¥å¹¶è¿”å›ï¼Œå³æ”¶é›†å½“å‰rankä¸Šçš„MSGinXä¿¡æ¯ï¼ŒåŒ…å«ä¸¤ä¸ªå­—å…¸ï¼š
    #   self.recv_ranks = { layer_idï¼ˆæ¥å—çš„å±‚idï¼‰: src_rankï¼ˆæ¥æºrankï¼‰ } # can include self.rank
    #   self.recv_layers = { src_rank: [layer_id] } # can include self.rank  
    # 4.éå†src_rankï¼Œä¸ºè¿™äº›æºrankå®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸ï¼Œå¹¶è¿›è¡Œåˆå§‹åŒ–ã€‚
    #   4.1.è‹¥src_rankå°±æ˜¯å½“å‰rankï¼Œä¼šè¿›è¡Œä¸€äº›é¢å¤–çš„ä¿é™©æ“ä½œï¼Œä»¥ç¡®ä¿å‘é€å’Œæ¥æ”¶æ˜¯ä¸€ä¸€å¯¹åº”çš„
    # 5.å°†æ‰€æœ‰src_rankæ·»åŠ åˆ° recv_tags å­—å…¸ä¸­ï¼š{src_rankï¼šsrc_rank, ...}
    # 6.self.ubatchszs = ubatchszs_bwd
    # 7.
    #   7.1.å°†è¦å‘é€çš„Xå¯¹åº”çš„layer idå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„ï¼Œæ”¾åˆ°ä¸€ä¸ªlistä¸­è¿”å›ï¼Œ[(layer_id,ubatchsize), ...]
    #   7.2.æ‰¾åˆ°å‘é€Xåˆ°å½“å‰rankçš„ä»»åŠ¡ï¼Œæ”¾è¿›å­—å…¸ä¸­ï¼Œå­—å…¸çš„keyä¸ºsrc_rankï¼Œvalä¸ºä¸€ä¸ªlistï¼Œè£…ç€è¯¥rankä¸Šå¯¹åº”Xçš„layer_idå’Œ
    #       ubatchsize_bwdã€‚{src_rankï¼š[(l,u),...], ...}
    def _initialize(self, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering='pack-by-pack'):
        """
        Argument: ordering = the sending order (this ordering is self contained)
        """
        # setup send dict # { layer_id: dst_rank } # can include self.rank
        # 1.æ‰¾åˆ°å½“å‰rankä¸Šå‘é€Xçš„ä»»åŠ¡ï¼ˆåª’ä»‹ä¸ºMSGï¼‰ï¼Œæ„å»ºä¸€ä¸ªå­—å…¸ï¼šï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xå¯¹åº”çš„é‚£ä¸€å±‚)ï¼šrank(dst_rank)}
        # åœ¨å½“å‰rankçš„taskä¸­å¯»æ‰¾ï¼šç±»å‹ä¸ºFWDã€éæœ€åä¸€ä¸ªlossè®¡ç®—ä»»åŠ¡ã€ä¸”éœ€è¦è¾“å‡ºè¾“å…¥çš„ä»»åŠ¡
        # è‹¥å­˜åœ¨è¿™æ ·çš„ä»»åŠ¡ï¼Œä¸”è¾“å‡ºåª’ä»‹ä¸ºMSGï¼Œå°†ç›®æ ‡rankæ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xæ˜¯è¾“å…¥åˆ°å“ªä¸€å±‚çš„)ï¼šrank(dst_rank)}
        self.send_ranks = self._find_send_ranks(rtasks)
        # è‹¥å­˜åœ¨è¦å‘é€çš„ä»»åŠ¡
        if self.send_ranks:
            # å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸ï¼Œè¯¥ç±»ä¸­çš„å­—å…¸åªæœ‰ä¸€ä¸ªçº¿ç¨‹èƒ½å‘å…¶æ·»åŠ æˆ–åˆ é™¤å¯¹è±¡
            self.send_dict = threadsafe_data_struct.OrderedDictionary() # between main and send threads
            # 1.åˆå§‹åŒ–æœ‰åºå­—å…¸ï¼Œå³ä¸ºä¼ è¿›æ¥çš„ layer_ids è¿™ä¸ªlistä¸­çš„ layer_id æ‰§è¡Œï¼šself.odict[id] = []
            # 2.åˆå§‹åŒ–ä¸€ä¸ªæˆå‘˜å˜é‡ï¼Œlayer_idsï¼Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¼ è¿›æ¥çš„layer_idï¼Œä¸”æ˜¯æœ‰åºçš„
            self.send_dict.init_layer_ids(list(self.send_ranks.keys()))
            self.send_tag = self.rank
            if self.verbose: print_str = "[MSGStashX]\nrank{} set up send_dict=\n{}\n".format(self.rank, self.send_dict)
        else:
            self.send_dict = None
            if self.verbose: print_str = "[MSGStashX]\nrank{} has NO send job\n".format(self.rank)

        # setup recv dicts
        # å¯¹å½“å‰rankä¸­é™¤ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ä»¥å¤–çš„BWDä»»åŠ¡ï¼Œè‹¥æ¥æ”¶çš„Xçš„åª’ä»‹ä¸ºMSGï¼Œå°†æ¥æ”¶è¯¥Xçš„layer_idå’Œsrc_rankäº’ç›¸ä½œä¸ºé”®å€¼å¯¹
        # æ·»åŠ åˆ°ä¸¤ä¸ªå­—å…¸ä¸­å¹¶è¿”å›
        # è¿”å›ä¸¤ä¸ªå­—å…¸ï¼š
        # self.recv_ranks = { layer_idï¼ˆæ¥å—çš„å±‚idï¼‰: src_rankï¼ˆæ¥æºrankï¼‰ } # can include self.rank
        # self.recv_layers = { src_rank: [layer_id] } # can include self.rank
        self.recv_ranks, self.recv_layers = self._find_recv_ranks_layers(rtasks)
        # 
        self.recv_dicts = ODict() # { src_rank: the thread safe dict } # can include self.rank
        # å¯¹æ¯ä¸€ä¸ªå‘é€Xåˆ°å½“å‰rankçš„src_rank:
        # å¯¹MSGXå®ä¾‹ï¼šå¯¹å‘ç¬¬ä¸€ä¸ªBWDä»»åŠ¡æ‰€åœ¨çš„rankçš„MSGXå®ä¾‹å‘é€Yçš„ src_rankï¼Œæ‰§è¡Œå†…éƒ¨é€»è¾‘
        for r in sorted(set(self.recv_layers.keys())):
            # è‹¥æ¥å—çš„Xçš„æ¥æºå°±æ˜¯è‡ªå·±è¿™ä¸ªrankï¼Œ
            if r == self.rank: # loopback to self dict
                # ä¸º rank r å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸
                self.recv_dicts[r] = threadsafe_data_struct.OrderedDictionary() # between send and main threads

                # =====ç›¸æ¯”elseå¤šå‡ºçš„é€»è¾‘ï¼Œå³å¤šäº†ä¸€æ­¥æ£€æŸ¥æ“ä½œï¼Œç¡®ä¿send_ranksä¸­å…·æœ‰å’Œä»BWDä»»åŠ¡ä¸­æå–å‡ºæ¥çš„æ¥æ”¶Xçš„layer idåŒæ ·çš„layer id====
                # =====å³ç¡®ä¿åŒä¸€ä¸ªrankä¸Šçš„å‘é€å’Œæ¥æ”¶æ˜¯ä¸€ä¸€å¯¹åº”çš„====
                # å–å‡ºå½“å‰rankä¸Šå¯¹åº”æºrankçš„layer idå¹¶æ’åºï¼Œå³æ¥æ”¶Xçš„layerçš„layer id
                self_layer_ids = sorted(self.recv_layers[self.rank])
                # l:è¾“å…¥Xæ˜¯è¾“å…¥åˆ°å“ªä¸€å±‚çš„ï¼Œdstï¼šç›®æ ‡rank
                # ä¿é™©æ“ä½œï¼šä»send_ranksä¸­æŠŠç›®æ ‡rankå’Œå½“å‰rankç›¸åŒçš„æå–å‡ºæ¥ï¼Œç¡®ä¿å…¶layer_idå­˜åœ¨äºæ¥æ”¶Xçš„å±‚çš„listä¸­
                for l,dst in self.send_ranks.items():
                    if dst == self.rank:
                        assert l in self_layer_ids

                # 1.åˆå§‹åŒ–æœ‰åºå­—å…¸ï¼Œå³ä¸ºä¼ è¿›æ¥çš„ layer_ids è¿™ä¸ªlistä¸­çš„ layer_id æ‰§è¡Œï¼šself.odict[id] = []
                # 2.åˆå§‹åŒ–ä¸€ä¸ªæˆå‘˜å˜é‡ï¼Œlayer_idsï¼Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¼ è¿›æ¥çš„layer_idï¼Œä¸”æ˜¯æœ‰åºçš„
                self.recv_dicts[r].init_layer_ids(self_layer_ids)
            # å¦åˆ™å°±æ˜¯ä»å…¶ä»–rankä¼ è¿›æ¥çš„ï¼ŒåŒæ ·å‘æ¥æ”¶å­—å…¸recv_dictsä¸­æ·»åŠ ä¸€ä¸ªé”®å€¼å¯¹ï¼Œ{æ¥æºrankï¼šçº¿ç¨‹å®‰å…¨å­—å…¸}
            # å¹¶ä¸ºè¯¥çº¿ç¨‹å®‰å…¨å­—å…¸æ‰§è¡Œåˆå§‹åŒ–æµç¨‹
            else: # recv from other rank
                self.recv_dicts[r] = threadsafe_data_struct.OrderedDictionary() # between recv and main threads
                self.recv_dicts[r].init_layer_ids(sorted(self.recv_layers[r]))
        #
        # 
        self.recv_tags = ODict() # { src_rank : tag }
        # å°†æ‰€æœ‰src_rankæ·»åŠ åˆ° recv_tags å­—å…¸ä¸­ï¼š{src_rankï¼šsrc_rank, ...}
        for src_rank in sorted(self.recv_layers.keys()):
            self.recv_tags[src_rank] = src_rank
        if self.verbose:
            if self.recv_dicts:
                print_str += "rank{} set up recv_dicts (src_ranks={})\n".format(self.rank, list(self.recv_dicts.keys()))
                for src_rank, recv_dict in self.recv_dicts.items():
                    print_str += recv_dict.__repr__(title="thread-safe dict (%s)"%("self queue" if src_rank == self.rank else "src_rank=%d"%src_rank )) + "\n"
            else: # empty
                print_str += "rank{} has NO recv job\n".format(self.rank)
        # setup number of ubatches in both sending and recving
        assert isinstance(ubatchszs_bwd,list)
        self.ubatchszs = ubatchszs_bwd
        if self.verbose: print_str += "rank{} set up ubatchszs = {}\n".format(self.rank, self.ubatchszs)
        # setup send and recv order
        self.ordering = ordering
        if ordering == 'layer-by-layer':
            self.send_order = None
            self.recv_orders = None

        # 7.
        # é»˜è®¤æ‰§è¡Œè¿™ä¸ª
        elif ordering == 'pack-by-pack':    
            # å°†è¦å‘é€çš„Xå¯¹åº”çš„layer idå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„ï¼Œæ”¾åˆ°ä¸€ä¸ªlistä¸­è¿”å›ï¼Œ[(layer_id,ubatchsize), ...]
            # å‘é€é¡ºåºï¼šå…ˆæ˜¯vtä¹‹é—´æœ‰åºï¼Œvtå†…ï¼ŒæŒ‰å„ä¸ªBWD ubsizeæ’åºï¼Œå¯¹æ¯ä¸ªubsizeï¼ŒæŒ‰vtè¦å‘é€çš„å±‚æ’åº
            # [(vt1's layer1,u1),(vt1's layer1,u1),(vt2's layer1,u2),(vt2's layer1,u2),...]
            self.send_order = self._find_send_order(rtasks)
            # æ‰¾åˆ°å‘é€Xåˆ°å½“å‰rankçš„ä»»åŠ¡ï¼Œæ”¾è¿›å­—å…¸ä¸­ï¼Œå­—å…¸çš„keyä¸ºsrc_rankï¼Œvalä¸ºä¸€ä¸ªlistï¼Œè£…ç€è¯¥rankä¸Šå¯¹åº”Xçš„layer_idå’Œ
            # ubatchsize_bwdã€‚{src_rankï¼š[(l,u),...], ...}
            # è¯¥å­—å…¸æ˜¯æŒ‰rankå·æ’åºçš„
            self.recv_orders = self._find_recv_order(rtasks)
            print(f"rank:{self.rank}, recv orders:{self.recv_orders}")
            if self.verbose:
                print_str += "rank{} set up send_order = {}\n".format(self.rank, self.send_order)
                print_str += "rank{} set up recv_orders = {}\n".format(self.rank, self.recv_orders)
        else:
            raise ValueError
        # setup X_names
        self.layer_x_names = layer_x_names # { layer_id: X_names } # TODO: less stashing X after checking Identity chain --> stash X is always needed
        self.xmeta = xmeta # dictionary of TensorInfo
        
        if self.verbose: print(print_str)

    # 1.åˆ›å»ºä¸€ä¸ªå‘é€æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _send_helper_thread
    #   ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨
    #   send_dictä¸­ï¼Œä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
    #   è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
    #   æ–°çš„tesnor
    # 2.ä¸ºæ¯ä¸ª src_rankï¼Œå³ç»™å½“å‰rankå‘é€Xçš„rankï¼Œåˆ›å»ºä¸€ä¸ªæ¥æ”¶æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _recv_helper_thread
    #   ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
    #   å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥Xã€‚ğŸ“Œè‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
    def _start_helper_threads(self):
        """ Start helper communication threads, one for each queue. """
        # Setup send thread
        cnt_send_thd = 0
        # { layer_id: dst_rankï¼Œ... }
        if self.send_dict is not None:
            # åˆ›å»ºä¸€ä¸ªå‘é€æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _send_helper_thread
            # target å‚æ•°æŒ‡å®šäº†çº¿ç¨‹è¦è¿è¡Œçš„ç›®æ ‡å‡½æ•°ï¼Œå³åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œçš„å‡½æ•°
            # 
            # ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨
            # send_dictä¸­ï¼Œä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
            # è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
            # æ–°çš„tesnor
            helper_thread = threading.Thread(target=self._send_helper_thread)
            helper_thread.daemon = True
            # å¯åŠ¨è¿™ä¸ªçº¿ç¨‹
            helper_thread.start()
            cnt_send_thd += 1
        # Setup recv thread for each queue (excluding self queue)
        cnt_recv_thd = 0
        # ä¸ºæ¯ä¸ª src_rankï¼Œå³ç»™å½“å‰rankå‘é€Xçš„rankï¼Œåˆ›å»ºä¸€ä¸ªæ¥æ”¶æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _recv_helper_thread
        for src_rank in self.recv_dicts.keys():
            if src_rank != self.rank:
                # ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
                # å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥X
                helper_thread = threading.Thread(target=self._recv_helper_thread, args=(src_rank,))
                helper_thread.daemon = True
                helper_thread.start()
                cnt_recv_thd += 1
        # print("[MSGStashX] rank{} started send_helper_threadx{} & recv_helper_threadx{}".format(self.rank,cnt_send_thd,cnt_recv_thd))
    
    # 
    def _send_helper_thread(self):
        """ This method is to be executed from a helper daemon thread. """
        assert self.send_dict is not None # must be non-empty
        if self.ordering == "layer-by-layer":
            while True: # each tasks iteration
                for layer_id in self.send_dict.layer_ids: # in-order of FWD layers
                    for ubs in self.ubatchszs: 
                        named_tensors = self.send_dict.remove(layer_id)
                        dst_rank = self.send_ranks[layer_id]
                        if dst_rank == self.rank:
                            self._send2self(layer_id, named_tensors, self.pin_memory)
                        else:
                            self._send(layer_id, named_tensors, dst_rank)

        # ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨send_dictä¸­ï¼Œ
        # ä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
        # è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
        # æ–°çš„tesnor
        elif self.ordering == "pack-by-pack":
            assert len(self.send_order) == len(self.send_dict.layer_ids) * len(self.ubatchszs)
            while True: # each tasks iteration
                for layer_id, _ in self.send_order: # [(layer_id, ubatchsize)ï¼Œ...]
                    # print("[MSGStashX] rank{} wait L{}, send_dict=\n{}\n".format(self.rank, layer_id, self.send_dict))

                    # ä» self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
                    # { layer_id: [u1's {"name1": tensor1, "name2": [tensor2]}, u2's {}, ... ] }
                    named_tensors = self.send_dict.remove(layer_id)
                    # ç›®æ ‡rank
                    dst_rank = self.send_ranks[layer_id]
                    # è‹¥è¦å‘é€åˆ°çš„rankå°±æ˜¯å½“å‰rankï¼Œå°±æ˜¯è¦å‘é€ç»™è‡ªå·±
                    if dst_rank == self.rank:
                        self._send2self(layer_id, named_tensors, self.pin_memory)

                    # è°ƒç”¨ dist.send æ–¹æ³•å°†å¼ é‡å‘é€åˆ°dst_rank
                    else:
                        self._send(layer_id, named_tensors, dst_rank)
        else:
            raise ValueError
    
    # é€šè¿‡å°†tesnoræ”¾åˆ°å›ºå®šå†…å­˜ä¸­ï¼Œè¾¾åˆ°rankå†…äº’ä¼ çš„æ•ˆæœ
    def _send2self(self, layer_id, named_tensors, pin_memory=True):
        """ Helper thread sends tensor to itself rank. """
        if pin_memory: # move tensors to pin memory if not already pinned.
            for name,tensor in named_tensors.items(): # { name: tensor/const, name: [tensors] }
                if isinstance(tensor, (torch.Tensor,Variable)):
                    assert not tensor.is_cuda and not tensor.requires_grad
                    named_tensors[name] = tensor.pin_memory() # If the tensor is not pinned, returns a new copy in pinned memory. Else, returns itself (already pinned).
                elif isinstance(tensor, (float,int)):
                    continue
                elif isinstance(tensor, list): # output tuple of bert pretrainhead 
                    pinned_tensor = []
                    for t in tensor:
                        assert not t.is_cuda and not t.requires_grad
                        pinned_tensor.append(t.pin_memory())
                    named_tensors[name] = pinned_tensor
                else:
                    raise ValueError("unknown tensor={}".format(tensor))
        self.recv_dicts[self.rank].add(layer_id, named_tensors) 
        # print("[MSGStashX] rank{} _send2self enqueued (X{},{})".format(self.rank, layer_id, list(named_tensors.keys())))

    # è°ƒç”¨ dist.send æ–¹æ³•å°†å¼ é‡å‘é€åˆ°dst_rank
    def _send(self, layer_id, named_tensors, dst_rank):
        """ Helper thread sends tensor by calling dist.send(). """
        if self.nvprof: nvtx_range_push("__L{} MSG to dst{}".format(layer_id,dst_rank)) 
        # print("[MSGStashX] rank{} _sending L{} to dst{}".format(self.rank, layer_id, dst_rank))
        # named_metas = self.xmeta.get(1, layer_id) # { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
        for (name,tensor), name2 in zip(named_tensors.items(), self.layer_x_names[layer_id]): # { name: tensor/const, name: [tensors] }
            assert name == name2
            if isinstance(tensor, (torch.Tensor,Variable)):
                assert not tensor.is_cuda and not tensor.requires_grad
                dist.send(tensor, dst_rank, self.group, self.send_tag)
            elif isinstance(tensor, (float,int)):
                dist.send(torch.tensor(tensor), dst_rank, self.group, self.send_tag)
            elif isinstance(tensor, list): # output tuple of bert pretrainhead 
                for t in tensor:
                    assert not t.is_cuda and not t.requires_grad
                    dist.send(t, dst_rank, self.group, self.send_tag)
            else:
                raise ValueError("unknown tensor={}".format(tensor))
        if self.nvprof: nvtx_range_pop() 
        # print("[MSGStashX] rank{} _sent L{} to dst{}".format(self.rank, layer_id, dst_rank))
            
        
    # ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
    # å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥Xï¼Œå°†è¯»å–åˆ°çš„named_tensorsè¿åŒå…¶layer_id
    # æ„æˆä¸€ä¸ªå…ƒç»„åŠ å…¥åˆ° self.recv_dicts[src_rank] è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­
    def _recv_helper_thread(self, src_rank):
        """ This method is to be executed from a helper daemon thread. """
        assert src_rank != self.rank
        if self.ordering == "layer-by-layer":
            while True: # each tasks iteration
                for layer_id in self.recv_dicts[src_rank].layer_ids: # in-order of FWD layers
                    for ubs in self.ubatchszs: 
                        named_tensors = self._recv(layer_id, ubs, src_rank, self.pin_memory) 
                        self.recv_dicts[src_rank].add(layer_id, named_tensors)

        # æ˜¾ç„¶æ‰§è¡Œè¿™ä¸ª
        elif self.ordering == "pack-by-pack":
            if self.verbose: print("rank{}: _recv_helper_thread(src_rank={}): self.recv_orders={}, self.recv_dicts={}".format(self.rank, src_rank, self.recv_orders, self.recv_dicts))
            print(f"rank:{self.rank}, {self.__class__.__name__}")
            print(f"rank:{self.rank}, {self.ubatchszs}")
            print(f"rank:{self.rank}, {self.recv_orders[src_rank]}")
            print(f"rank:{self.rank}, {self.recv_dicts[src_rank].layer_ids}")
            assert len(self.recv_orders[src_rank]) == len(self.recv_dicts[src_rank].layer_ids) * len(self.ubatchszs)
            while True: # each tasks iteration
                for layer_id, ubs in self.recv_orders[src_rank]: # [(layer_id, ubatchsize)]
                    # è°ƒç”¨dist.recvå‡½æ•°æ¥æ”¶tesnorï¼Œæ¥æ”¶çš„tensoræ”¾å…¥ä¸€ä¸ªODictä¸­è¿”å›
                    # ğŸ“Œè‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
                    named_tensors = self._recv(layer_id, ubs, src_rank, self.pin_memory) 
                    # å°†æ”¶åˆ°çš„æ•°æ®æ·»åŠ åˆ° recv_dicts[src_rank] è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­
                    self.recv_dicts[src_rank].add(layer_id, named_tensors)
        else:
            raise ValueError

    # è°ƒç”¨dist.recvå‡½æ•°æ¥æ”¶tensorï¼Œæ¥æ”¶çš„tensoræ”¾å…¥ä¸€ä¸ªODictä¸­è¿”å›
    def _recv(self, layer_id, ubatchsize, src_rank, pin_memory=True):
        """ Helper thread receives tensor by calling dist.recv(). """ 
        # print("[rank{}]\tmsg_handler._send: entered".format(self.rank))

        # è·å–è¦æ¥æ”¶çš„Xçš„å…ƒæ•°æ®ï¼Œéœ€è¦æ ¹æ®å…¶å¤§å°å’Œç±»å‹ç”Ÿæˆtensor
        named_metas = self.xmeta.get(ubatchsize, layer_id) # { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
        #
        named_tensors = ODict()
        # nameå³layer_idå±‚çš„è¾“å…¥åç§°
        for name in self.layer_x_names[layer_id]:
            meta = named_metas[name]
            if isinstance(meta, TensorMeta):
                tensor = torch.empty(meta.shape, dtype=meta.dtype, device="cpu", pin_memory=pin_memory)
                # å°†æ•°æ®å­˜å‚¨åœ¨åˆ›å»ºçš„å¼ é‡ä¸­
                # è‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
                # print(f"[rank{self.rank}]æ­£åœ¨æ¥æ”¶layer{layer_id}")
                dist.recv(tensor, src_rank, self.group, self.recv_tags[src_rank])
                named_tensors[name] = tensor 
            elif isinstance(meta, ConstMeta):
                tensor = torch.tensor(meta.const, device="cpu", pin_memory=pin_memory)
                dist.recv(tensor, src_rank, self.group, self.recv_tags[src_rank])
                named_tensors[name] = tensor.item() # convert a 0-dim tensor to a python number
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                named_tensors[name] = []
                for m in meta:
                    tensor = torch.empty(m.shape, dtype=m.dtype, device="cpu", pin_memory=pin_memory)
                    dist.recv(tensor, src_rank, self.group, self.recv_tags[src_rank])
                    named_tensors[name].append(tensor)
            else:
                raise ValueError("unknown meta={}".format(meta))
            # print("[rank{}]\tmsg_handler._send: tensor(shape={}, dtype={}) sent".format(self.rank, tensor.shape, tensor.dtype))
        return named_tensors
    
    # Call by upstream thread. Nonblocking send. 
    # å‘ send_dict è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­çš„ odict[layer_id] è¿™ä¸ªlistæ·»åŠ ï¼šself.odict[layer_id].append(named_tensors)
    def isend(self, layer_id, named_tensors):
        ''' Call by upstream thread. Nonblocking send. 
            The same API for two ordering of 'layer-by-layer' and 'pack-by-pack' '''
        self.send_dict.add(layer_id, named_tensors) # tuple uses reference to tensor

    # 1.æ‰¾åˆ°å¯¹åº”ç»™å®šlayer_idçš„src_rankï¼Œå³ä»å“ªä¸ªrankä¸Šä¼ Xè¿‡æ¥çš„
    # 2.ä»è¯¥src rankå¯¹åº”çš„çº¿ç¨‹å®‰å…¨å­—å…¸ä¸Šï¼Œå³ä»å…¶å†…éƒ¨çš„ self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
    #   å³ä¸€ä¸ª ï¼ˆname, tensorï¼‰ã€‚è‹¥å†…éƒ¨æ²¡æœ‰tensoræ˜¾ç„¶ä¼šè¢«é˜»å¡ä½(wait)
    def recv(self, layer_id):
        ''' Call by downstream thread. Blocking recv. Return named_tensors. '''
        src_rank = self.recv_ranks[layer_id] # { layer_id: src_rank } # can include self.rank
        # ä»è¯¥src rankå¯¹åº”çš„çº¿ç¨‹å®‰å…¨å­—å…¸ä¸Šï¼Œå³ä»å…¶å†…éƒ¨çš„ self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
        # è‹¥å†…éƒ¨æ²¡æœ‰tensoræ˜¾ç„¶ä¼šè¢«é˜»å¡ä½(wait)
        return self.recv_dicts[src_rank].remove(layer_id) # tuple uses reference to tensor    
    
    def has_no_send(self):
        return self.send_dict is None
    
    def has_no_recv(self):
        return False if self.recv_dicts else True
    
    
class MSGX(MSGStashX):
    """ Handles gloo send/recv of Y/dX between cpu processes. 
        NOTE: Tentative for last fwd task to bwd criterion
        TODO: 1) To support all Y/dX; 2) replace input data structure to queue
    """
    def __init__(self, rank, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering='pack-by-pack', pin_memory=True, verbose=False, nvprof=False): 
        super().__init__(rank, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering, pin_memory, verbose, nvprof)

    #############################################################################################################
    # å¯¹ä¸‹é¢ä¸¤ä¸ªç¡®å®šrankçš„æ–¹æ³•çš„æ€»ç»“ï¼šå¯è§æ˜¯è¿™æ ·ä¸€ç§åœºæ™¯ï¼Œåªæœ‰æœ€åä¸€ä¸ªFWDvtå‘ç¬¬ä¸€ä¸ªBWDvtå‘é€Yï¼Œä¸”åœ¨cpuä¸Šå‘é€ï¼Œå³åª’ä»‹ä¸ºMSG
    # å¹¶æ²¡æœ‰ç®¡dX
    #############################################################################################################
        
    # æ€»ç»“ï¼šæ‰¾åˆ°è¦æ¥æ”¶Xçš„ç›®æ ‡rankï¼Œå³é¦–ä¸ªBWDvtæ‰€åœ¨rankï¼Œå’Œè¯¥vté¦–å±‚ç»„æˆä¸€ä¸ªé”®å€¼å¯¹
    #
    # åœ¨å½“å‰rankä¸Šæ‰¾æœ€åä¸€ä¸ªFWD vtï¼Œè‹¥æœ‰ï¼Œè®°å½•è¯¥vtè¾“å‡ºYçš„ç›®æ ‡rankï¼ˆè¾“å‡ºåª’ä»‹å¿…é¡»æ˜¯MSGæ‰ä¼šè®°å½•ï¼‰
    #
    # è‹¥å½“å‰rankä¸­å­˜åœ¨æœ€åä¸€ä¸ªFWDä»»åŠ¡ä¸”è¾“å‡ºYçš„åª’ä»‹ä¸ºMSGï¼Œå‘send_rankså­—å…¸æ·»åŠ ä¸€ä¸ªé”®å€¼å¯¹ï¼š
    # {l(å±‚å·ï¼ŒğŸ“Œæ¥æ”¶è¾“å‡º Y çš„é‚£ä¸€å±‚ï¼Œæ˜¯l+1ä¸æ˜¯l)ï¼šrank(dst_rank)}
    # è¿”å› send_ranks å­—å…¸
    # ğŸ“Œåˆ†æï¼švPPä»»åŠ¡çš„æœ€åä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºYçš„åª’ä»‹ä¸ºP2Pï¼Œè¿™é‡Œçš„send_ranksåº”è¯¥æ˜¯ç©ºçš„
    def _find_send_ranks(self, rtasks):
        send_ranks = ODict()
        for vt in rtasks[self.rank]:
            # è‹¥æ˜¯æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œä¸”ä¼šè¾“å‡ºä¸€ä¸ªY
            if vt.is_last_fwd and vt.Out['Y']:
                # l ä¸ºè¾“å‡º Y çš„é‚£ä¸€å±‚çš„layer_id
                l = vt.layers[-1]
                m = vt.Out['Y'][l]
                # mä¸­ä¿å­˜çš„rankæœ¬æ¥å°±æ˜¯ dst_rank
                # ğŸ“Œåˆ†æ24/10/8ï¼šä¸ç®¡æ˜¯vPPè¿˜æ˜¯vDPï¼ŒOut['Y'][l]è¿™ä¸ªåª’ä»‹å°±ä¸å¯èƒ½æ˜¯MSG
                # âŒï¼šä»¥ä¸Šå›ç­”é”™è¯¯ï¼Œæœ‰è¿™ç§æƒ…å†µï¼Œå³å‰åæƒ³microbatchå¤§å°ä¸ä¸€æ ·
                print(f"MSGXå®ä¾‹, m.medium:{m.medium}")
                if m.medium == "MSG":
                    send_ranks[l+1] = m.rank # dst_rank
        if self.verbose: print("[MSGX] found send_ranks={}".format(send_ranks))
        return send_ranks

    # æ€»ç»“ï¼šæ‰¾åˆ°äº§ç”ŸXå¹¶å‘é€å®ƒçš„æ¥æºrankï¼Œå³æœ€åä¸€ä¸ªFWDvtæ‰€åœ¨çš„rankï¼Œå’Œé¦–ä¸ªBWDvté¦–å±‚çš„å±‚å·ç»„æˆä¸€ä¸ªé”®å€¼å¯¹
    #
    # åœ¨å½“å‰rankä¸Šæ‰¾ç¬¬ä¸€ä¸ªBWD vtï¼Œè‹¥æœ‰ï¼Œè®°å½•ç»™è¯¥vtå‘é€è¾“å…¥Xçš„æ¥æºrankï¼ˆè¾“å…¥åª’ä»‹å¿…é¡»æ˜¯MSGæ‰ä¼šè®°å½•ï¼‰
    #
    # è‹¥å½“å‰rankä¸­å­˜åœ¨ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ï¼Œä¸”æ¥æ”¶çš„Xçš„åª’ä»‹ä¸ºMSGï¼Œå°†æ¥æ”¶è¯¥Xçš„layer_idå’Œsrc_rankäº’ç›¸ä½œä¸ºé”®å€¼å¯¹æ·»åŠ åˆ°ä¸¤ä¸ªå­—å…¸ä¸­å¹¶è¿”å›
    # è¿”å›ä¸¤ä¸ªå­—å…¸ï¼š
    # self.recv_ranks = { layer_idï¼ˆæ¥å—çš„å±‚idï¼‰: src_rankï¼ˆæ¥æºrankï¼‰ } # can include self.rank
    # self.recv_layers = { src_rank: [layer_id] } # can include self.rank
    # ğŸ“Œåˆ†æï¼švPPç¬¬ä¸€ä¸ªBWDä»»åŠ¡æ¥æ”¶è¾“å…¥Xçš„åª’ä»‹åº”è¯¥æ˜¯P2P,è¿™é‡Œçš„ä¸¤ä¸ªå­—å…¸åº”è¯¥æ˜¯ç©ºçš„
    def _find_recv_ranks_layers(self, rtasks):
        recv_ranks = ODict() # { layer_id: src_rank } # can include self.rank
        recv_layers = ODict() # { src_rank: [layer_id] } # can include self.rank
        for vt in rtasks[self.rank]:
            if vt.type == 'BWD' and vt.has_criterion and vt.In['X']:
                l = vt.layers[0]
                m = vt.In['X'][l]
                if m.medium == "MSG":
                    recv_ranks[l] = m.rank # src_rank
                    if m.rank not in recv_layers:
                        recv_layers[m.rank] = []
                    recv_layers[m.rank].append(l)
        if self.verbose: print("[MSGX] found recv_ranks={}, recv_layers={}".format(recv_ranks, recv_layers))
        return recv_ranks, recv_layers

    #############################################################################################################
    # å¯¹ä¸‹é¢ä¸¤ä¸ªorderæ–¹æ³•çš„æ€»ç»“ï¼šå¯¹ä¸€ä¸‹ä¸¤ä¸ªæ–¹æ³•çš„å‘½åå’Œä¸Šé¢ä¸¤ä¸ªæ–¹æ³•çš„å‘½åæ˜¯åè¿‡æ¥çš„ã€‚_find_send_ranksè™½ç„¶å‘½åæ–¹å¼æ˜¯
    # sendï¼Œä½†å…¶å®æ‰¾çš„æ˜¯æ¥æ”¶æ–¹ï¼Œå³ç›®æ ‡rankã€‚ä½†ä¸‹é¢è¿™ä¸ª_find_send_orderå°±æ˜¯ç¡®å®šå‘é€çš„é¡ºåºã€‚
    #############################################################################################################

    # ä¸ºæœ€åä¸€ä¸ªfwdä»»åŠ¡æ‰€åœ¨rankçš„MSGXå®ä¾‹é…ç½®å‘é€è¾“å‡ºYçš„é¡ºåºï¼ˆæœ€åä¸€ä¸ªfwdä»»åŠ¡å¹¶éçœŸæ­£çš„æœ€åä¸€ä¸ªï¼ŒçœŸæ­£çš„æœ€åä¸€ä¸ªåŒæ—¶ä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªbwdä»»åŠ¡ï¼‰
    #
    # åœ¨å½“å‰rankä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥å…¶è¾“å‡ºYçš„åª’ä»‹ä¸ºMSGï¼Œå°†â€œè¦æ¥æ”¶è¾“å‡ºYâ€çš„å±‚lå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„
    # åŠ å…¥åˆ°orderåˆ—è¡¨ä¸­ï¼Œ[(æ¥æ”¶Yçš„layer_id,ubatchsize), ...]ï¼Œæœ€åè¿”å›è¯¥åˆ—è¡¨
    # ğŸ“Œåˆ†æï¼švPPçš„æœ€åä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºYçš„åª’ä»‹ä¸ºP2Pï¼Œè¿”å›çš„ orders åº”è¯¥æ˜¯ç©ºçš„
    def _find_send_order(self, rtasks):
        """ find the send order of [(layer_id,ubatchsize)] from self rank. """
        order = []
        # åœ¨å½“å‰rankä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥å…¶è¾“å‡ºYçš„åª’ä»‹ä¸ºMSGï¼Œå°†è¦æ¥æ”¶è¾“å‡ºYçš„å±‚lå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„
        # åŠ å…¥åˆ°orderåˆ—è¡¨ä¸­
        for vt in rtasks[self.rank]: # pre-run task graph at self rank
            if vt.is_last_fwd:
                for u in self.ubatchszs:
                    l = vt.layers[-1]
                    m = vt.Out['Y'][l]
                    if m.medium == "MSG":
                        order.append((l+1,u))
        if self.verbose: print("[MSGX] found send order={}".format(order))
        return order
    
    # ä¸ºç¬¬ä¸€ä¸ªbwdä»»åŠ¡æ‰€åœ¨rankçš„MSGXå®ä¾‹é…ç½®æ¥æ”¶ï¼ˆç¬¬ä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºçš„Yçš„ï¼‰é¡ºåº
    #
    # åœ¨æ‰€æœ‰rankçš„ä»»åŠ¡ä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥è¯¥ä»»åŠ¡è¾“å‡ºçš„Yçš„åª’ä»‹ä¸ºMSGï¼Œä¸”ç›®æ ‡rankå°±æ˜¯å½“å‰rankï¼Œåˆ™å°†
    # (l+1(è¦æ¥æ”¶Yçš„å±‚)ï¼Œu)æ·»åŠ åˆ°src_rankå¯¹åº”çš„åˆ—è¡¨ä¸­ã€‚{src_rankï¼š[(æ¥æ”¶Yçš„l,u),...], ...}
    # ğŸ“Œåˆ†æï¼švPPçš„æœ€åä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºYçš„åª’ä»‹ä¸ºP2Pï¼Œè¿”å›çš„ orders åº”è¯¥æ˜¯ç©ºçš„
    def _find_recv_order(self, rtasks):
        """ find the recv orders of { src_rank : (layer_id,ubatchsize) } to self rank. """
        orders = ODict() # { src_rank: order }
        # åœ¨æ‰€æœ‰rankçš„ä»»åŠ¡ä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥è¯¥ä»»åŠ¡è¾“å‡ºçš„Yçš„åª’ä»‹ä¸ºMSGï¼Œä¸”ç›®æ ‡rankå°±æ˜¯å½“å‰rankï¼Œåˆ™å°†
        # (l+1(è¦æ¥æ”¶Yçš„å±‚)ï¼Œu)æ·»åŠ åˆ°src_rankå¯¹åº”çš„åˆ—è¡¨ä¸­
        for src_rank, vts in rtasks.items(): # pre-run task graph at all ranks
            for vt in vts:
                if vt.is_last_fwd:
                    for u in self.ubatchszs:
                        l = vt.layers[-1]
                        m = vt.Out['Y'][l]
                        # 
                        if m.medium == "MSG":
                            # è‹¥æœ€åä¸€ä¸ªfwdä»»åŠ¡å‘é€Yçš„ç›®æ ‡rankå°±æ˜¯å½“å‰rank
                            if m.rank == self.rank: # can include self queue # and vt.rank != self.rank: 
                                if src_rank not in orders:
                                    orders[src_rank] = []
                                # 
                                orders[src_rank].append((l+1,u))
        if self.verbose: print("[MSGX] rank{} found recv orders={}".format(self.rank, orders))
        return orders
    

# ä¸“é—¨ç”¨äºæœ€åä¸€ä¸ªFWDvtâ†’é¦–ä¸ªBWDvtåœ¨ä¸€ä¸ªGPUæƒ…å†µä¸‹çš„åº•å±‚å­˜å‚¨ç»“æ„å’Œå‘é€æ¥æ”¶ï¼ˆå®é™…ä¸Šå°±æ˜¯è‡ªå·±å’Œè‡ªå·±å‘é€ï¼‰
# å…¶å®ä¸»è¦è¿˜æ˜¯å®è¡Œåœ¨å‰åå‘microbatchå¤§å°ä¸ä¸€è‡´æ—¶åˆ‡å‰²Xçš„åŠŸèƒ½
# ä¸harmonyåŸç‰ˆä»£ç çš„åŒºåˆ«åœ¨äºï¼Œä¸¤ä¸ªsendæ–¹æ³•ä¸­å¯¹tensoræ˜¯å¦åœ¨cudaä¸Šçš„æ£€æŸ¥å˜ä¸ºè¦ç¡®ä¿tensoråœ¨cudaä¸Š
# å› ä¸ºç°åœ¨çš„tensroå°±æ˜¯åœ¨cudaä¸Šçš„ï¼Œä¸éœ€è¦è¿›è¡Œå¸è½½
class MSGStashX_2(object):
    """ Handles gloo send/recv of stashing X between cpu processes. 
        Assumption:
            0) distributed environment already initialized
            1) uses task graph and profiled tensor metas
            2) only CPU tensor and no grad
            3) equal microbatchsize from FWD to BWD (after UBatchSizeConverter)
            4) only FWD(non-criterion) to BWD(non-criterion) has stashX
    """
    def __init__(self, ubscvt, rank, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering='pack-by-pack', pin_memory=True, verbose=False, nvprof=False): 
        self.rank = rank
        assert dist.get_rank() == rank
        self.group = dist.new_group(ranks=None, backend='gloo')
        self.pin_memory = pin_memory
        self.verbose = verbose
        self.nvprof = nvprof

        self.converted_ubwd = ubscvt.find_converted(0)
        print(f"è½¬åŒ–åçš„ubwdåˆ—è¡¨ä¸º:{self.converted_ubwd}")
        assert type(self.converted_ubwd) is list
        self.send_and_recv_num = len(self.converted_ubwd)
        print(f"å‘é€/æ¥æ”¶æ¬¡æ•°:{self.send_and_recv_num}")

        # This function requires that 1) all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group; 2) groups should be created in the same order in all processes.
        self._initialize(rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering)
        # 1.åˆ›å»ºä¸€ä¸ªå‘é€æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _send_helper_thread
        #   ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨
        #   send_dictä¸­ï¼Œä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
        #   è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
        #   æ–°çš„tesnor
        # 2.ä¸ºæ¯ä¸ª src_rankï¼Œå³ç»™å½“å‰rankå‘é€Xçš„rankï¼Œåˆ›å»ºä¸€ä¸ªæ¥æ”¶æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _recv_helper_thread
        #   ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
        #   å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥Xã€‚ğŸ“Œè‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
        self._start_helper_threads()

    # åœ¨å½“å‰rankçš„taskä¸­å¯»æ‰¾ï¼šç±»å‹ä¸ºFWDã€éæœ€åä¸€ä¸ªlossè®¡ç®—ä»»åŠ¡ã€ä¸”éœ€è¦è¾“å‡ºè¾“å…¥çš„ä»»åŠ¡
    # è‹¥å­˜åœ¨è¿™æ ·çš„ä»»åŠ¡ï¼Œä¸”è¾“å‡ºåª’ä»‹ä¸ºMSGï¼Œå°†ç›®æ ‡rankæ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xæ˜¯è¾“å…¥åˆ°å“ªä¸€å±‚çš„)ï¼šrank(dst_rank)}
    def _find_send_ranks(self, rtasks):
        send_ranks = ODict()
        # åœ¨å½“å‰rankçš„taskä¸­å¯»æ‰¾ï¼šç±»å‹ä¸ºFWDã€éæœ€åä¸€ä¸ªlossè®¡ç®—ä»»åŠ¡ã€ä¸”éœ€è¦è¾“å‡ºè¾“å…¥çš„ä»»åŠ¡
        # è‹¥å­˜åœ¨è¿™æ ·çš„ä»»åŠ¡ï¼Œä¸”è¾“å‡ºåª’ä»‹ä¸ºMSGï¼Œå°†ç›®æ ‡rankæ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œ{l(å±‚å·)ï¼šrank(dst_rank)}
        for vt in rtasks[self.rank]:
            # 
            if vt.type == 'FWD' and (not vt.has_criterion) and vt.Out['X']: # FIX 4)
                for l,m in vt.Out['X'].items(): 
                    # è‹¥åª’ä»‹ä¸ºMSGï¼Œ
                    if m.medium == "MSG":
                        send_ranks[l] = m.rank # dst_rank
        return send_ranks

    # å¯¹å½“å‰rankä¸­é™¤ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ä»¥å¤–çš„BWDä»»åŠ¡ï¼Œè‹¥æ¥æ”¶çš„Xçš„åª’ä»‹ä¸ºMSGï¼Œå°†æ¥æ”¶è¯¥Xçš„layer_idå’Œsrc_rankäº’ç›¸ä½œä¸ºé”®å€¼å¯¹
    # æ·»åŠ åˆ°ä¸¤ä¸ªå­—å…¸ä¸­å¹¶è¿”å›
    # è¿”å›ä¸¤ä¸ªå­—å…¸ï¼š{ layer_id: src_rank }ï¼Œ{ src_rank: [layer_id] }
    def _find_recv_ranks_layers(self, rtasks):
        recv_ranks = ODict() # { layer_id: src_rank } # can include self.rank
        recv_layers = ODict() # { src_rank: [layer_id] } # can include self.rank
        for vt in rtasks[self.rank]:
            if vt.type == 'BWD' and (not vt.has_criterion) and vt.In['X']: # FIX 4)
                for l,m in vt.In['X'].items(): 
                    if m.medium == "MSG":
                        recv_ranks[l] = m.rank # src_rank
                        if m.rank not in recv_layers:
                            recv_layers[m.rank] = []
                        recv_layers[m.rank].append(l)
        return recv_ranks, recv_layers

    # åœ¨å½“å‰rankä¸Šå¯»æ‰¾é™¤æœ€åä¸€ä¸ªfwdä»»åŠ¡å¤–çš„å…¶ä»–fwdä»»åŠ¡
    # å°†è¦å‘é€çš„Xå¯¹åº”çš„layerå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„ï¼Œæ”¾åˆ°ä¸€ä¸ªlistä¸­è¿”å›ï¼Œ[(layer_id,bwd_ubsize), ...]
    # 
    def _find_send_order(self, rtasks):
        """ find the send order of [(layer_id,ubatchsize)] from self rank. """
        order = []
        # å¯¹å½“å‰rankä¸­çš„æ‰€æœ‰taskï¼Œ
        for vt in rtasks[self.rank]: # pre-run task graph at self rank
            # 
            if vt.type == 'FWD' and (not vt.has_criterion) and vt.is_gpu: # FIX 4)
                # éå†BWDçš„microbatch sizeåˆ—è¡¨
                for u in self.ubatchszs:
                    # å¯¹è¯¥vtä¸Šæ¯ä¸€ä¸ªè¦å‘é€çš„Xï¼Œè‹¥åª’ä»‹ä¸ºMSGï¼Œå°†(layer_id,bwd_ubsize)ä½œä¸ºå…ƒç»„åŠ å…¥åˆ°order list
                    for l,m in vt.Out['X'].items(): 
                        if m.medium == "MSG":
                            order.append((l,u)) # can include self queue
        return order
    
    # æ‰¾åˆ°å‘é€è¾“å…¥Xåˆ°å½“å‰rankçš„ä»»åŠ¡ï¼Œæ”¾è¿›å­—å…¸ä¸­ï¼Œå­—å…¸çš„keyä¸ºsrc_rankï¼Œvalä¸ºä¸€ä¸ªlistï¼Œè£…ç€è¯¥rankä¸Šå¯¹åº”Xçš„layer_idå’Œ
    # ubatchsize_bwdã€‚{src_rankï¼š[(l,u),...], ...}
    # è¯¥å­—å…¸æ˜¯æŒ‰rankå·æ’åºçš„
    def _find_recv_order(self, rtasks):
        """ find the recv orders of { src_rank : (layer_id,ubatchsize) } to self rank. """
        orders = ODict() # { src_rank: order }
        for src_rank, vts in rtasks.items(): # pre-run task graph at all ranks
            for vt in vts:
                if vt.type == 'FWD' and (not vt.has_criterion) and vt.is_gpu: # FIX 4)
                    for u in self.ubatchszs:
                        for l,m in vt.Out['X'].items(): 
                            if m.medium == "MSG":
                                # è‹¥src_rankçš„å‘é€Xçš„taskçš„ç›®æ ‡rankç­‰äºå½“å‰rankï¼Œå°†src_rankä¸Šå¯¹åº”è¯¥Xçš„layer_idå’Œubatchsize_bwd
                                # åŠ å…¥åˆ°è¯¥src_rankçš„åˆ—è¡¨ä¸­ã€‚src_rankï¼š[(l,u),...]
                                if m.rank == self.rank: # can include self queue # and vt.rank != self.rank: 
                                    if src_rank not in orders:
                                        orders[src_rank] = []
                                    orders[src_rank].append((l,u))
        return orders
    
    # 1.æ‰¾åˆ°å½“å‰rankä¸Šå‘é€Xçš„ä»»åŠ¡ï¼ˆåª’ä»‹ä¸ºMSGï¼‰ï¼Œæ„å»ºä¸€ä¸ªå­—å…¸ï¼šï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xå¯¹åº”çš„é‚£ä¸€å±‚)ï¼šrank(dst_rank)}
    # 2.è‹¥ä¸Šä¸€æ­¥ç”Ÿæˆçš„å­—å…¸ä¸ä¸ºç©ºï¼Œå³å­˜åœ¨MSGXä»»åŠ¡ã€‚å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸ï¼Œç”¨äºåœ¨çº¿ç¨‹é—´å‘é€æ¥æ”¶æ•°æ®
    #   2.1.åˆå§‹åŒ–æœ‰åºå­—å…¸ï¼Œå³ä¸ºä¼ è¿›æ¥çš„ layer_ids è¿™ä¸ªlistä¸­çš„ layer_id æ‰§è¡Œï¼šself.odict[id] = []
    #   2.2.åˆå§‹åŒ–ä¸€ä¸ªæˆå‘˜å˜é‡ï¼Œlayer_idsï¼Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¼ è¿›æ¥çš„layer_idï¼Œä¸”æ˜¯æœ‰åºçš„
    # 3.ä»å½“å‰rankçš„BWDä»»åŠ¡ä¸­ï¼ŒæŠŠé‚£äº›æ¥æ”¶è¾“å…¥Xçš„ä»»åŠ¡çš„ä¿¡æ¯æå–å‡ºæ¥å¹¶è¿”å›ï¼Œå³æ”¶é›†å½“å‰rankä¸Šçš„MSGinXä¿¡æ¯ï¼ŒåŒ…å«ä¸¤ä¸ªå­—å…¸ï¼š
    #   self.recv_ranks = { layer_idï¼ˆæ¥å—çš„å±‚idï¼‰: src_rankï¼ˆæ¥æºrankï¼‰ } # can include self.rank
    #   self.recv_layers = { src_rank: [layer_id] } # can include self.rank  
    # 4.éå†src_rankï¼Œä¸ºè¿™äº›æºrankå®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸ï¼Œå¹¶è¿›è¡Œåˆå§‹åŒ–ã€‚
    #   4.1.è‹¥src_rankå°±æ˜¯å½“å‰rankï¼Œä¼šè¿›è¡Œä¸€äº›é¢å¤–çš„ä¿é™©æ“ä½œï¼Œä»¥ç¡®ä¿å‘é€å’Œæ¥æ”¶æ˜¯ä¸€ä¸€å¯¹åº”çš„
    # 5.å°†æ‰€æœ‰src_rankæ·»åŠ åˆ° recv_tags å­—å…¸ä¸­ï¼š{src_rankï¼šsrc_rank, ...}
    # 6.self.ubatchszs = ubatchszs_bwd
    # 7.
    #   7.1.å°†è¦å‘é€çš„Xå¯¹åº”çš„layer idå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„ï¼Œæ”¾åˆ°ä¸€ä¸ªlistä¸­è¿”å›ï¼Œ[(layer_id,ubatchsize), ...]
    #   7.2.æ‰¾åˆ°å‘é€Xåˆ°å½“å‰rankçš„ä»»åŠ¡ï¼Œæ”¾è¿›å­—å…¸ä¸­ï¼Œå­—å…¸çš„keyä¸ºsrc_rankï¼Œvalä¸ºä¸€ä¸ªlistï¼Œè£…ç€è¯¥rankä¸Šå¯¹åº”Xçš„layer_idå’Œ
    #       ubatchsize_bwdã€‚{src_rankï¼š[(l,u),...], ...}
    def _initialize(self, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering='pack-by-pack'):
        """
        Argument: ordering = the sending order (this ordering is self contained)
        """
        # setup send dict # { layer_id: dst_rank } # can include self.rank
        # 1.æ‰¾åˆ°å½“å‰rankä¸Šå‘é€Xçš„ä»»åŠ¡ï¼ˆåª’ä»‹ä¸ºMSGï¼‰ï¼Œæ„å»ºä¸€ä¸ªå­—å…¸ï¼šï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xå¯¹åº”çš„é‚£ä¸€å±‚)ï¼šrank(dst_rank)}
        # åœ¨å½“å‰rankçš„taskä¸­å¯»æ‰¾ï¼šç±»å‹ä¸ºFWDã€éæœ€åä¸€ä¸ªlossè®¡ç®—ä»»åŠ¡ã€ä¸”éœ€è¦è¾“å‡ºè¾“å…¥çš„ä»»åŠ¡
        # è‹¥å­˜åœ¨è¿™æ ·çš„ä»»åŠ¡ï¼Œä¸”è¾“å‡ºåª’ä»‹ä¸ºMSGï¼Œå°†ç›®æ ‡rankæ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œ{l(å±‚å·ï¼Œè¾“å…¥Xæ˜¯è¾“å…¥åˆ°å“ªä¸€å±‚çš„)ï¼šrank(dst_rank)}
        self.send_ranks = self._find_send_ranks(rtasks)
        # è‹¥å­˜åœ¨è¦å‘é€çš„ä»»åŠ¡
        if self.send_ranks:
            # å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸ï¼Œè¯¥ç±»ä¸­çš„å­—å…¸åªæœ‰ä¸€ä¸ªçº¿ç¨‹èƒ½å‘å…¶æ·»åŠ æˆ–åˆ é™¤å¯¹è±¡
            self.send_dict = threadsafe_data_struct.OrderedDictionary() # between main and send threads
            # 1.åˆå§‹åŒ–æœ‰åºå­—å…¸ï¼Œå³ä¸ºä¼ è¿›æ¥çš„ layer_ids è¿™ä¸ªlistä¸­çš„ layer_id æ‰§è¡Œï¼šself.odict[id] = []
            # 2.åˆå§‹åŒ–ä¸€ä¸ªæˆå‘˜å˜é‡ï¼Œlayer_idsï¼Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¼ è¿›æ¥çš„layer_idï¼Œä¸”æ˜¯æœ‰åºçš„
            self.send_dict.init_layer_ids(list(self.send_ranks.keys()))
            self.send_tag = self.rank
            if self.verbose: print_str = "[MSGStashX]\nrank{} set up send_dict=\n{}\n".format(self.rank, self.send_dict)
        else:
            self.send_dict = None
            if self.verbose: print_str = "[MSGStashX]\nrank{} has NO send job\n".format(self.rank)

        # setup recv dicts
        # å¯¹å½“å‰rankä¸­é™¤ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ä»¥å¤–çš„BWDä»»åŠ¡ï¼Œè‹¥æ¥æ”¶çš„Xçš„åª’ä»‹ä¸ºMSGï¼Œå°†æ¥æ”¶è¯¥Xçš„layer_idå’Œsrc_rankäº’ç›¸ä½œä¸ºé”®å€¼å¯¹
        # æ·»åŠ åˆ°ä¸¤ä¸ªå­—å…¸ä¸­å¹¶è¿”å›
        # è¿”å›ä¸¤ä¸ªå­—å…¸ï¼š
        # self.recv_ranks = { layer_idï¼ˆæ¥å—çš„å±‚idï¼‰: src_rankï¼ˆæ¥æºrankï¼‰ } # can include self.rank
        # self.recv_layers = { src_rank: [layer_id] } # can include self.rank
        self.recv_ranks, self.recv_layers = self._find_recv_ranks_layers(rtasks)
        # 
        self.recv_dicts = ODict() # { src_rank: the thread safe dict } # can include self.rank
        # å¯¹æ¯ä¸€ä¸ªå‘é€Xåˆ°å½“å‰rankçš„src_rank:
        # å¯¹MSGXå®ä¾‹ï¼šå¯¹å‘ç¬¬ä¸€ä¸ªBWDä»»åŠ¡æ‰€åœ¨çš„rankçš„MSGXå®ä¾‹å‘é€Yçš„ src_rankï¼Œæ‰§è¡Œå†…éƒ¨é€»è¾‘
        for r in sorted(set(self.recv_layers.keys())):
            # è‹¥æ¥å—çš„Xçš„æ¥æºå°±æ˜¯è‡ªå·±è¿™ä¸ªrankï¼Œ
            if r == self.rank: # loopback to self dict
                # ä¸º rank r å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å­—å…¸
                self.recv_dicts[r] = threadsafe_data_struct.OrderedDictionary() # between send and main threads

                # =====ç›¸æ¯”elseå¤šå‡ºçš„é€»è¾‘ï¼Œå³å¤šäº†ä¸€æ­¥æ£€æŸ¥æ“ä½œï¼Œç¡®ä¿send_ranksä¸­å…·æœ‰å’Œä»BWDä»»åŠ¡ä¸­æå–å‡ºæ¥çš„æ¥æ”¶Xçš„layer idåŒæ ·çš„layer id====
                # =====å³ç¡®ä¿åŒä¸€ä¸ªrankä¸Šçš„å‘é€å’Œæ¥æ”¶æ˜¯ä¸€ä¸€å¯¹åº”çš„====
                # å–å‡ºå½“å‰rankä¸Šå¯¹åº”æºrankçš„layer idå¹¶æ’åºï¼Œå³æ¥æ”¶Xçš„layerçš„layer id
                self_layer_ids = sorted(self.recv_layers[self.rank])
                # l:è¾“å…¥Xæ˜¯è¾“å…¥åˆ°å“ªä¸€å±‚çš„ï¼Œdstï¼šç›®æ ‡rank
                # ä¿é™©æ“ä½œï¼šä»send_ranksä¸­æŠŠç›®æ ‡rankå’Œå½“å‰rankç›¸åŒçš„æå–å‡ºæ¥ï¼Œç¡®ä¿å…¶layer_idå­˜åœ¨äºæ¥æ”¶Xçš„å±‚çš„listä¸­
                for l,dst in self.send_ranks.items():
                    if dst == self.rank:
                        assert l in self_layer_ids

                # 1.åˆå§‹åŒ–æœ‰åºå­—å…¸ï¼Œå³ä¸ºä¼ è¿›æ¥çš„ layer_ids è¿™ä¸ªlistä¸­çš„ layer_id æ‰§è¡Œï¼šself.odict[id] = []
                # 2.åˆå§‹åŒ–ä¸€ä¸ªæˆå‘˜å˜é‡ï¼Œlayer_idsï¼Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¼ è¿›æ¥çš„layer_idï¼Œä¸”æ˜¯æœ‰åºçš„
                self.recv_dicts[r].init_layer_ids(self_layer_ids)
            # å¦åˆ™å°±æ˜¯ä»å…¶ä»–rankä¼ è¿›æ¥çš„ï¼ŒåŒæ ·å‘æ¥æ”¶å­—å…¸recv_dictsä¸­æ·»åŠ ä¸€ä¸ªé”®å€¼å¯¹ï¼Œ{æ¥æºrankï¼šçº¿ç¨‹å®‰å…¨å­—å…¸}
            # å¹¶ä¸ºè¯¥çº¿ç¨‹å®‰å…¨å­—å…¸æ‰§è¡Œåˆå§‹åŒ–æµç¨‹
            else: # recv from other rank
                self.recv_dicts[r] = threadsafe_data_struct.OrderedDictionary() # between recv and main threads
                self.recv_dicts[r].init_layer_ids(sorted(self.recv_layers[r]))
        #
        # 
        self.recv_tags = ODict() # { src_rank : tag }
        # å°†æ‰€æœ‰src_rankæ·»åŠ åˆ° recv_tags å­—å…¸ä¸­ï¼š{src_rankï¼šsrc_rank, ...}
        for src_rank in sorted(self.recv_layers.keys()):
            self.recv_tags[src_rank] = src_rank
        if self.verbose:
            if self.recv_dicts:
                print_str += "rank{} set up recv_dicts (src_ranks={})\n".format(self.rank, list(self.recv_dicts.keys()))
                for src_rank, recv_dict in self.recv_dicts.items():
                    print_str += recv_dict.__repr__(title="thread-safe dict (%s)"%("self queue" if src_rank == self.rank else "src_rank=%d"%src_rank )) + "\n"
            else: # empty
                print_str += "rank{} has NO recv job\n".format(self.rank)
        # setup number of ubatches in both sending and recving
        assert isinstance(ubatchszs_bwd,list)
        self.ubatchszs = ubatchszs_bwd
        if self.verbose: print_str += "rank{} set up ubatchszs = {}\n".format(self.rank, self.ubatchszs)
        # setup send and recv order
        self.ordering = ordering
        if ordering == 'layer-by-layer':
            self.send_order = None
            self.recv_orders = None

        # 7.
        # é»˜è®¤æ‰§è¡Œè¿™ä¸ª
        elif ordering == 'pack-by-pack':    
            # å°†è¦å‘é€çš„Xå¯¹åº”çš„layer idå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„ï¼Œæ”¾åˆ°ä¸€ä¸ªlistä¸­è¿”å›ï¼Œ[(layer_id,ubatchsize), ...]
            # å‘é€é¡ºåºï¼šå…ˆæ˜¯vtä¹‹é—´æœ‰åºï¼Œvtå†…ï¼ŒæŒ‰å„ä¸ªBWD ubsizeæ’åºï¼Œå¯¹æ¯ä¸ªubsizeï¼ŒæŒ‰vtè¦å‘é€çš„å±‚æ’åº
            # [(vt1's layer1,u1),(vt1's layer1,u1),(vt2's layer1,u2),(vt2's layer1,u2),...]
            self.send_order = self._find_send_order(rtasks)
            # æ‰¾åˆ°å‘é€Xåˆ°å½“å‰rankçš„ä»»åŠ¡ï¼Œæ”¾è¿›å­—å…¸ä¸­ï¼Œå­—å…¸çš„keyä¸ºsrc_rankï¼Œvalä¸ºä¸€ä¸ªlistï¼Œè£…ç€è¯¥rankä¸Šå¯¹åº”Xçš„layer_idå’Œ
            # ubatchsize_bwdã€‚{src_rankï¼š[(l,u),...], ...}
            # è¯¥å­—å…¸æ˜¯æŒ‰rankå·æ’åºçš„
            self.recv_orders = self._find_recv_order(rtasks)
            if self.verbose:
                print_str += "rank{} set up send_order = {}\n".format(self.rank, self.send_order)
                print_str += "rank{} set up recv_orders = {}\n".format(self.rank, self.recv_orders)
        else:
            raise ValueError
        # setup X_names
        self.layer_x_names = layer_x_names # { layer_id: X_names } # TODO: less stashing X after checking Identity chain --> stash X is always needed
        self.xmeta = xmeta # dictionary of TensorInfo
        
        if self.verbose: print(print_str)

    # 1.åˆ›å»ºä¸€ä¸ªå‘é€æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _send_helper_thread
    #   ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨
    #   send_dictä¸­ï¼Œä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
    #   è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
    #   æ–°çš„tesnor
    # 2.ä¸ºæ¯ä¸ª src_rankï¼Œå³ç»™å½“å‰rankå‘é€Xçš„rankï¼Œåˆ›å»ºä¸€ä¸ªæ¥æ”¶æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _recv_helper_thread
    #   ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
    #   å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥Xã€‚ğŸ“Œè‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
    def _start_helper_threads(self):
        """ Start helper communication threads, one for each queue. """
        # Setup send thread
        cnt_send_thd = 0
        # { layer_id: dst_rankï¼Œ... }
        if self.send_dict is not None:
            # åˆ›å»ºä¸€ä¸ªå‘é€æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _send_helper_thread
            # target å‚æ•°æŒ‡å®šäº†çº¿ç¨‹è¦è¿è¡Œçš„ç›®æ ‡å‡½æ•°ï¼Œå³åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œçš„å‡½æ•°
            # 
            # ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨
            # send_dictä¸­ï¼Œä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
            # è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
            # æ–°çš„tesnor
            helper_thread = threading.Thread(target=self._send_helper_thread)
            helper_thread.daemon = True
            # å¯åŠ¨è¿™ä¸ªçº¿ç¨‹
            helper_thread.start()
            cnt_send_thd += 1
        # Setup recv thread for each queue (excluding self queue)
        cnt_recv_thd = 0
        # ä¸ºæ¯ä¸ª src_rankï¼Œå³ç»™å½“å‰rankå‘é€Xçš„rankï¼Œåˆ›å»ºä¸€ä¸ªæ¥æ”¶æ•°æ®çš„è¾…åŠ©çº¿ç¨‹ _recv_helper_thread
        for src_rank in self.recv_dicts.keys():
            if src_rank != self.rank:
                # ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
                # å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥X
                helper_thread = threading.Thread(target=self._recv_helper_thread, args=(src_rank,))
                helper_thread.daemon = True
                helper_thread.start()
                cnt_recv_thd += 1
        # print("[MSGStashX] rank{} started send_helper_threadx{} & recv_helper_threadx{}".format(self.rank,cnt_send_thd,cnt_recv_thd))
    
    # 
    def _send_helper_thread(self):
        """ This method is to be executed from a helper daemon thread. """
        assert self.send_dict is not None # must be non-empty
        if self.ordering == "layer-by-layer":
            while True: # each tasks iteration
                for layer_id in self.send_dict.layer_ids: # in-order of FWD layers
                    for ubs in self.ubatchszs: 
                        named_tensors = self.send_dict.remove(layer_id)
                        dst_rank = self.send_ranks[layer_id]
                        if dst_rank == self.rank:
                            self._send2self(layer_id, named_tensors, self.pin_memory)
                        else:
                            self._send(layer_id, named_tensors, dst_rank)

        # ä¸æ–­çš„ä»å¤´å¼€å§‹éå† send_order ä¸­ä¿å­˜çš„layer_idï¼Œå³éå†æ‰€æœ‰è¦å‘é€çš„Xå¯¹åº”çš„layer idã€‚è¦å‘é€çš„tensoréƒ½ä¿å­˜åœ¨send_dictä¸­ï¼Œ
        # ä¸æ–­å°è¯•é€šè¿‡layer_idå–send_dictä¸­ä¿å­˜çš„tensorï¼Œå‘é€å‡ºå»
        # è¦å‘é€å°±è¦æŠŠtensorä»dictä¸­åˆ æ‰ï¼Œå³removeï¼Œè‹¥send_dicté”®layer_idå¯¹åº”çš„å€¼ä¸ºç©ºï¼Œremoveå‡½æ•°ä¼šé˜»å¡ä½ï¼Œç›´åˆ°å…¶ä¸­åŠ å…¥äº†
        # æ–°çš„tesnor
        elif self.ordering == "pack-by-pack":
            # [8,8,8,8]
            # [4,4,4,4,4,4,4,4]
            # 2 == 1 Ã— 8
            print(len(self.send_order))
            print(len(self.send_dict.layer_ids) * len(self.converted_ubwd))
            assert len(self.send_order) == len(self.send_dict.layer_ids) * len(self.converted_ubwd)
            while True: # each tasks iteration
                for layer_id, _ in self.send_order: # [(layer_id, ubatchsize)ï¼Œ...]
                    # print("[MSGStashX] rank{} wait L{}, send_dict=\n{}\n".format(self.rank, layer_id, self.send_dict))

                    # ä» self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
                    # { layer_id: [u1's {"name1": tensor1, "name2": [tensor2]}, u2's {}, ... ] }
                    named_tensors = self.send_dict.remove(layer_id)
                    # ç›®æ ‡rank
                    dst_rank = self.send_ranks[layer_id]
                    # è‹¥è¦å‘é€åˆ°çš„rankå°±æ˜¯å½“å‰rankï¼Œå°±æ˜¯è¦å‘é€ç»™è‡ªå·±
                    if dst_rank == self.rank:
                        self._send2self(layer_id, named_tensors)

                    # è°ƒç”¨ dist.send æ–¹æ³•å°†å¼ é‡å‘é€åˆ°dst_rank
                    else:
                        self._send(layer_id, named_tensors, dst_rank)
        else:
            raise ValueError
    
    # ç›´æ¥å°†tesnoræ”¾åˆ°åº•å±‚å­˜å‚¨ç»“æ„ä¸­ï¼Œè¾¾åˆ°rankå†…äº’ä¼ çš„æ•ˆæœ
    def _send2self(self, layer_id, named_tensors):
        """ Helper thread sends tensor to itself rank. """
        self.recv_dicts[self.rank].add(layer_id, named_tensors) 
        # print("[MSGStashX] rank{} _send2self enqueued (X{},{})".format(self.rank, layer_id, list(named_tensors.keys())))

    # è°ƒç”¨ dist.send æ–¹æ³•å°†å¼ é‡å‘é€åˆ°dst_rank
    def _send(self, layer_id, named_tensors, dst_rank):
        """ Helper thread sends tensor by calling dist.send(). """
        if self.nvprof: nvtx_range_push("__L{} MSG to dst{}".format(layer_id,dst_rank)) 
        # print("[MSGStashX] rank{} _sending L{} to dst{}".format(self.rank, layer_id, dst_rank))
        # named_metas = self.xmeta.get(1, layer_id) # { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
        for (name,tensor), name2 in zip(named_tensors.items(), self.layer_x_names[layer_id]): # { name: tensor/const, name: [tensors] }
            assert name == name2
            if isinstance(tensor, (torch.Tensor,Variable)):
                assert tensor.is_cuda and not tensor.requires_grad
                dist.send(tensor, dst_rank, self.group, self.send_tag)
            elif isinstance(tensor, (float,int)):
                dist.send(torch.tensor(tensor), dst_rank, self.group, self.send_tag)
            elif isinstance(tensor, list): # output tuple of bert pretrainhead 
                for t in tensor:
                    assert t.is_cuda and not t.requires_grad
                    dist.send(t, dst_rank, self.group, self.send_tag)
            else:
                raise ValueError("unknown tensor={}".format(tensor))
        if self.nvprof: nvtx_range_pop() 
        # print("[MSGStashX] rank{} _sent L{} to dst{}".format(self.rank, layer_id, dst_rank))
            
        
    # ä¸æ–­åœ°éå† self.recv_orders[src_rank]ï¼š{src_rankï¼š[(l,u),...], ...}
    # å³ä¸æ–­å°è¯•æ¥ä»src_rankæ¥æ”¶å¯¹åº”layer lçš„microbatchå¤§å°ä¸ºuçš„è¾“å…¥Xï¼Œå°†è¯»å–åˆ°çš„named_tensorsè¿åŒå…¶layer_id
    # æ„æˆä¸€ä¸ªå…ƒç»„åŠ å…¥åˆ° self.recv_dicts[src_rank] è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­
    def _recv_helper_thread(self, src_rank):
        """ This method is to be executed from a helper daemon thread. """
        assert src_rank != self.rank
        if self.ordering == "layer-by-layer":
            while True: # each tasks iteration
                for layer_id in self.recv_dicts[src_rank].layer_ids: # in-order of FWD layers
                    for ubs in self.ubatchszs: 
                        named_tensors = self._recv(layer_id, ubs, src_rank, self.pin_memory) 
                        self.recv_dicts[src_rank].add(layer_id, named_tensors)

        # æ˜¾ç„¶æ‰§è¡Œè¿™ä¸ª
        elif self.ordering == "pack-by-pack":
            if self.verbose: print("rank{}: _recv_helper_thread(src_rank={}): self.recv_orders={}, self.recv_dicts={}".format(self.rank, src_rank, self.recv_orders, self.recv_dicts))
            assert len(self.recv_orders[src_rank]) == len(self.recv_dicts[src_rank].layer_ids) * len(self.ubatchszs)
            while True: # each tasks iteration
                for layer_id, ubs in self.recv_orders[src_rank]: # [(layer_id, ubatchsize)]
                    # è°ƒç”¨dist.recvå‡½æ•°æ¥æ”¶tesnorï¼Œæ¥æ”¶çš„tensoræ”¾å…¥ä¸€ä¸ªODictä¸­è¿”å›
                    # ğŸ“Œè‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
                    named_tensors = self._recv(layer_id, ubs, src_rank, self.pin_memory) 
                    # å°†æ”¶åˆ°çš„æ•°æ®æ·»åŠ åˆ° recv_dicts[src_rank] è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­
                    self.recv_dicts[src_rank].add(layer_id, named_tensors)
        else:
            raise ValueError

    # è°ƒç”¨dist.recvå‡½æ•°æ¥æ”¶tensorï¼Œæ¥æ”¶çš„tensoræ”¾å…¥ä¸€ä¸ªODictä¸­è¿”å›
    def _recv(self, layer_id, ubatchsize, src_rank, pin_memory=True):
        """ Helper thread receives tensor by calling dist.recv(). """ 
        # print("[rank{}]\tmsg_handler._send: entered".format(self.rank))

        # è·å–è¦æ¥æ”¶çš„Xçš„å…ƒæ•°æ®ï¼Œéœ€è¦æ ¹æ®å…¶å¤§å°å’Œç±»å‹ç”Ÿæˆtensor
        named_metas = self.xmeta.get(ubatchsize, layer_id) # { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
        #
        named_tensors = ODict()
        # nameå³layer_idå±‚çš„è¾“å…¥åç§°
        for name in self.layer_x_names[layer_id]:
            meta = named_metas[name]
            if isinstance(meta, TensorMeta):
                tensor = torch.empty(meta.shape, dtype=meta.dtype, device="cpu", pin_memory=pin_memory)
                # å°†æ•°æ®å­˜å‚¨åœ¨åˆ›å»ºçš„å¼ é‡ä¸­
                # è‹¥æ²¡æœ‰æ”¶åˆ°æ•°æ®ä¼šé˜»å¡ä½
                # print(f"[rank{self.rank}]æ­£åœ¨æ¥æ”¶layer{layer_id}")
                dist.recv(tensor, src_rank, self.group, self.recv_tags[src_rank])
                named_tensors[name] = tensor 
            elif isinstance(meta, ConstMeta):
                tensor = torch.tensor(meta.const, device="cpu", pin_memory=pin_memory)
                dist.recv(tensor, src_rank, self.group, self.recv_tags[src_rank])
                named_tensors[name] = tensor.item() # convert a 0-dim tensor to a python number
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                named_tensors[name] = []
                for m in meta:
                    tensor = torch.empty(m.shape, dtype=m.dtype, device="cpu", pin_memory=pin_memory)
                    dist.recv(tensor, src_rank, self.group, self.recv_tags[src_rank])
                    named_tensors[name].append(tensor)
            else:
                raise ValueError("unknown meta={}".format(meta))
            # print("[rank{}]\tmsg_handler._send: tensor(shape={}, dtype={}) sent".format(self.rank, tensor.shape, tensor.dtype))
        return named_tensors
    
    # Call by upstream thread. Nonblocking send. 
    # å‘ send_dict è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­çš„ odict[layer_id] è¿™ä¸ªlistæ·»åŠ ï¼šself.odict[layer_id].append(named_tensors)
    def isend(self, layer_id, named_tensors):
        ''' Call by upstream thread. Nonblocking send. 
            The same API for two ordering of 'layer-by-layer' and 'pack-by-pack' '''
        self.send_dict.add(layer_id, named_tensors) # tuple uses reference to tensor

    # 1.æ‰¾åˆ°å¯¹åº”ç»™å®šlayer_idçš„src_rankï¼Œå³ä»å“ªä¸ªrankä¸Šä¼ Xè¿‡æ¥çš„
    # 2.ä»è¯¥src rankå¯¹åº”çš„çº¿ç¨‹å®‰å…¨å­—å…¸ä¸Šï¼Œå³ä»å…¶å†…éƒ¨çš„ self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
    #   å³ä¸€ä¸ª ï¼ˆname, tensorï¼‰ã€‚è‹¥å†…éƒ¨æ²¡æœ‰tensoræ˜¾ç„¶ä¼šè¢«é˜»å¡ä½(wait)
    def recv(self, layer_id):
        ''' Call by downstream thread. Blocking recv. Return named_tensors. '''
        src_rank = self.recv_ranks[layer_id] # { layer_id: src_rank } # can include self.rank
        # ä»è¯¥src rankå¯¹åº”çš„çº¿ç¨‹å®‰å…¨å­—å…¸ä¸Šï¼Œå³ä»å…¶å†…éƒ¨çš„ self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
        # è‹¥å†…éƒ¨æ²¡æœ‰tensoræ˜¾ç„¶ä¼šè¢«é˜»å¡ä½(wait)
        return self.recv_dicts[src_rank].remove(layer_id) # tuple uses reference to tensor    
    
    def has_no_send(self):
        return self.send_dict is None
    
    def has_no_recv(self):
        return False if self.recv_dicts else True

# ä¸“é—¨ç”¨äºæœ€åä¸€ä¸ªFWDvtå’Œé¦–ä¸ªBWDvtåœ¨ä¸€ä¸ªGPUä¸Šï¼Œæ­¤æ—¶Yçš„ä¼ è¾“åª’ä»‹ä¸ºSWPï¼ŒåŸæœ‰çš„MSGXæ— æ³•è¯†åˆ«è¿™ç§æƒ…å†µï¼Œå¯¼è‡´
# å‡ºç°MSGXå®ä¾‹è®¤ä¸ºæ—¢æ²¡æœ‰å‘é€ä¹Ÿæ²¡æœ‰æ¥æ”¶
class MSGX_2(MSGStashX_2):
    """ Handles gloo send/recv of Y/dX between cpu processes. 
        NOTE: Tentative for last fwd task to bwd criterion
        TODO: 1) To support all Y/dX; 2) replace input data structure to queue
    """
    def __init__(self, ubscvt, rank, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering='pack-by-pack', pin_memory=True, verbose=False, nvprof=False): 
        super().__init__(ubscvt, rank, rtasks, layer_x_names, xmeta, ubatchszs_bwd, ordering, pin_memory, verbose, nvprof)
        

    #############################################################################################################
    # å¯¹ä¸‹é¢ä¸¤ä¸ªç¡®å®šrankçš„æ–¹æ³•çš„æ€»ç»“ï¼šå¯è§æ˜¯è¿™æ ·ä¸€ç§åœºæ™¯ï¼Œåªæœ‰æœ€åä¸€ä¸ªFWDvtå‘ç¬¬ä¸€ä¸ªBWDvtå‘é€Yï¼Œä¸”åœ¨cpuä¸Šå‘é€ï¼Œå³åª’ä»‹ä¸ºMSG
    # å¹¶æ²¡æœ‰ç®¡dX
    #############################################################################################################
        
    # æ€»ç»“ï¼šæ‰¾åˆ°è¦æ¥æ”¶Xçš„ç›®æ ‡rankï¼Œå³é¦–ä¸ªBWDvtæ‰€åœ¨rankï¼Œå’Œè¯¥vté¦–å±‚ç»„æˆä¸€ä¸ªé”®å€¼å¯¹
    #
    # åœ¨å½“å‰rankä¸Šæ‰¾æœ€åä¸€ä¸ªFWD vtï¼Œè‹¥æœ‰ï¼Œè®°å½•è¯¥vtè¾“å‡ºYçš„ç›®æ ‡rankï¼ˆè¾“å‡ºåª’ä»‹å¿…é¡»æ˜¯MSGæ‰ä¼šè®°å½•ï¼‰
    #
    # è‹¥å½“å‰rankä¸­å­˜åœ¨æœ€åä¸€ä¸ªFWDä»»åŠ¡ä¸”è¾“å‡ºYçš„åª’ä»‹ä¸ºMSGï¼Œå‘send_rankså­—å…¸æ·»åŠ ä¸€ä¸ªé”®å€¼å¯¹ï¼š
    # {l(å±‚å·ï¼ŒğŸ“Œæ¥æ”¶è¾“å‡º Y çš„é‚£ä¸€å±‚ï¼Œæ˜¯l+1ä¸æ˜¯l)ï¼šrank(dst_rank)}
    # è¿”å› send_ranks å­—å…¸
    # ğŸ“Œåˆ†æï¼švPPä»»åŠ¡çš„æœ€åä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºYçš„åª’ä»‹ä¸ºP2Pï¼Œè¿™é‡Œçš„send_ranksåº”è¯¥æ˜¯ç©ºçš„
    def _find_send_ranks(self, rtasks):
        send_ranks = ODict()
        for vt in rtasks[self.rank]:
            # è‹¥æ˜¯æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œä¸”ä¼šè¾“å‡ºä¸€ä¸ªY
            if vt.is_last_fwd and vt.Out['Y']:
                # l ä¸ºè¾“å‡º Y çš„é‚£ä¸€å±‚çš„layer_id
                l = vt.layers[-1]
                m = vt.Out['Y'][l]
                # mä¸­ä¿å­˜çš„rankæœ¬æ¥å°±æ˜¯ dst_rank
                # ğŸ“Œåˆ†æ24/10/8ï¼šä¸ç®¡æ˜¯vPPè¿˜æ˜¯vDPï¼ŒOut['Y'][l]è¿™ä¸ªåª’ä»‹å°±ä¸å¯èƒ½æ˜¯MSG
                # âŒï¼šä»¥ä¸Šå›ç­”é”™è¯¯ï¼Œæœ‰è¿™ç§æƒ…å†µï¼Œå³å‰åæƒ³microbatchå¤§å°ä¸ä¸€æ ·
                print(f"MSGXå®ä¾‹, m.medium:{m.medium}")
                if m.medium == "SWP":
                    print(f"ç›®æ ‡rankä¸º{m.rank}")
                    send_ranks[l+1] = self.rank # dst_rank
        if self.verbose: print("[MSGX] found send_ranks={}".format(send_ranks))
        return send_ranks

    # æ€»ç»“ï¼šæ‰¾åˆ°äº§ç”ŸXå¹¶å‘é€å®ƒçš„æ¥æºrankï¼Œå³æœ€åä¸€ä¸ªFWDvtæ‰€åœ¨çš„rankï¼Œå’Œé¦–ä¸ªBWDvté¦–å±‚çš„å±‚å·ç»„æˆä¸€ä¸ªé”®å€¼å¯¹
    #
    # åœ¨å½“å‰rankä¸Šæ‰¾ç¬¬ä¸€ä¸ªBWD vtï¼Œè‹¥æœ‰ï¼Œè®°å½•ç»™è¯¥vtå‘é€è¾“å…¥Xçš„æ¥æºrankï¼ˆè¾“å…¥åª’ä»‹å¿…é¡»æ˜¯MSGæ‰ä¼šè®°å½•ï¼‰
    #
    # è‹¥å½“å‰rankä¸­å­˜åœ¨ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ï¼Œä¸”æ¥æ”¶çš„Xçš„åª’ä»‹ä¸ºMSGï¼Œå°†æ¥æ”¶è¯¥Xçš„layer_idå’Œsrc_rankäº’ç›¸ä½œä¸ºé”®å€¼å¯¹æ·»åŠ åˆ°ä¸¤ä¸ªå­—å…¸ä¸­å¹¶è¿”å›
    # è¿”å›ä¸¤ä¸ªå­—å…¸ï¼š
    # self.recv_ranks = { layer_idï¼ˆæ¥å—çš„å±‚idï¼‰: src_rankï¼ˆæ¥æºrankï¼‰ } # can include self.rank
    # self.recv_layers = { src_rank: [layer_id] } # can include self.rank
    # ğŸ“Œåˆ†æï¼švPPç¬¬ä¸€ä¸ªBWDä»»åŠ¡æ¥æ”¶è¾“å…¥Xçš„åª’ä»‹åº”è¯¥æ˜¯P2P,è¿™é‡Œçš„ä¸¤ä¸ªå­—å…¸åº”è¯¥æ˜¯ç©ºçš„
    def _find_recv_ranks_layers(self, rtasks):
        recv_ranks = ODict() # { layer_id: src_rank } # can include self.rank
        recv_layers = ODict() # { src_rank: [layer_id] } # can include self.rank
        for vt in rtasks[self.rank]:
            if vt.type == 'BWD' and vt.has_criterion and vt.In['X']:
                l = vt.layers[0]
                m = vt.In['X'][l]
                print(f"æ¥æºrankä¸º{m.rank}")
                if m.medium == "SWP":
                    # ä¸èƒ½ä½¿ç”¨m.rankèµ‹å€¼ï¼Œå› ä¸ºåª’ä»‹ä¸ºSWPçš„æƒ…å†µä¸‹ï¼Œm.rankä¸ºNoneï¼ˆåœ¨è¯¥åª’ä»‹å®ä¾‹åŒ–æ—¶ä¸ä¼šèµ‹å€¼ï¼‰
                    recv_ranks[l] = self.rank # src_rank
                    if self.rank not in recv_layers:
                        recv_layers[self.rank] = []
                    recv_layers[self.rank].append(l)
        if self.verbose: print("[MSGX] found recv_ranks={}, recv_layers={}".format(recv_ranks, recv_layers))
        return recv_ranks, recv_layers

    #############################################################################################################
    # å¯¹ä¸‹é¢ä¸¤ä¸ªorderæ–¹æ³•çš„æ€»ç»“ï¼šå¯¹ä¸€ä¸‹ä¸¤ä¸ªæ–¹æ³•çš„å‘½åå’Œä¸Šé¢ä¸¤ä¸ªæ–¹æ³•çš„å‘½åæ˜¯åè¿‡æ¥çš„ã€‚_find_send_ranksè™½ç„¶å‘½åæ–¹å¼æ˜¯
    # sendï¼Œä½†å…¶å®æ‰¾çš„æ˜¯æ¥æ”¶æ–¹ï¼Œå³ç›®æ ‡rankã€‚ä½†ä¸‹é¢è¿™ä¸ª_find_send_orderå°±æ˜¯ç¡®å®šå‘é€çš„é¡ºåºã€‚
    #############################################################################################################
      

    # ä¸ºæœ€åä¸€ä¸ªfwdä»»åŠ¡æ‰€åœ¨rankçš„MSGXå®ä¾‹é…ç½®å‘é€è¾“å‡ºYçš„é¡ºåºï¼ˆæœ€åä¸€ä¸ªfwdä»»åŠ¡å¹¶éçœŸæ­£çš„æœ€åä¸€ä¸ªï¼ŒçœŸæ­£çš„æœ€åä¸€ä¸ªåŒæ—¶ä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªbwdä»»åŠ¡ï¼‰
    #
    # åœ¨å½“å‰rankä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥å…¶è¾“å‡ºYçš„åª’ä»‹ä¸ºMSGï¼Œå°†â€œè¦æ¥æ”¶è¾“å‡ºYâ€çš„å±‚lå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„
    # åŠ å…¥åˆ°orderåˆ—è¡¨ä¸­ï¼Œ[(æ¥æ”¶Yçš„layer_id,ubatchsize), ...]ï¼Œæœ€åè¿”å›è¯¥åˆ—è¡¨
    # ğŸ“Œåˆ†æï¼švPPçš„æœ€åä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºYçš„åª’ä»‹ä¸ºP2Pï¼Œè¿”å›çš„ orders åº”è¯¥æ˜¯ç©ºçš„
    def _find_send_order(self, rtasks):
        """ find the send order of [(layer_id,ubatchsize)] from self rank. """
        order = []
        # åœ¨å½“å‰rankä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥å…¶è¾“å‡ºYçš„åª’ä»‹ä¸ºMSGï¼Œå°†è¦æ¥æ”¶è¾“å‡ºYçš„å±‚lå’Œæ¯ä¸€ä¸ªubatchszs_bwdç»„æˆä¸€ä¸ªå…ƒç»„
        # åŠ å…¥åˆ°orderåˆ—è¡¨ä¸­
        for vt in rtasks[self.rank]: # pre-run task graph at self rank
            if vt.is_last_fwd:
                for u in self.converted_ubwd:
                    l = vt.layers[-1]
                    m = vt.Out['Y'][l]
                    if m.medium == "SWP":
                        order.append((l+1,u))
        if self.verbose: print("[MSGX] found send order={}".format(order))
        return order
    
    # ä¸ºç¬¬ä¸€ä¸ªbwdä»»åŠ¡æ‰€åœ¨rankçš„MSGXå®ä¾‹é…ç½®æ¥æ”¶ï¼ˆç¬¬ä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºçš„Yçš„ï¼‰é¡ºåº
    #
    # åœ¨æ‰€æœ‰rankçš„ä»»åŠ¡ä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥è¯¥ä»»åŠ¡è¾“å‡ºçš„Yçš„åª’ä»‹ä¸ºMSGï¼Œä¸”ç›®æ ‡rankå°±æ˜¯å½“å‰rankï¼Œåˆ™å°†
    # (l+1(è¦æ¥æ”¶Yçš„å±‚)ï¼Œu)æ·»åŠ åˆ°src_rankå¯¹åº”çš„åˆ—è¡¨ä¸­ã€‚{src_rankï¼š[(æ¥æ”¶Yçš„l,u),...], ...}
    # ğŸ“Œåˆ†æï¼švPPçš„æœ€åä¸€ä¸ªfwdä»»åŠ¡è¾“å‡ºYçš„åª’ä»‹ä¸ºP2Pï¼Œè¿”å›çš„ orders åº”è¯¥æ˜¯ç©ºçš„
    def _find_recv_order(self, rtasks):
        """ find the recv orders of { src_rank : (layer_id,ubatchsize) } to self rank. """
        orders = ODict() # { src_rank: order }
        # åœ¨æ‰€æœ‰rankçš„ä»»åŠ¡ä¸Šå¯»æ‰¾æœ€åä¸€ä¸ªfwdä»»åŠ¡ï¼Œè‹¥è¯¥ä»»åŠ¡è¾“å‡ºçš„Yçš„åª’ä»‹ä¸ºMSGï¼Œä¸”ç›®æ ‡rankå°±æ˜¯å½“å‰rankï¼Œåˆ™å°†
        # (l+1(è¦æ¥æ”¶Yçš„å±‚)ï¼Œu)æ·»åŠ åˆ°src_rankå¯¹åº”çš„åˆ—è¡¨ä¸­
        for src_rank, vts in rtasks.items(): # pre-run task graph at all ranks
            for vt in vts:
                if vt.is_last_fwd:
                    for u in self.converted_ubwd:
                        l = vt.layers[-1]
                        m = vt.Out['Y'][l]
                        # 
                        if m.medium == "SWP":
                            # è‹¥æœ€åä¸€ä¸ªfwdä»»åŠ¡å‘é€Yçš„ç›®æ ‡rankå°±æ˜¯å½“å‰rank
                            if m.rank == self.rank: # can include self queue # and vt.rank != self.rank: 
                                if src_rank not in orders:
                                    orders[src_rank] = []
                                # 
                                orders[src_rank].append((l+1,u))
        if self.verbose: print("[MSGX] found recv orders={}".format(orders))
        return orders

class LocalX(object):
    """ Handles local X/dX in vDP. 
        Assumption:
            1) only CPU tensor and no grad
            2) equal microbatchsize from FWD to BWD (after UBatchSizeConverter)
    """
    def __init__(self, rank, layer_ids):
        self.rank = rank
        self.self_dict = threadsafe_data_struct.OrderedDictionary()
        self.self_dict.init_layer_ids(layer_ids)
        # print("[LocalStashX] rank{} created self_dict={}".format(self.rank, self.self_dict.__repr__(title="self queue")))
    
    def isend(self, layer_id, named_tensors):
        ''' Call by upstream thread. '''
        self.self_dict.add(layer_id, named_tensors) # tuple uses reference to tensor

    def recv(self, layer_id):
        ''' Call by downstream thread. Blocking recv. Return named_tensors. '''
        return self.self_dict.remove(layer_id) # tuple uses reference to tensor    

# ä¸“é—¨ç”¨æ¥æš‚å­˜stageçš„è¾“å‡º
# å…ˆå®Œæˆä¸€ç‰ˆä¸éœ€è¦æå‰å®šä¹‰é¡ºåºçš„
class CacheStashX(object):
    def __init__(self, rank, layer_ids, nvprof=False):
        self.rank = rank
        self.nvprof = nvprof
        # self.self_dict = ODict()
        assert isinstance(layer_ids, list)
        # for id in sorted(layer_ids): 
        #     self.self_dict[id] = []

        self.put_queue = threadsafe_data_struct.Queue()

    def put(self, layer_id, named_tensors):
        # self.self_dict[layer_id].append(named_tensors) # tuple uses reference to tensor
        self.put_queue.add((layer_id, named_tensors))

    def fetch(self, layer_id):
        # named_tensors = self.self_dict[layer_id].pop(0)
        # return named_tensors
        return self.put_queue.remove()