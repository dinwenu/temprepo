# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import os
import sys
from collections import OrderedDict as ODict
import threading

import torch
from torch.autograd import Variable

from torch.cuda.nvtx import range_push as nvtx_range_push 
from torch.cuda.nvtx import range_pop as nvtx_range_pop 

from prof_data_struct import ConstMeta, TensorMeta
from task_data_struct import Medium, vTask
import threadsafe_data_struct

# åªæœ‰å•ä¸ªGPUå’ŒvDPæ‰ä¼šç”¨è¿™ä¸ª
if os.environ.get('CUDA_LAUNCH_BLOCKING') in ['1','True', True]:
    MEMCPY_NONBLK = False
else:
    MEMCPY_NONBLK = True

""" Handles local swap-in/out of stashX/X/dX. 

    Assumption:
        0) stateless
        1) during swap, stashX/X/dX has no grad
"""

# 1.åœ¨cpuä¸Šçš„pinned memoryä¸Šåˆ›å»ºä¸€ä¸ªç©ºtensor
# 2.å¼‚æ­¥çš„å°†gpuä¸Šçš„tensoræ‹·è´åˆ°åˆšåˆšåˆ†é…çš„ç©ºtensorä¸Š
# è¿”å›cpu_named_tensors
@torch.no_grad()
def swapout(cuda_named_tensors, pin_memory=True): 
    """ Argument: cuda_named_tensors (StashX of vPP, LocalX of vDP)
        Return: cpu_named_tensors in pinned memory
    """
    cpu_named_tensors = ODict()
    for name,tensor in cuda_named_tensors.items(): # { name: tensor/const, name: [tensors] }
        if isinstance(tensor, (torch.Tensor,Variable)):
            assert tensor.is_cuda and not tensor.requires_grad
            # åœ¨cpuä¸Šçš„pinned memoryä¸Šåˆ›å»ºä¸€ä¸ªç©ºtensor
            cpu_named_tensors[name] = torch.empty(tensor.shape, dtype=tensor.dtype, device="cpu", pin_memory=pin_memory)
            # å¼‚æ­¥çš„å°†gpuä¸Šçš„tensoræ‹·è´åˆ°åˆšåˆšåˆ†é…çš„ç©ºtensorä¸Š
            cpu_named_tensors[name].copy_(tensor, non_blocking=MEMCPY_NONBLK) # inplace copy from cuda tensor to pinned memory 
            # assert cpu_named_tensors[name].is_pinned()
        elif isinstance(tensor, (float,int)):
            cpu_named_tensors[name] = tensor
        elif isinstance(tensor, list): # output tuple of bert pretrainhead 
            tmp = []
            for t in tensor:
                assert t.is_cuda and not t.requires_grad
                tmp.append( torch.empty(t.shape, dtype=t.dtype, device="cpu", pin_memory=pin_memory) )
                tmp[-1].copy_(t, non_blocking=MEMCPY_NONBLK)
                # assert tmp[-1].is_pinned()
            cpu_named_tensors[name] = tmp
        else:
            raise ValueError("unknown tensor={}".format(tensor))
    del cuda_named_tensors
    return cpu_named_tensors

# å°†å­—å…¸ä¸­çš„tensoræ›¿æ¢ä¸ºGPUç‰ˆæœ¬ï¼Œå³ç§»åŠ¨åˆ°GPUä¸Š
@torch.no_grad()
def swapin(cpu_named_tensors): 
    """ Argument: cpu_named_tensors (stashX of vPP, stashX/X/dX of vDP) 
        Return: cuda_named_tensors
    """
    cuda_named_tensors = ODict()
    for name,tensor in cpu_named_tensors.items(): # { name: tensor/const, name: [tensors] }
        if isinstance(tensor, (torch.Tensor,Variable)):
            assert not tensor.is_cuda and not tensor.requires_grad
            # assert tensor.is_pinned()
            # å°†tensorç§»åŠ¨åˆ°GPUä¸Š
            cuda_named_tensors[name] = tensor.cuda(non_blocking=MEMCPY_NONBLK) # to(device='cuda', non_blocking=True) # create new cuda tensor
        elif isinstance(tensor, (float,int)):
            cuda_named_tensors[name] = tensor
        elif isinstance(tensor, list): # output tuple of bert pretrainhead 
            tmp = []
            for t in tensor:
                assert not t.is_cuda and not t.requires_grad
                # assert t.is_pinned()
                tmp.append( t.cuda(non_blocking=MEMCPY_NONBLK) )
            cuda_named_tensors[name] = tmp
        else:
            raise ValueError("unknown tensor={}".format(tensor))
    # del cpu_named_tensors
    return cuda_named_tensors

""" Prefetch StashX/X/dX """

class SwapIn(object):
    """ Handle prefetch StashX in vPP/vDP and LocalX (X/dX) in vDP in background thread.
        Simliar to P2P.prerecv with double buffering.
        
        Step:
        1) main thread: waits for the running prefetch finish
        2) main thread: allocate or reuse buffer on GPU
        3) swapin thread: synchronize X on CPU
        4) swapin thread: copy in X in swapin_cudastream

        Assumption:
        0) statefull
        1) during swap, StashX/X/dX has no grad (but double buffering can have grad) 
        2) FIFO ordering. put each layerId, and get prefetched X. 
    """
    # sync_fnï¼šå³æ¥æ”¶stashXæˆ–local_Xçš„çº¿ç¨‹çš„recvå‡½æ•°
    def __init__(self, sync_fn, rank, swapin_stream=None, compute_stream=None, nvprof=False):
        self.sync_fn = sync_fn
        self.rank = rank
        self.swapin_stream = swapin_stream if swapin_stream is not None else torch.cuda.Stream(device=rank)
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        self.nvprof = nvprof
        assert self.rank == torch.cuda.current_device() 
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [ layerId, layerId, ...]  # between main and swapin thread
        self.get_queue = threadsafe_data_struct.Queue() # [ #0X, #1X, ...] # between swapin and main thread
        self.swapin_thread = threading.Thread(target=self._thread_func)
        self.swapin_thread.daemon = True
        self.swapin_thread.start()
        # for preget
        self.is_running = False
        self.ubatch_idx = int(0)
        self.double_bufs = [None, None]
        # print("[SwapIn] rank{} started swapin_thread".format(self.rank))
    
    # å°†is_runningç½®ä¸ºfalseï¼Œå¹¶å¼¹å‡ºçº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue ä¸­çš„é¦–ä¸ªå…ƒç´ 
    # è‹¥get_queueä¸­æ²¡æœ‰ä¸œè¥¿ï¼Œè¿™é‡Œå°±ä¼šä¸€ç›´ç­‰å¾…
    def _wait(self): 
        ''' Wait for the running swapin. Called in main thread.
            Assumption: only one swapin can be running.
            Return: swapined cuda_named_tensors.
        '''
        assert self.is_running, "no running swapin"
        self.is_running = False
        return self.get_queue.remove()

    # def _wait(self): # Deprecated: causing strange slowdown
    #     ''' Wait for the running swapin (iput). Called in main thread.
    #         Assumption: only one swapin can be running.
    #         Return: swapined cuda_named_tensors.
    #     '''
    #     assert self.is_running, "no running swapin"
    #     self.is_running = False
    #     # if self.nvprof: nvtx_range_push("L{} Wait(X)".format(layer_id)) 
    #     cuda_named_tensors, ev_swapin = self.get_queue.remove()
    #     self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
    #     # torch.cuda.default_stream(self.rank).synchronize() # wait Compute (PlanC)
    #     # if self.nvprof: nvtx_range_pop() 
    #     return cuda_named_tensors
    
    # åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
    # è‹¥ç»™å®šäº†bufferï¼Œåˆ™è¯¥å‡½æ•°ç›´æ¥è¿”å›bufferï¼Œä»€ä¹ˆä¹Ÿä¸åš
    def _allocate(self, layer_id, named_metas, buffer): 
        ''' Allocate or reuse the buffer for next swapin. Called in main thread.
            Argument: named_metas = XMETA.get(ubatchsize, layer_id) # { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                       buffer = allocated cuda_named_tensors
            Return: newly allocated or reused buffer
        '''
        if buffer is None: # allocate new tensors
            if self.nvprof: nvtx_range_push("L{} Alloc(X)".format(layer_id)) 
            cuda_named_tensors = ODict()
            for name, meta in named_metas.items():
                if isinstance(meta, TensorMeta):
                    assert meta.is_ubatch
                    cuda_named_tensors[name] = torch.empty(meta.shape, dtype=meta.dtype, device="cuda:%d"%self.rank) 
                elif isinstance(meta, ConstMeta): 
                    cuda_named_tensors[name] = meta.const
                elif isinstance(meta, list): # output tuple of bert pretrainhead 
                    cuda_named_tensors[name] = [ torch.empty(m.shape, dtype=m.dtype, device="cuda:%d"%self.rank) for m in meta ]
                else:
                    raise ValueError("unknown meta={}".format(meta))   
            if self.nvprof: nvtx_range_pop() 
            return cuda_named_tensors
        else: # reuse buffered tensors
            # TODO: confirm named_metas matches buffer
            return buffer

    # 1.æ£€æµ‹is_runningæ˜¯å¦ä¸ºfalseï¼Œå³ä¸å…è®¸æœ‰æ­£åœ¨æ‰§è¡Œçš„swap in
    # 2.å°† is_running ç½®ä¸ºtrue
    # 3.å°†(layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œè¿™æ„å‘³ç€SwapInçº¿ç¨‹ä¼šå¼€å§‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
    def _sync_copyin(self, layer_id, cuda_named_tensors, ev_compute=None):
        ''' Put allocated buffer to background swapin. Call by main thread thread. 
            Assumption: only one swapin can be running.
            Return: swapined cuda_named_tensors. '''
        assert not self.is_running, "the swapin is still running"
        self.is_running = True
        self.put_queue.add((layer_id, cuda_named_tensors, ev_compute))

    # å°†cpuä¸Šçš„tesnoræ‹·è´åˆ°gpuä¸Šçš„tensorã€‚è¯¥å‡½æ•°ä¼šè¿”å›ä¸€ä¸ªè®°å½•çš„äº‹ä»¶ï¼Œç”¨äºè®©compute streamç­‰å¾…
    @torch.no_grad() 
    def _copyin(self, cpu_named_tensors, cuda_named_tensors):
        ''' Call by background swapin thread.
            Argument: cpu_named_tensors = src buffer on CPU
                      cuda_named_tensors = dst buffers on GPU
            (Return: this cuda_named_tensors with filled data)
        '''
        #
        with torch.cuda.stream(self.swapin_stream): # context-manager selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.
            for (cname,ctensor), (gname, gtensor) in zip(cpu_named_tensors.items(), cuda_named_tensors.items()): # { name: tensor/const, name: [tensors] }
                assert cname == gname and type(ctensor) == type(gtensor)
                if isinstance(ctensor, (torch.Tensor,Variable)):
                    assert not ctensor.requires_grad, \
                    "{} (requires_grad:{})".format(ctensor, ctensor.requires_grad)
                    # assert ctensor.is_pinned() 
                    gtensor.data.copy_(ctensor.data, non_blocking=MEMCPY_NONBLK)
                elif isinstance(ctensor, (float,int)):
                    assert ctensor == gtensor
                elif isinstance(ctensor, list): # output tuple of bert pretrainhead 
                    for ct,gt in zip(ctensor,gtensor):
                        assert not ct.requires_grad
                        # assert ct.is_pinned()
                        gt.data.copy_(ct.data, non_blocking=MEMCPY_NONBLK)
                else:
                    raise ValueError("unknown tensor={}".format(tensor)) # ğŸ“Œä»£ç å†™é”™äº†ï¼Œctensor
        return self.swapin_stream.record_event() # record a swapin event in this stream for compute stream to wait
        # # wait for copy stream 
        # if MEMCPY_NONBLK: self.swapin_stream.synchronize() # Wait for all the kernels in this stream to complete. 
                
    # ä¸æ–­ä» put_queue ä¸­æ‹¿å– layer_id, cuda_named_tensors, ev_compute å¹¶æ‰§è¡Œï¼š
    # 1.å¦‚æœ ev_compute ä¸ä¸º Noneï¼Œåˆ™åœ¨å½“å‰ CUDA æµä¸Šç­‰å¾…è¯¥äº‹ä»¶çš„å®Œæˆã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨æ‰§è¡Œåç»­æ“ä½œä¹‹å‰ï¼Œ
    #   å¿…é¡»ç­‰å¾…å…ˆå‰çš„è®¡ç®—å®Œæˆ
    # 2.è°ƒç”¨ msg_stashx.recv æ–¹æ³•ï¼Œå³æ‹¿åˆ°ä»src_rankä¼ æ¥çš„ cpu_tensorï¼Œè‹¥æ²¡æœ‰tensorä¼šè¢«é˜»å¡ä½
    #   2.1.æ‰¾åˆ°å¯¹åº”ç»™å®šlayer_idçš„src_rankï¼Œå³ä»å“ªä¸ªrankä¸Šä¼ Xè¿‡æ¥çš„
    #   2.2.ä»è¯¥src rankå¯¹åº”çš„çº¿ç¨‹å®‰å…¨å­—å…¸ä¸Šï¼Œå³ä»å…¶å†…éƒ¨çš„ self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
    #       å³ä¸€ä¸ª ï¼ˆname, tensorï¼‰
    #       è‹¥å†…éƒ¨æ²¡æœ‰tensoræ˜¾ç„¶ä¼šè¢«é˜»å¡ä½(wait)
    # 3.å°†cpuä¸Šçš„tesnoræ‹·è´åˆ°gpuä¸Šçš„tensorã€‚è¯¥å‡½æ•°ä¼šè¿”å›ä¸€ä¸ªè®°å½•çš„äº‹ä»¶ï¼Œç”¨äºè®©compute streamç­‰å¾…
    # 4.å°† (cuda_named_tensors, ev_swapin) æ”¾å…¥ get_queue é˜Ÿåˆ—ä¸­
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each iput'ed element
            layer_id, cuda_named_tensors, ev_compute = self.put_queue.remove() # blk
            # å¦‚æœ ev_compute ä¸ä¸º Noneï¼Œåˆ™åœ¨å½“å‰ CUDA æµä¸Šç­‰å¾…è¯¥äº‹ä»¶çš„å®Œæˆã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨æ‰§è¡Œåç»­æ“ä½œä¹‹å‰ï¼Œ
            # å¿…é¡»ç­‰å¾…å…ˆå‰çš„è®¡ç®—å®Œæˆ
            if ev_compute is not None:
                self.swapin_stream.wait_event(ev_compute) # Stream waits for this event 
                # ev_compute.synchronize() # this CPU thread waits for this event. # Deprecated (too slow)
            # if self.nvprof: nvtx_range_push("__L{} SyncCopyIn(X)".format(layer_id)) 
            # sync
            #
            # sync_fnå³ msg_stashx.recv æ–¹æ³•
            # 1.æ‰¾åˆ°å¯¹åº”ç»™å®šlayer_idçš„src_rankï¼Œå³ä»å“ªä¸ªrankä¸Šä¼ Xè¿‡æ¥çš„
            # 2.ä»è¯¥src rankå¯¹åº”çš„çº¿ç¨‹å®‰å…¨å­—å…¸ä¸Šï¼Œå³ä»å…¶å†…éƒ¨çš„ self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
            #   å³ä¸€ä¸ª ï¼ˆname, tensorï¼‰
            #   è‹¥å†…éƒ¨æ²¡æœ‰tensoræ˜¾ç„¶ä¼šè¢«é˜»å¡ä½(wait)
            cpu_named_tensors = self.sync_fn(layer_id) # thread safe dict
            # copyin
            # å°†cpuä¸Šçš„tesnoræ‹·è´åˆ°gpuä¸Šçš„tensorã€‚è¯¥å‡½æ•°ä¼šè¿”å›ä¸€ä¸ªè®°å½•çš„äº‹ä»¶ï¼Œç”¨äºè®©compute streamç­‰å¾…
            ev_swapin = self._copyin(cpu_named_tensors, cuda_named_tensors)
            # ready to use
            # å°† (cuda_named_tensors, ev_swapin) æ”¾å…¥ get_queue é˜Ÿåˆ—ä¸­
            self.get_queue.add( (cuda_named_tensors, ev_swapin) )
            # clean up reference
            del cuda_named_tensors
            # if self.nvprof: nvtx_range_pop() 
   
    def fetch(self, layer_id, named_metas):
        ''' Blocking fetch current X on GPU. Call by main thread.
            Feature: 
            0) Stateless
            1) Blocking compute stream (otherwise fetch ubatches can drift from compute ubatches) by cuda events
        '''
        # record previous compute event for swapin stream to wait
        ev_compute = self.compute_stream.record_event()
        # fetch current one
        cuda_named_tensors = self._allocate(layer_id, named_metas, None)
        self._sync_copyin(layer_id, cuda_named_tensors, ev_compute)
        # if self.nvprof: nvtx_range_push("L{} Wait(X)".format(layer_id)) 
        cuda_named_tensors, ev_swapin = self._wait()
        self.compute_stream.wait_event(ev_swapin)
        # if self.nvprof: nvtx_range_pop() 
        return cuda_named_tensors # to be delete'd by runtime

    # 1.è‹¥æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„é¢„å–æ“ä½œ:
    #   1.1.åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
    #   1.2.å°† is_running ç½®ä¸ºtrue
    #   1.3.å°†(suc_layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œ
    #       è¿™æ„å‘³ç€SwapInçº¿ç¨‹ä¼šå¼€å§‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
    #   --å¦åˆ™ï¼Œè‹¥æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œç›´æ¥æ‰§è¡Œ2
    # 2.å°†is_runningç½®ä¸ºfalseï¼Œå¹¶å¼¹å‡ºçº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue ä¸­çš„é¦–ä¸ªå…ƒç´  (cur_named_tensors, ev_swapin)
    #   è‹¥get_queueä¸­æ²¡æœ‰ä¸œè¥¿ï¼Œè¿™é‡Œå°±ä¼šä¸€ç›´ç­‰å¾…
    # 3.ç­‰å¾…ev_swapinå®Œæˆ
    # 4.è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªmicrobatchçš„stashXçš„é¢„å–æ“ä½œï¼Œä¼šä¸ºä¸‹ä¸€ä¸ªmicroabtché¢„å–stashX
    def prefetch(self, layer_id, named_metas, is_end=False): 
        ''' Blocking wait current X and unblocking pre-swapin next X. Call by main thread. 
            Assumption: 
                      1) use double buffering for parallel compute and swapin
                      2) double buffering doesn't change in shape TODO: fix
                      3) PlanA: use cudaEvent to sync with compute stream, and event calls are still on CPU async w.r.t GPU streams
                      4) no prefetch for successor group's X
            Argument: layer_id = vLayer id of current StashX/X/dX
                      named_metas = current { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      is_end = whether ends pre-swapin after current one
            Return: recved current named_tensors. '''    
        # record previous compute event for swapin stream to wait
        ev_compute = self.compute_stream.record_event()
        # indexing double buffer
        cur_buf_idx = self.ubatch_idx % 2 # for compute
        next_buf_idx = (self.ubatch_idx+1) % 2 # for pre-swapin

        # wait current one (if no current one, have one)
        # ğŸ“Œä»ç¬¬äºŒä¸ªmicrobatchå¼€å§‹ï¼Œè¿™é‡Œæ‰ä¼šç•¥è¿‡ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªMicorbatché¢„å–äº†ç¬¬äºŒä¸ªçš„X
        # å› æ­¤ifä¸ºfalseï¼Œç›´æ¥æ‰§è¡Œä¸‹é¢çš„waitç­‰å¾…æ¥æ”¶å®Œæ¯•ï¼ˆä¹Ÿæœ‰å¯èƒ½å·²ç»æ¥æ”¶å®Œäº†ï¼‰
        if not self.is_running:
            # åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
            # è‹¥ç»™å®šäº†bufferï¼Œåˆ™è¯¥å‡½æ•°ç›´æ¥è¿”å›bufferï¼Œä»€ä¹ˆä¹Ÿä¸åš
            self.double_bufs[cur_buf_idx] = self._allocate(layer_id, named_metas, None)
            # 1.æ£€æµ‹is_runningæ˜¯å¦ä¸ºfalseï¼Œå³ä¸å…è®¸æœ‰æ­£åœ¨æ‰§è¡Œçš„swap in
            # 2.å°† is_running ç½®ä¸ºtrueï¼Œè¡¨ç¤ºSwapInå®ä¾‹çš„çº¿ç¨‹è¦å¼€å§‹å·¥ä½œäº†
            # 3.å°†(suc_layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œ
            #   ğŸ“Œè¿™æ„å‘³ç€ä¼šè§¦å‘SwapInçº¿ç¨‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
            #  ï¼ˆå°±æ˜¯å¾€self.double_bufs[cur_buf_idx]è¿™ä¸ªå­—å…¸ä¸­çš„tensoræ‹·è´ï¼‰
            self._sync_copyin(layer_id, self.double_bufs[cur_buf_idx], ev_compute)

        # if self.nvprof: nvtx_range_push("L{} Wait(X)".format(layer_id)) 
        # å°†is_runningç½®ä¸ºfalseï¼Œå¹¶å¼¹å‡ºçº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue ä¸­çš„é¦–ä¸ªå…ƒç´ 
        # è‹¥get_queueä¸­æ²¡æœ‰ä¸œè¥¿ï¼Œè¿™é‡Œå°±ä¼šä¸€ç›´ç­‰å¾…
        cur_named_tensors, ev_swapin = self._wait()
        # ç­‰å¾…swap inå®Œæˆ
        self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
        # if self.nvprof: nvtx_range_pop() 
        # pre-swapin next one if exsits
        # è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªmicrobatch
        if not is_end:
            # åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
            # è‹¥ç»™å®šäº†bufferï¼Œåˆ™è¯¥å‡½æ•°ç›´æ¥è¿”å›bufferï¼Œä»€ä¹ˆä¹Ÿä¸åš
            # ğŸ“Œæ³¨æ„ç¬¬ä¸€æ¬¡è¿è¡Œåˆ°è¿™é‡Œæ—¶ï¼Œè™½ç„¶å½¢å¼ä¸Šç»™äº†buffer(ç¬¬3ä¸ªå‚æ•°)ï¼Œä½†æœ¬è´¨ä¸Šä¼ è¿›å»çš„æ˜¯None
            self.double_bufs[next_buf_idx] = self._allocate(layer_id, named_metas, self.double_bufs[next_buf_idx])
            # è§¦å‘SwapInçº¿ç¨‹ä»cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
            # å³å½“å‰layerè¦æ¥æ”¶çš„ä¸‹ä¸€ä¸ªmicrobatchè¿è¡Œæ‰€éœ€çš„X
            self._sync_copyin(layer_id, self.double_bufs[next_buf_idx], ev_compute)
            self.ubatch_idx += 1
        else: # clean up
            self.ubatch_idx = 0
            del self.double_bufs 
            self.double_bufs = [None, None]
        return cur_named_tensors # reference only; to be deleted by runtime
    
    # suc_infoï¼š
    # è‹¥åç»§ä»»åŠ¡æ˜¯BWDï¼ˆéç¬¬ä¸€ä¸ªBWDï¼‰ï¼Œä¸”è¾“å…¥åª’ä»‹æ˜¯MSGï¼Œè¿”å› (l(åç»§ä»»åŠ¡çš„é¦–å±‚id), åçº§ä»»åŠ¡è¾“å…¥Xçš„å…ƒæ•°æ®) ã€‚éMSGç›´æ¥è¿”å›None
    # å…¶ä»–æƒ…å†µç›´æ¥è¿”å›None

    # 1.åœ¨é»˜è®¤çš„è®¡ç®—æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ï¼Œev_compute
    # 2.åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
    # 3.
    #   3.1.æ£€æµ‹is_runningæ˜¯å¦ä¸ºfalseï¼Œå³ä¸å…è®¸æœ‰æ­£åœ¨æ‰§è¡Œçš„swap in
    #   3.2.å°† is_running ç½®ä¸ºtrue
    #   3.3.å°†(layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œè¿™æ„å‘³ç€SwapInçº¿ç¨‹ä¼šå¼€å§‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
    def prefetch_suc(self, suc_info): 
        ''' Prefetc successor group's 1st ubatch if exists. Call by main thread. 
            Assumption: same 1) 3) above
            Argument: suc_info = None or successor group's (layer_id, named_metas)
        '''  
        if suc_info is None:
            return
        suc_layer_id, suc_named_metas = suc_info
        # if self.nvprof: nvtx_range_push("PrefetchSuc(L{}-X)".format(suc_layer_id)) 
        # must after is_end
        assert self.ubatch_idx == 0 and self.double_bufs == [None, None]
        # record previous compute event for swapin stream to wait
        ev_compute = self.compute_stream.record_event()
        # pre-swapin successor group's 1st ubatch if exsits
        # åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
        self.double_bufs[0] = self._allocate(suc_layer_id, suc_named_metas, None)
        # 1.æ£€æµ‹is_runningæ˜¯å¦ä¸ºfalseï¼Œå³ä¸å…è®¸æœ‰æ­£åœ¨æ‰§è¡Œçš„swap in
        # 2.å°† is_running ç½®ä¸ºtrue
        # 3.å°†(suc_layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œè¿™æ„å‘³ç€SwapInçº¿ç¨‹ä¼šå¼€å§‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
        self._sync_copyin(suc_layer_id, self.double_bufs[0], ev_compute)
        # if self.nvprof: nvtx_range_pop() 

# 
class SwapOut(object):
    """ Handle offload StashX in vPP/vDP and LocalX (X/dX) in vDP in background thread.
        
        Step:
        1) main thread: allocate pinned memory on CPU
        2) main thread: copy out X in swapout_stream
        3) main thread: optional delete
        4) swapout thread: wait for copy out
        5) swapout thread: isend to downstream ubatchconvert/msgstashx/localx

        Assumption:
        0) stateless
        1) during swap, StashX/X/dX has no grad
        2) FIFO ordering. put layer-by-layer, and swapout layer-by-layer. 
    """
    def __init__(self, output_fn, rank, swapout_stream=None, compute_stream=None, blocking=False, pin_memory=True, nvprof=False): # compute_stream2=None, 
        self.output_fn = output_fn # args: layer_id, named_tensor
        self.rank = rank
        self.swapout_stream = swapout_stream if swapout_stream is not None else torch.cuda.Stream(device=rank)
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        assert self.rank == torch.cuda.current_device() 
        self.blocking = blocking # é»˜è®¤ä¸ºfalse
        self.pin_memory = pin_memory
        self.nvprof = nvprof
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [ (layer_id, named_tensor, ev_compute), ...]  # between main and swapout thread
        # self.get_queue = threadsafe_data_struct.Queue() # [ ev_swapout, ...] # between swapout and main thread
        self.swapout_thread = threading.Thread(target=self._thread_func)
        self.swapout_thread.daemon = True
        self.swapout_thread.start()
        #
        # print("[SwapOut] rank{} started swapout_thread".format(self.rank))
    

    # flagä¼ è¿›æ¥çš„æ—¶å€™ä¸ºNone
    #
    # 1.è®°å½•ä¸€ä¸ªé»˜è®¤è®¡ç®—æµä¸Šçš„äº‹ä»¶ ev_compute
    # 2.åœ¨swapout streamä¸Šç­‰å¾… ev_compute äº‹ä»¶å®Œæˆï¼Œå³ç¡®ä¿è®¡ç®—å®Œæˆåæ‰èƒ½swapout
    # 3.åœ¨swapout_streamä¸Š:
    #   3.1.åœ¨cpuä¸Šçš„pinned memoryä¸Šåˆ›å»ºä¸€ä¸ªç©ºtensor
    #   3.2.å¼‚æ­¥çš„å°†gpuä¸Šçš„tensoræ‹·è´åˆ°åˆšåˆšåˆ†é…çš„ç©ºtensorä¸Š
    #   è¿”å›cpu_named_tensors
    # 4.å°† (layer_id, cpu_named_tensors, ev_swapout, flag) æ·»åŠ åˆ° put_queue é˜Ÿåˆ—ä¸­ã€‚ğŸ“Œè¿™æ„å‘³ç€å½“å‰å®ä¾‹çš„çº¿ç¨‹ä¼šå°†å·²ç»å¸è½½
    #   åˆ°cpuä¸Šçš„tensoræ”¾åˆ° MSGstashX å®ä¾‹çš„ send_dict ä¸­ã€‚
    #   ğŸ“Œè¿™ä¹Ÿæ„å‘³ç€ MSGstashX çš„å‘é€çº¿ç¨‹å°†å‘ dst_rank å‘é€æ­¤ tensor
    def offload(self, layer_id, cuda_named_tensors, flag=None):
        ''' Call by main thread. '''
        # record previous compute event for swapout stream to wait
        # 1.è®°å½•ä¸€ä¸ªé»˜è®¤è®¡ç®—æµä¸Šçš„äº‹ä»¶ ev_compute
        ev_compute = self.compute_stream.record_event()
        # Allocate and CopyOut and (Delete)
        # 2.åœ¨swapout streamä¸Šç­‰å¾… ev_compute äº‹ä»¶å®Œæˆï¼Œå³ç¡®ä¿è®¡ç®—å®Œæˆåæ‰èƒ½swapout
        self.swapout_stream.wait_event(ev_compute) # Stream waits for this event 
        # self.swapout_stream.wait_event(ev_compute2) # Stream waits for this event 
        # if self.nvprof: nvtx_range_push("L{} SwapOut(X)".format(layer_id)) 
        # 3.åœ¨swapout_streamä¸Š:
        #   3.1.åœ¨cpuä¸Šçš„pinned memoryä¸Šåˆ›å»ºä¸€ä¸ªç©ºtensor
        #   3.2.å¼‚æ­¥çš„å°†gpuä¸Šçš„tensoræ‹·è´åˆ°åˆšåˆšåˆ†é…çš„ç©ºtensorä¸Š
        #   è¿”å›cpu_named_tensors
        with torch.cuda.stream(self.swapout_stream): # context-manager selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.
            cpu_named_tensors = swapout(cuda_named_tensors, self.pin_memory)
        # åœ¨ swapout_stream ä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶
        ev_swapout = self.swapout_stream.record_event() # record a swapout event in this stream for compute stream to wait
        # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œä¸ç”¨ç®¡
        if self.blocking: # optional blocking
            self.compute_stream.wait_event(ev_swapout)
        # if self.nvprof: nvtx_range_pop() 
        # wait in background thread
        # å°† (layer_id, cpu_named_tensors, ev_swapout, flag) æ·»åŠ åˆ° put_queue é˜Ÿåˆ—ä¸­
        self.put_queue.add((layer_id, cpu_named_tensors, ev_swapout, flag))
            
    # ç¡®ä¿tensoråœ¨å®Œå…¨offloadåˆ°cpuåï¼Œå†æ·»åŠ åˆ°MSGXçº¿ç¨‹çš„send_dictä¸­ï¼Œå‘é€å‡ºå»
    # ä¸æ–­å°è¯•ä» put_queue è¿™ä¸ªçº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ä¸­æ‹¿å– layer_id, cpu_named_tensors, ev_swapout, flagï¼Œæ‰§è¡Œï¼š
    # 1.ç­‰å¾…ev_swapoutäº‹ä»¶æ‰§è¡Œå®Œæˆ
    # 2.è°ƒç”¨ output_fn å‡½æ•°
    #   2.1.è°ƒç”¨ MSGstashX å®ä¾‹çš„ isend æ–¹æ³•ï¼Œå‘ send_dict è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­çš„ odict[layer_id] è¿™ä¸ª
    #       listæ·»åŠ ï¼šself.odict[layer_id].append(named_tensors). è¿™æ„å‘³ç€MSGstashXå®ä¾‹çš„å‘é€çº¿ç¨‹å°†ä¼šå¼€å§‹å‘ç›®æ ‡rank
    #       å‘é€ tensor
    #   2.2.è°ƒç”¨UBatchSizeConverterçš„ isend æ–¹æ³•ï¼Œå°†layer_idå’Œinput2ï¼šcpu_named_tensorsåŠ å…¥åˆ° input_queue é˜Ÿåˆ—ä¸­ï¼Œè¿™æ„å‘³
    #       ç€ UBatchSizeConverter å®ä¾‹çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œtensorå¤§å°çš„è½¬æ¢ï¼Œè€Œåè¿˜æ˜¯è°ƒç”¨MSGstashXçš„isendæ–¹æ³•ï¼Œå°†convertå¥½çš„
    #       tensoråˆ—è¡¨åŠ å…¥åˆ°MSGstashXçš„send_dictä¸­ï¼Œåç»­å°±ä¸2.1ä¸€æ ·äº†
    #       
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each layer
            layer_id, cpu_named_tensors, ev_swapout, flag = self.put_queue.remove()
            # get ready for downstream
            # if self.nvprof: nvtx_range_push("__L{} WaitCopyOut(X)".format(layer_id)) 
            # if MEMCPY_NONBLK: self.swapout_stream.synchronize() # Wait for all the kernels in this stream to complete. 
            
            # é»˜è®¤ä¸ºéé˜»å¡ï¼Œè¿™é‡Œæ‰§è¡Œ
            # è‹¥éé˜»å¡ï¼Œè¿™é‡Œéœ€è¦ç­‰ä¸€ä¸‹ï¼Œev_swapout å¯èƒ½è¿˜æ²¡æ‰§è¡Œå®Œ
            if MEMCPY_NONBLK: ev_swapout.synchronize() # this CPU thread waits for this event. 
            
            # èµ·ç åœ¨vPPçš„åœºæ™¯ä¸‹ï¼Œflagå°±æ˜¯Noneï¼Œæ²¡å‘ç°ä»»ä½•å°†å…¶ç½®ä¸ºTrueçš„æ“ä½œ
            self.output_fn(layer_id, cpu_named_tensors) if flag is None else \
            self.output_fn(layer_id, cpu_named_tensors, flag)
            # if self.nvprof: nvtx_range_pop() 


# SwapInçš„å˜ä½“ç±»ï¼Œæ— éœ€GPUtensorçš„åˆ›å»ºå’ŒCPUâ†’GPUçš„å¤åˆ¶
class StashIn(object):
    """ Handle prefetch StashX in vPP/vDP and LocalX (X/dX) in vDP in background thread.
        Simliar to P2P.prerecv with double buffering.
        
        Step:
        1) main thread: waits for the running prefetch finish
        2) main thread: allocate or reuse buffer on GPU
        3) swapin thread: synchronize X on CPU
        4) swapin thread: copy in X in swapin_cudastream

        Assumption:
        0) statefull
        1) during swap, StashX/X/dX has no grad (but double buffering can have grad) 
        2) FIFO ordering. put each layerId, and get prefetched X. 
    """
    # sync_fnï¼šå³æ¥æ”¶stashXæˆ–local_Xçš„çº¿ç¨‹çš„recvå‡½æ•°
    def __init__(self, sync_fn, rank, swapin_stream=None, compute_stream=None, nvprof=False):
        self.sync_fn = sync_fn
        self.rank = rank
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        self.nvprof = nvprof
        assert self.rank == torch.cuda.current_device() 
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [ layerId, layerId, ...]  # between main and swapin thread
        self.get_queue = threadsafe_data_struct.Queue() # [ #0X, #1X, ...] # between swapin and main thread
        self.swapin_thread = threading.Thread(target=self._thread_func)
        self.swapin_thread.daemon = True
        self.swapin_thread.start()
        # for preget
        self.is_running = False
        self.ubatch_idx = int(0)
        self.double_bufs = [None, None]
        # print("[SwapIn] rank{} started swapin_thread".format(self.rank))
    
    # å°†is_runningç½®ä¸ºfalseï¼Œå¹¶å¼¹å‡ºçº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue ä¸­çš„é¦–ä¸ªå…ƒç´ 
    # è‹¥get_queueä¸­æ²¡æœ‰ä¸œè¥¿ï¼Œè¿™é‡Œå°±ä¼šä¸€ç›´ç­‰å¾…
    def _wait(self): 
        ''' Wait for the running swapin. Called in main thread.
            Assumption: only one swapin can be running.
            Return: swapined cuda_named_tensors.
        '''
        assert self.is_running, "no running swapin"
        self.is_running = False
        return self.get_queue.remove()

    # å•çº¯çš„æ£€æµ‹ä»¥ä¸‹æ˜¯å¦æ˜¯is_running+æ”¾è¿›put_queueä¸­
    def _sync_copyin(self, layer_id, ev_compute=None):
        ''' Put allocated buffer to background swapin. Call by main thread thread. 
            Assumption: only one swapin can be running.
            Return: swapined cuda_named_tensors. '''
        assert not self.is_running, "the swapin is still running"
        self.is_running = True
        self.put_queue.add((layer_id, ev_compute))

    # ç°åœ¨å•çº¯çš„ç­‰å¾…é»˜è®¤è®¡ç®—æµçš„æ‰§è¡Œå®Œæˆ+è°ƒç”¨åº•å±‚çš„å­˜å‚¨ç»“æ„æ‹¿æ•°æ®
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each iput'ed element
            layer_id, ev_compute = self.put_queue.remove() # blk
            # å¦‚æœ ev_compute ä¸ä¸º Noneï¼Œåˆ™åœ¨å½“å‰ CUDA æµä¸Šç­‰å¾…è¯¥äº‹ä»¶çš„å®Œæˆã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨æ‰§è¡Œåç»­æ“ä½œä¹‹å‰ï¼Œ
            # å¿…é¡»ç­‰å¾…å…ˆå‰çš„è®¡ç®—å®Œæˆ
            if ev_compute is not None:
                self.compute_stream.wait_event(ev_compute) # Stream waits for this event 
                # ev_compute.synchronize() # this CPU thread waits for this event. # Deprecated (too slow)
            # if self.nvprof: nvtx_range_push("__L{} SyncCopyIn(X)".format(layer_id)) 
            # sync
            #
            # sync_fnå³ msg_stashx.recv æ–¹æ³•
            # 1.æ‰¾åˆ°å¯¹åº”ç»™å®šlayer_idçš„src_rankï¼Œå³ä»å“ªä¸ªrankä¸Šä¼ Xè¿‡æ¥çš„
            # 2.ä»è¯¥src rankå¯¹åº”çš„çº¿ç¨‹å®‰å…¨å­—å…¸ä¸Šï¼Œå³ä»å…¶å†…éƒ¨çš„ self.odict[layer_id] åˆ—è¡¨ä¸­å¼¹å‡ºå¹¶è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
            #   å³ä¸€ä¸ª ï¼ˆname, tensorï¼‰
            #   è‹¥å†…éƒ¨æ²¡æœ‰tensoræ˜¾ç„¶ä¼šè¢«é˜»å¡ä½(wait)
            cuda_named_tensors = self.sync_fn(layer_id) # thread safe dict
            # ready to use
            self.get_queue.add( (cuda_named_tensors) )
            # clean up reference
            del cuda_named_tensors
            # if self.nvprof: nvtx_range_pop() 

    # åˆ é™¤äº†GPUä¸Štensorçš„åˆ†é…ï¼Œç›´æ¥é€šè¿‡_sync_copyinæ¿€æ´»çº¿ç¨‹å»æ•°æ®ç»“æ„æ‹¿æ•°æ®
    def fetch(self, layer_id):
        ''' Blocking fetch current X on GPU. Call by main thread.
            Feature: 
            0) Stateless
            1) Blocking compute stream (otherwise fetch ubatches can drift from compute ubatches) by cuda events
        '''
        # record previous compute event for swapin stream to wait
        ev_compute = self.compute_stream.record_event()
        # fetch current one
        self._sync_copyin(layer_id, ev_compute)
        # if self.nvprof: nvtx_range_push("L{} Wait(X)".format(layer_id)) 
        cuda_named_tensors = self._wait()
        # if self.nvprof: nvtx_range_pop() 
        return cuda_named_tensors # to be delete'd by runtime

    # 1.è‹¥æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„é¢„å–æ“ä½œ:
    #   1.1.åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
    #   1.2.å°† is_running ç½®ä¸ºtrue
    #   1.3.å°†(suc_layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œ
    #       è¿™æ„å‘³ç€SwapInçº¿ç¨‹ä¼šå¼€å§‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
    #   --å¦åˆ™ï¼Œè‹¥æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œç›´æ¥æ‰§è¡Œ2
    # 2.å°†is_runningç½®ä¸ºfalseï¼Œå¹¶å¼¹å‡ºçº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue ä¸­çš„é¦–ä¸ªå…ƒç´  (cur_named_tensors, ev_swapin)
    #   è‹¥get_queueä¸­æ²¡æœ‰ä¸œè¥¿ï¼Œè¿™é‡Œå°±ä¼šä¸€ç›´ç­‰å¾…
    # 3.ç­‰å¾…ev_swapinå®Œæˆ
    # 4.è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªmicrobatchçš„stashXçš„é¢„å–æ“ä½œï¼Œä¼šä¸ºä¸‹ä¸€ä¸ªmicroabtché¢„å–stashX
    def prefetch(self, layer_id, named_metas, is_end=False): 
        ''' Blocking wait current X and unblocking pre-swapin next X. Call by main thread. 
            Assumption: 
                      1) use double buffering for parallel compute and swapin
                      2) double buffering doesn't change in shape TODO: fix
                      3) PlanA: use cudaEvent to sync with compute stream, and event calls are still on CPU async w.r.t GPU streams
                      4) no prefetch for successor group's X
            Argument: layer_id = vLayer id of current StashX/X/dX
                      named_metas = current { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      is_end = whether ends pre-swapin after current one
            Return: recved current named_tensors. '''    
        # record previous compute event for swapin stream to wait
        ev_compute = self.compute_stream.record_event()
        # indexing double buffer
        cur_buf_idx = self.ubatch_idx % 2 # for compute
        next_buf_idx = (self.ubatch_idx+1) % 2 # for pre-swapin

        # wait current one (if no current one, have one)
        # ğŸ“Œä»ç¬¬äºŒä¸ªmicrobatchå¼€å§‹ï¼Œè¿™é‡Œæ‰ä¼šç•¥è¿‡ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªMicorbatché¢„å–äº†ç¬¬äºŒä¸ªçš„X
        # å› æ­¤ifä¸ºfalseï¼Œç›´æ¥æ‰§è¡Œä¸‹é¢çš„waitç­‰å¾…æ¥æ”¶å®Œæ¯•ï¼ˆä¹Ÿæœ‰å¯èƒ½å·²ç»æ¥æ”¶å®Œäº†ï¼‰
        if not self.is_running:
            # åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
            # è‹¥ç»™å®šäº†bufferï¼Œåˆ™è¯¥å‡½æ•°ç›´æ¥è¿”å›bufferï¼Œä»€ä¹ˆä¹Ÿä¸åš
            self.double_bufs[cur_buf_idx] = self._allocate(layer_id, named_metas, None)
            # 1.æ£€æµ‹is_runningæ˜¯å¦ä¸ºfalseï¼Œå³ä¸å…è®¸æœ‰æ­£åœ¨æ‰§è¡Œçš„swap in
            # 2.å°† is_running ç½®ä¸ºtrueï¼Œè¡¨ç¤ºSwapInå®ä¾‹çš„çº¿ç¨‹è¦å¼€å§‹å·¥ä½œäº†
            # 3.å°†(suc_layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œ
            #   ğŸ“Œè¿™æ„å‘³ç€ä¼šè§¦å‘SwapInçº¿ç¨‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
            #  ï¼ˆå°±æ˜¯å¾€self.double_bufs[cur_buf_idx]è¿™ä¸ªå­—å…¸ä¸­çš„tensoræ‹·è´ï¼‰
            self._sync_copyin(layer_id, self.double_bufs[cur_buf_idx], ev_compute)

        # if self.nvprof: nvtx_range_push("L{} Wait(X)".format(layer_id)) 
        # å°†is_runningç½®ä¸ºfalseï¼Œå¹¶å¼¹å‡ºçº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue ä¸­çš„é¦–ä¸ªå…ƒç´ 
        # è‹¥get_queueä¸­æ²¡æœ‰ä¸œè¥¿ï¼Œè¿™é‡Œå°±ä¼šä¸€ç›´ç­‰å¾…
        cur_named_tensors, ev_swapin = self._wait()
        # ç­‰å¾…swap inå®Œæˆ
        self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
        # if self.nvprof: nvtx_range_pop() 
        # pre-swapin next one if exsits
        # è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªmicrobatch
        if not is_end:
            # åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
            # è‹¥ç»™å®šäº†bufferï¼Œåˆ™è¯¥å‡½æ•°ç›´æ¥è¿”å›bufferï¼Œä»€ä¹ˆä¹Ÿä¸åš
            # ğŸ“Œæ³¨æ„ç¬¬ä¸€æ¬¡è¿è¡Œåˆ°è¿™é‡Œæ—¶ï¼Œè™½ç„¶å½¢å¼ä¸Šç»™äº†buffer(ç¬¬3ä¸ªå‚æ•°)ï¼Œä½†æœ¬è´¨ä¸Šä¼ è¿›å»çš„æ˜¯None
            self.double_bufs[next_buf_idx] = self._allocate(layer_id, named_metas, self.double_bufs[next_buf_idx])
            # è§¦å‘SwapInçº¿ç¨‹ä»cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
            # å³å½“å‰layerè¦æ¥æ”¶çš„ä¸‹ä¸€ä¸ªmicrobatchè¿è¡Œæ‰€éœ€çš„X
            self._sync_copyin(layer_id, self.double_bufs[next_buf_idx], ev_compute)
            self.ubatch_idx += 1
        else: # clean up
            self.ubatch_idx = 0
            del self.double_bufs 
            self.double_bufs = [None, None]
        return cur_named_tensors # reference only; to be deleted by runtime
    
    # suc_infoï¼š
    # è‹¥åç»§ä»»åŠ¡æ˜¯BWDï¼ˆéç¬¬ä¸€ä¸ªBWDï¼‰ï¼Œä¸”è¾“å…¥åª’ä»‹æ˜¯MSGï¼Œè¿”å› (l(åç»§ä»»åŠ¡çš„é¦–å±‚id), åçº§ä»»åŠ¡è¾“å…¥Xçš„å…ƒæ•°æ®) ã€‚éMSGç›´æ¥è¿”å›None
    # å…¶ä»–æƒ…å†µç›´æ¥è¿”å›None

    # 1.åœ¨é»˜è®¤çš„è®¡ç®—æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ï¼Œev_compute
    # 2.åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
    # 3.
    #   3.1.æ£€æµ‹is_runningæ˜¯å¦ä¸ºfalseï¼Œå³ä¸å…è®¸æœ‰æ­£åœ¨æ‰§è¡Œçš„swap in
    #   3.2.å°† is_running ç½®ä¸ºtrue
    #   3.3.å°†(layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œè¿™æ„å‘³ç€SwapInçº¿ç¨‹ä¼šå¼€å§‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
    def prefetch_suc(self, suc_info): 
        ''' Prefetc successor group's 1st ubatch if exists. Call by main thread. 
            Assumption: same 1) 3) above
            Argument: suc_info = None or successor group's (layer_id, named_metas)
        '''  
        if suc_info is None:
            return
        suc_layer_id, suc_named_metas = suc_info
        # if self.nvprof: nvtx_range_push("PrefetchSuc(L{}-X)".format(suc_layer_id)) 
        # must after is_end
        assert self.ubatch_idx == 0 and self.double_bufs == [None, None]
        # record previous compute event for swapin stream to wait
        ev_compute = self.compute_stream.record_event()
        # pre-swapin successor group's 1st ubatch if exsits
        # åœ¨GPUä¸ŠæŒ‰ç…§ç»™å®šçš„å…ƒæ•°æ®çš„shapeå’Œç±»å‹ç”Ÿæˆä¸€ä¸ªç©ºtensorï¼Œè£…è¿›namedå­—å…¸è¿”å›
        self.double_bufs[0] = self._allocate(suc_layer_id, suc_named_metas, None)
        # 1.æ£€æµ‹is_runningæ˜¯å¦ä¸ºfalseï¼Œå³ä¸å…è®¸æœ‰æ­£åœ¨æ‰§è¡Œçš„swap in
        # 2.å°† is_running ç½®ä¸ºtrue
        # 3.å°†(suc_layer_id, cuda_named_tensors, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼Œè¿™æ„å‘³ç€SwapInçº¿ç¨‹ä¼šå¼€å§‹cpuåˆ°gpuçš„æ‹·è´æ“ä½œ
        self._sync_copyin(suc_layer_id, self.double_bufs[0], ev_compute)
        # if self.nvprof: nvtx_range_pop() 

class StashOut(object):
    """ Handle offload StashX in vPP/vDP and LocalX (X/dX) in vDP in background thread.
        
        Step:
        1) main thread: allocate pinned memory on CPU
        2) main thread: copy out X in swapout_stream
        3) main thread: optional delete
        4) swapout thread: wait for copy out
        5) swapout thread: isend to downstream ubatchconvert/msgstashx/localx

        Assumption:
        0) stateless
        1) during swap, StashX/X/dX has no grad
        2) FIFO ordering. put layer-by-layer, and swapout layer-by-layer. 
    """
    def __init__(self, output_fn, rank, swapout_stream=None, compute_stream=None, blocking=False, pin_memory=True, nvprof=False): # compute_stream2=None, 
        self.output_fn = output_fn # args: layer_id, named_tensor
        self.rank = rank
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        assert self.rank == torch.cuda.current_device() 
        self.blocking = blocking # é»˜è®¤ä¸ºfalse
        self.pin_memory = pin_memory
        self.nvprof = nvprof
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [ (layer_id, named_tensor, ev_compute), ...]  # between main and swapout thread
        # self.get_queue = threadsafe_data_struct.Queue() # [ ev_swapout, ...] # between swapout and main thread
        self.swapout_thread = threading.Thread(target=self._thread_func)
        self.swapout_thread.daemon = True
        self.swapout_thread.start()
        #
        # print("[SwapOut] rank{} started swapout_thread".format(self.rank))
    
    # flagä¼ è¿›æ¥çš„æ—¶å€™ä¸ºNone
    #
    # ä»…ç”¨ä½œç­‰å¾…é»˜è®¤æµä¸­çš„è®¡ç®—å®Œæˆï¼Œä¸æ‰§è¡Œå¸è½½
    def offload(self, layer_id, cuda_named_tensors, flag=None):
        ''' Call by main thread. '''
        # record previous compute event for swapout stream to wait
        # 1.è®°å½•ä¸€ä¸ªé»˜è®¤è®¡ç®—æµä¸Šçš„äº‹ä»¶ ev_compute
        ev_compute = self.compute_stream.record_event()
        # Allocate and CopyOut and (Delete)
        # 2.åœ¨swapout streamä¸Šç­‰å¾… ev_compute äº‹ä»¶å®Œæˆï¼Œå³ç¡®ä¿è®¡ç®—å®Œæˆåæ‰èƒ½swapout
        self.compute_stream.wait_event(ev_compute) # Stream waits for this event 
        # if self.nvprof: nvtx_range_pop() 
        # wait in background thread
        # å°† (layer_id, cpu_named_tensors, ev_swapout, flag) æ·»åŠ åˆ° put_queue é˜Ÿåˆ—ä¸­
        self.put_queue.add((layer_id, cuda_named_tensors, flag))
            
    # ç¡®ä¿tensoråœ¨å®Œå…¨offloadåˆ°cpuåï¼Œå†æ·»åŠ åˆ°MSGXçº¿ç¨‹çš„send_dictä¸­ï¼Œå‘é€å‡ºå»
    # ä¸æ–­å°è¯•ä» put_queue è¿™ä¸ªçº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ä¸­æ‹¿å– layer_id, cpu_named_tensors, ev_swapout, flagï¼Œæ‰§è¡Œï¼š
    # 1.ç­‰å¾…ev_swapoutäº‹ä»¶æ‰§è¡Œå®Œæˆ
    # 2.è°ƒç”¨ output_fn å‡½æ•°
    #   2.1.è°ƒç”¨ MSGstashX å®ä¾‹çš„ isend æ–¹æ³•ï¼Œå‘ send_dict è¿™ä¸ªçº¿ç¨‹å®‰å…¨å­—å…¸ä¸­çš„ odict[layer_id] è¿™ä¸ª
    #       listæ·»åŠ ï¼šself.odict[layer_id].append(named_tensors). è¿™æ„å‘³ç€MSGstashXå®ä¾‹çš„å‘é€çº¿ç¨‹å°†ä¼šå¼€å§‹å‘ç›®æ ‡rank
    #       å‘é€ tensor
    #   2.2.è°ƒç”¨UBatchSizeConverterçš„ isend æ–¹æ³•ï¼Œå°†layer_idå’Œinput2ï¼šcpu_named_tensorsåŠ å…¥åˆ° input_queue é˜Ÿåˆ—ä¸­ï¼Œè¿™æ„å‘³
    #       ç€ UBatchSizeConverter å®ä¾‹çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œtensorå¤§å°çš„è½¬æ¢ï¼Œè€Œåè¿˜æ˜¯è°ƒç”¨MSGstashXçš„isendæ–¹æ³•ï¼Œå°†convertå¥½çš„
    #       tensoråˆ—è¡¨åŠ å…¥åˆ°MSGstashXçš„send_dictä¸­ï¼Œåç»­å°±ä¸2.1ä¸€æ ·äº†
    #       
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each layer
            layer_id, cuda_named_tensors, flag = self.put_queue.remove()
            # get ready for downstream
            # if self.nvprof: nvtx_range_push("__L{} WaitCopyOut(X)".format(layer_id)) 
            # if MEMCPY_NONBLK: self.swapout_stream.synchronize() # Wait for all the kernels in this stream to complete. 

            # èµ·ç åœ¨vPPçš„åœºæ™¯ä¸‹ï¼Œflagå°±æ˜¯Noneï¼Œæ²¡å‘ç°ä»»ä½•å°†å…¶ç½®ä¸ºTrueçš„æ“ä½œ
            self.output_fn(layer_id, cuda_named_tensors) if flag is None else \
            self.output_fn(layer_id, cuda_named_tensors, flag)
            # if self.nvprof: nvtx_range_pop() 
