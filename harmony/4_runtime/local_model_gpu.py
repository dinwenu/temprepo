# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import os
import copy
import gc
import threading
from math import ceil
from collections import OrderedDict as ODict

import torch
from torch.nn import Parameter

from torch.cuda.nvtx import range_push as nvtx_range_push 
from torch.cuda.nvtx import range_pop as nvtx_range_pop 

from task_data_struct import Medium, vTask
import threadsafe_data_struct

if os.environ.get('CUDA_LAUNCH_BLOCKING') in ['1','True', True]:
    MEMCPY_NONBLK = False
else:
    MEMCPY_NONBLK = True

# é€’å½’åœ°åˆ é™¤æ¨¡å—ï¼ˆåŒ…æ‹¬å­æ¨¡å—ï¼‰ä¸­çš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
def delete_param_grad_buf(top_module, manual_gc=False):
    ''' Recursively delete all params, grads, and buffers from this module on either CPU or GPU.

        Note: nn.Parameters (namely, Variable) is wrapper of torch.Tenosr. The core tensor can be accessed by param.data, but not recommended (access .data is source of all evils ref: https://discuss.pytorch.org/t/how-to-delete-every-grad-after-training/63644) 
        Note: Delete param? 
            -. param.data/param.data.storage() can not be del'ed
            -. for param in self.local_model_gpu.parameters(): del param # doesn't affect content
            -. param.data = None # TypeError: Variable data has to be a tensor, but got NoneType
            -. del moduel._parameters[key] will leave moduel._parameters[key]=None. Then have to new Parameter(). Then del new Parameter can cause uncollectable alloc on GPU.
            +. param.data = torch.empty(0, device="cpu") # use pytorch's current behavior -- in-place update and let python do the gc, working for both GPU and CPU (equal to del tensor)
        Note: Assign grad?
            -. param.data.grad can not be assigned 
            -. param.grad.data = only tensor, not None
            +. param.grad = * instead, 
            +. param.grad = None works for both GPU and CPU (equal to del tensor)
        Note: Delete buffer?
            +. del _buffer[key] # works
            +. _buffers[key] = fn(buf) # works
            +. buffer has no grad
    '''   
    @torch.no_grad()
    def fn(m): # m = each module
        for key, param in m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
            if param is not None:
                # delete param
                # åˆ é™¤å‚æ•°ï¼šåˆ›å»ºäº†ä¸€ä¸ªå½¢çŠ¶ä¸ºç©ºçš„å¼ é‡ï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨ CPU ä¸Šã€‚è¿™ä¸ªå¼ é‡æ²¡æœ‰ä»»ä½•å…ƒç´ ï¼Œå› ä¸ºå®ƒçš„å½¢çŠ¶æ˜¯ (0,)
                param.data = torch.empty(0, device="cpu")
                # delete grad
                # åˆ™å°†æ¢¯åº¦ç½®ä¸º Noneï¼Œç›¸å½“äºåˆ é™¤æ¢¯åº¦
                if param.grad is not None:
                    param.grad = None
                # å°†å‚æ•°ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä½¿å…¶æˆä¸ºå¶å­èŠ‚ç‚¹ï¼Œä»¥é˜²æ­¢æ¢¯åº¦ä¼ æ’­
                param.detach_() # in-place detaches self Tensor from the graph that created it, making it a leaf.
                assert not param.requires_grad
        # å¯¹äºæ¯ä¸ªæ¨¡å—çš„ç¼“å†²åŒºï¼Œé€šè¿‡ for key, buf in m._buffers.items() éå†æ¨¡å—çš„ç¼“å†²åŒºå­—å…¸ã€‚å¦‚æœç¼“å†²åŒºä¸ä¸º Noneï¼Œ
        # åˆ™å°†å…¶æ›¿æ¢ä¸ºä¸€ä¸ªç©ºçš„é›¶å¼ é‡ï¼Œç›¸å½“äºåˆ é™¤ç¼“å†²åŒºæ•°æ®
        for key, buf in m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
            if buf is not None:
                m._buffers[key] = torch.empty(0, device="cpu")
    # applyï¼šç”¨äºé€’å½’åœ°åº”ç”¨ä¸€ä¸ªå‡½æ•°åˆ°æ¨¡å—çš„æ¯ä¸ªå­æ¨¡å—ä¸Š
    top_module.apply(fn) # Applies ``fn`` recursively to every submodule (as returned by ``.children()`` as well as self.
    #
    if manual_gc:
        gc.collect(); torch.cuda.empty_cache() # can block all cudaStreams

class LocalModelGPU(object):
    """ A wrapper class of process-local vlayer on GPU.
    """
    # 1.ä¿é™©æ“ä½œï¼šç¡®ä¿pinned_modelï¼ˆå°±æ˜¯ä¸€ä¸ªvlayerï¼‰çš„å‚æ•°å’Œbufferéƒ½åœ¨å›ºå®šå†…å­˜ä¸­
    # 2.å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model å¯¹åº”çš„layerçš„dataä¸Š
    #   å³ç°åœ¨empty_modelçš„è¯¥å±‚layerå­˜åœ¨äºGPUä¸Š
    # 3.åˆ é™¤GPUä¸Šå½“å‰layerçš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
    def __init__(self, pinned_model, shared_model, empty_model, id, X_names, Y_names, rank=-1, world_size=-1, no_pin_model=False, no_pin_grad_buf=False):
        """ Call this in subprocess """ 
        self.pinned_model = pinned_model
        self.shared_model = shared_model
        self.id = id # vlayer_id
        self.X_names = X_names
        self.Y_names = Y_names
        self.rank, self.world_size = rank, world_size
        assert self.rank == torch.cuda.current_device()
        self.no_pin_model = no_pin_model
        self.no_pin_grad_buf = no_pin_grad_buf

        # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œè¿™é‡Œæ‰§è¡Œï¼š
        # ç¡®ä¿pinned_modelï¼ˆå°±æ˜¯ä¸€ä¸ªvlayerï¼‰çš„å‚æ•°å’Œbufferéƒ½åœ¨å›ºå®šå†…å­˜ä¸­
        if not self.no_pin_model:
            # confirm pinned model is pinned, local, CPU, and no grad
            for param in self.pinned_model.parameters():
                assert param.data.is_pinned() and (not param.data.is_shared()) and (not param.data.is_cuda)
                assert (not param.requires_grad) and (param.grad is None)
            for buf in self.pinned_model.buffers():
                assert buf.data.is_pinned() and (not buf.data.is_shared()) and (not buf.data.is_cuda)
                assert (not buf.requires_grad) and (buf.grad is None)
        
        # use existing empty model one
        assert isinstance(empty_model, torch.nn.Module)
        # ä»£è¡¨GPUä¸Šçš„æ¨¡å‹
        self.model = empty_model
        
        # self.model.cuda() # Moves all model parameters and buffers to the GPU. (replace CPU params and buffers with newly alloacated GPU param and buffers) Return self module.
        # å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model å¯¹åº”çš„layerçš„dataä¸Š
        # å³ç°åœ¨empty_modelçš„è¯¥å±‚layerå­˜åœ¨äºGPUä¸Š
        self.swapin_param_buf(True)
        # initialize empty shell on GPU
        # åˆ é™¤empty_modelå½“å‰layerä¸Šçš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
        self.del_param_grad_buf(manual_gc=True)
        # print("[LocalModelGPU][id%d] rank%d: initialized local model on GPU (empty shell)"%(self.id, self.rank))
    
    def del_param_grad_buf(self, manual_gc=False):
        # é€’å½’åœ°åˆ é™¤æ¨¡å—ï¼ˆåŒ…æ‹¬å­æ¨¡å—ï¼‰ä¸­çš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
        delete_param_grad_buf(self.model, manual_gc=manual_gc)
   
    # å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model
    # å³ç°åœ¨empty_modelå­˜åœ¨äºGPUä¸Š
    @torch.no_grad()
    def swapin_param_buf(self, forward_only=True): 
        ''' Recursively allocate and copy-in all params and buffers from cpu module to self.local_model_gpu
            
            Note: if gpu_m._parameters[key] is previously del'ed to None, then swapin needs to create a new Parameter. Then it may leave uncollectable allocation on GPU after del this new'ed Parameter.
        '''
        # ç”±äºno_pin_modelå‚æ•°é»˜è®¤ä¸ºfalseï¼Œè¿™é‡Œçš„modelæŒ‡ä»£å­˜æ”¾åœ¨å›ºå®šå†…å­˜ä¸­çš„model
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()): # iterator over all modules in the network
            # print("{} <- {}".format(gpu_m, cpu_m))
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    # å°† CPU æ¨¡å‹ä¸­çš„å‚æ•°æ•°æ®å¤åˆ¶åˆ° GPU æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨ GPU ä¸Šåˆ†é…å†…å­˜
                    gpu_m._parameters[key].data = param.cuda(non_blocking=MEMCPY_NONBLK) # Returns a copy of this tensor object in CUDA memory.
                    # assert gpu_m._parameters[key].grad is None and (not gpu_m._parameters[key].requires_grad), "swapin requires no grad for both FP and BP"
                    # if not forward_only: 
                    #     gpu_m._parameters[key].requires_grad_(True)
                    # assert not param.is_cuda
                    # print("\t _parameter[{}]".format(key))
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    # if len(buf.shape) == 0: # Scalar buffer stays on CPU
                    #     gpu_m._buffers[key] = buf.clone().detach().requires_grad_(False)
                    # else:
                    gpu_m._buffers[key] = buf.cuda(non_blocking=MEMCPY_NONBLK) # Returns a copy of this tensor object in CUDA memory.
                    # assert not buf.is_cuda and (not gpu_m._buffers[key].requires_grad)
                    # print("\t _buffers[{}]".format(key))
        # print("[LocalModelGPU] rank{} swapin'ed params and bufs".format(self.rank))
        
        if not forward_only: # backward # move to here for 1) batching swap on GPU, 2) GPU CPU parallelism
            self.set_param_requiresgrad()
    
    # æ§åˆ¶æ¨¡å‹çš„å‚æ•°æ˜¯å¦éœ€è¦æ¢¯åº¦
    @torch.no_grad()
    def set_param_requiresgrad(self, requires_grad=True): 
        ''' From def swapin_param_buf() '''
        for gpu_m in self.model.modules(): # iterator over all modules in the network
            for key, param in gpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    param.requires_grad_(requires_grad)

    # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„tensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
    @torch.no_grad()
    def alloc_param_buf(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    gpu_m._parameters[key].data = torch.empty(param.shape,  dtype=param.dtype, device=self.rank, requires_grad=False)
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key] = torch.empty(buf.shape,  dtype=buf.dtype, device=self.rank, requires_grad=False)
    
    # å°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
    @torch.no_grad()
    def copyin_param_buf(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    gpu_m._parameters[key].data.copy_(param.data, non_blocking=MEMCPY_NONBLK) # inplace copy
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key].data.copy_(buf.data, non_blocking=MEMCPY_NONBLK) # inplace copy

    @torch.no_grad()
    def copyin_param_buf_blocking(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                   gpu_m._parameters[key].data.copy_(param.data, non_blocking=False) # inplace copy
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key].data.copy_(buf.data, non_blocking=False) # inplace copy

    def __call__(self, *input, **kwargs):
        return self.model(*input, **kwargs)
        # ref: https://github.com/pytorch/pytorch/blob/472be69a736c0b2aece4883be9f8b18e2f3dfbbd/torch/nn/modules/module.py#L487
    
    def average_grad(self, p2p_handler):
        p2p_handler.average_gradients(self.model)

    def average_buf(self, p2p_handler):
        p2p_handler.average_buffers(self.model)

    # 1.åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
    # 2.å°†tensoræ‹·è´åˆ°pinned memoryä¸­
    # 3.å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
    # 4.ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Šï¼Œå³æ‹·è´åˆ°shared_modelä¸Š
    # shared_modelçš„param.gradå®é™…ä¸Šåœ¨pinned memoryä¸Š
    @torch.no_grad()
    def swapout_grad(self):
        ''' in-place copy gradient from local model gpu to local .grad on cpu 
            (lazily allocate local .grad if not exists)
        '''
        for gpu_param, cpu_param in zip(self.model.parameters(), self.shared_model.parameters()):
            if gpu_param.grad is not None:
                # è‹¥shared memoryç‰ˆæœ¬æ¨¡å‹ï¼ˆæ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„gradå±æ€§ä¸ºç©ºï¼Œä¸ºå…¶åˆ†é…ä¸€ä¸ªpinned 
                # memoryä¸Šçš„tensorä½œä¸ºå€¼
                if cpu_param.grad is None:
                    assert cpu_param.requires_grad
                    # åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                    g = torch.zeros(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False)
                    # å°†tensoræ‹·è´åˆ°pinned memoryä¸­
                    if not self.no_pin_grad_buf:
                        g = g.pin_memory()
                    # å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
                    cpu_param.grad = g
                    # cpu_param.grad = torch.empty(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False, pin_memory=not self.no_pin_grad_buf) # NOTE: dont' use empty(pin_memory) with shared memory 
                    assert not cpu_param.grad.is_shared() # and cpu_param.grad.is_pinned()
                # ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Š
                cpu_param.grad.data.copy_(gpu_param.grad.data, non_blocking=MEMCPY_NONBLK) # in-place copies the elements from src tensor into self tensor (and returns self)
            else:
                print(ValueError("model has grad = None to swap"))
                cpu_param.grad = None
        # print("[LocalModelGPU] rank{} swapout'ed grad".format(self.rank))

    # shared_modelçš„param.gradå®é™…ä¸Šåœ¨pinned memoryä¸Š
    @torch.no_grad()
    def swapout_grad_blocking(self):
        ''' in-place copy gradient from local model gpu to local .grad on cpu 
            (lazily allocate local .grad if not exists)
        '''
        for gpu_param, cpu_param in zip(self.model.parameters(), self.shared_model.parameters()):
            if gpu_param.grad is not None:
                # è‹¥shared memoryç‰ˆæœ¬æ¨¡å‹ï¼ˆæ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„gradå±æ€§ä¸ºç©ºï¼Œä¸ºå…¶åˆ†é…ä¸€ä¸ªpinned 
                # memoryä¸Šçš„tensorä½œä¸ºå€¼
                if cpu_param.grad is None:
                    assert cpu_param.requires_grad
                    # åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                    g = torch.zeros(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False)
                    # å°†tensoræ‹·è´åˆ°pinned memoryä¸­
                    if not self.no_pin_grad_buf:
                        g = g.pin_memory()
                    # å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
                    cpu_param.grad = g
                    # cpu_param.grad = torch.empty(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False, pin_memory=not self.no_pin_grad_buf) # NOTE: dont' use empty(pin_memory) with shared memory 
                    assert not cpu_param.grad.is_shared() # and cpu_param.grad.is_pinned()
                # ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Š
                cpu_param.grad.data.copy_(gpu_param.grad.data, non_blocking=False) # in-place copies the elements from src tensor into self tensor (and returns self)
            else:
                print(ValueError("model has grad = None to swap"))
                cpu_param.grad = None
        # print("[LocalModelGPU] rank{} swapout'ed grad".format(self.rank))

    # 1.è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
    # 2.å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
    # æœ€ç»ˆè¿™ä¸ªpinned_bufä¼šä»¥æˆå‘˜å˜é‡çš„å½¢å¼æŒ‚åœ¨shared_modelä¸Š
    @torch.no_grad()
    def swapout_buf(self): 
        ''' in-place copy buffers from local model gpu to local pinned buf or shared buf on cpu 
            (lazily allocate local pinned buf if not exists)
        '''
        if self.no_pin_grad_buf:
            for gpu_buf, cpu_buf in zip(self.model.buffers(), self.shared_model.buffers()):
                if gpu_buf is not None:
                    cpu_buf.data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK)
                else:
                    assert cpu_buf is None, "local_model_gpu has unexpected None buffer to swap"
        else:
            if not hasattr(self.shared_model, "pinned_buf"):
                self.shared_model.pinned_buf = ODict()
                # è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                for name, buf in self.shared_model.named_buffers():
                    if buf is not None:
                        pin_buf = torch.zeros(buf.shape, dtype=buf.dtype, device="cpu", requires_grad=False).pin_memory()
                        # NOTE: dont' use torch.empty(pin_memory) with shared memory 
                        assert not pin_buf.is_shared() and pin_buf.is_pinned()
                    else:
                        pin_buf = None
                    self.shared_model.pinned_buf[name] = pin_buf
                for (gpu_name, gpu_buf), (pin_name, pin_buf) in zip(self.model.named_buffers(), self.shared_model.pinned_buf.items()):
                    assert gpu_name == pin_name
                    assert (gpu_buf is not None and pin_buf is not None) or \
                           (gpu_buf is None and pin_buf is None) 
            named_pin_buf = self.shared_model.pinned_buf
            # å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
            for name, gpu_buf in self.model.named_buffers():
                if gpu_buf is not None:
                    named_pin_buf[name].data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK)
                else:
                    assert named_pin_buf[name] is None, "local_model_gpu has unexpected None buffer to swap"
                # å°±æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’å…¨ä¸º1çš„çŸ©é˜µï¼Œæ ¹æœ¬æ²¡å˜ï¼Œå¯è§ä¸æ˜¯å¯è®­ç»ƒçš„å‚æ•°
                # print(f"rank:{self.rank}, åˆšåˆšå¸è½½çš„bufferä¸º:{named_pin_buf[name]}")

        # for gpu_buf, cpu_buf in zip(self.model.buffers(), self.pinned_model.buffers()):
        #     cpu_buf.data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK) # Copies the elements from src tensor into self tensor (and returns self); in-place copy (no new tensor created)
        # print("[LocalModelGPU] rank{} swapout'ed buf".format(self.rank))

    @torch.no_grad()
    def swapout_buf_blocking(self): 
        ''' in-place copy buffers from local model gpu to local pinned buf or shared buf on cpu 
            (lazily allocate local pinned buf if not exists)
        '''
        if self.no_pin_grad_buf:
            for gpu_buf, cpu_buf in zip(self.model.buffers(), self.shared_model.buffers()):
                if gpu_buf is not None:
                    cpu_buf.data.copy_(gpu_buf.data, non_blocking=False)
                else:
                    assert cpu_buf is None, "local_model_gpu has unexpected None buffer to swap"
        else:
            if not hasattr(self.shared_model, "pinned_buf"):
                self.shared_model.pinned_buf = ODict()
                # è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                for name, buf in self.shared_model.named_buffers():
                    if buf is not None:
                        pin_buf = torch.zeros(buf.shape, dtype=buf.dtype, device="cpu", requires_grad=False).pin_memory()
                        # NOTE: dont' use torch.empty(pin_memory) with shared memory 
                        assert not pin_buf.is_shared() and pin_buf.is_pinned()
                    else:
                        pin_buf = None
                    self.shared_model.pinned_buf[name] = pin_buf
                for (gpu_name, gpu_buf), (pin_name, pin_buf) in zip(self.model.named_buffers(), self.shared_model.pinned_buf.items()):
                    assert gpu_name == pin_name
                    assert (gpu_buf is not None and pin_buf is not None) or \
                           (gpu_buf is None and pin_buf is None) 
            named_pin_buf = self.shared_model.pinned_buf
            # å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
            for name, gpu_buf in self.model.named_buffers():
                if gpu_buf is not None:
                    named_pin_buf[name].data.copy_(gpu_buf.data, non_blocking=False)
                else:
                    assert named_pin_buf[name] is None, "local_model_gpu has unexpected None buffer to swap"

    def train(self):
        self.model.train()

    def eval(self):
        self.model.eval()


class LocalModelGPU_2(object):
    """ A wrapper class of process-local vlayer on GPU.
    """
    # 1.ä¿é™©æ“ä½œï¼šç¡®ä¿pinned_modelï¼ˆå°±æ˜¯ä¸€ä¸ªvlayerï¼‰çš„å‚æ•°å’Œbufferéƒ½åœ¨å›ºå®šå†…å­˜ä¸­
    # 2.å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model å¯¹åº”çš„layerçš„dataä¸Š
    #   å³ç°åœ¨empty_modelçš„è¯¥å±‚layerå­˜åœ¨äºGPUä¸Š
    # 3.åˆ é™¤GPUä¸Šå½“å‰layerçš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
    def __init__(self, pinned_model, shared_model, empty_model, id, X_names, Y_names, rank=-1, world_size=-1, no_pin_model=False, no_pin_grad_buf=False):
        """ Call this in subprocess """ 
        self.pinned_model = pinned_model
        self.shared_model = shared_model
        self.id = id # vlayer_id
        self.X_names = X_names
        self.Y_names = Y_names
        self.rank, self.world_size = rank, world_size
        assert self.rank == torch.cuda.current_device()
        self.no_pin_model = no_pin_model
        self.no_pin_grad_buf = no_pin_grad_buf

        # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œè¿™é‡Œæ‰§è¡Œï¼š
        # ç¡®ä¿pinned_modelï¼ˆå°±æ˜¯ä¸€ä¸ªvlayerï¼‰çš„å‚æ•°å’Œbufferéƒ½åœ¨å›ºå®šå†…å­˜ä¸­
        if not self.no_pin_model:
            # confirm pinned model is pinned, local, CPU, and no grad
            for param in self.pinned_model.parameters():
                assert param.data.is_pinned() and (not param.data.is_shared()) and (not param.data.is_cuda)
                assert (not param.requires_grad) and (param.grad is None)
            for buf in self.pinned_model.buffers():
                assert buf.data.is_pinned() and (not buf.data.is_shared()) and (not buf.data.is_cuda)
                assert (not buf.requires_grad) and (buf.grad is None)
        
        # use existing empty model one
        assert isinstance(empty_model, torch.nn.Module)
        # ä»£è¡¨GPUä¸Šçš„æ¨¡å‹
        self.model = empty_model
        
        # self.model.cuda() # Moves all model parameters and buffers to the GPU. (replace CPU params and buffers with newly alloacated GPU param and buffers) Return self module.
        # å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model å¯¹åº”çš„layerçš„dataä¸Š
        # å³ç°åœ¨empty_modelçš„è¯¥å±‚layerå­˜åœ¨äºGPUä¸Š
        self.swapin_param_buf(True)
        # initialize empty shell on GPU
        # åˆ é™¤empty_modelå½“å‰layerä¸Šçš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
        self.del_param_grad_buf(manual_gc=True)
        # print("[LocalModelGPU][id%d] rank%d: initialized local model on GPU (empty shell)"%(self.id, self.rank))
    
    def del_param_grad_buf(self, manual_gc=False):
        # é€’å½’åœ°åˆ é™¤æ¨¡å—ï¼ˆåŒ…æ‹¬å­æ¨¡å—ï¼‰ä¸­çš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
        delete_param_grad_buf(self.model, manual_gc=manual_gc)
   
    # å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model
    # å³ç°åœ¨empty_modelå­˜åœ¨äºGPUä¸Š
    @torch.no_grad()
    def swapin_param_buf(self, forward_only=True): 
        ''' Recursively allocate and copy-in all params and buffers from cpu module to self.local_model_gpu
            
            Note: if gpu_m._parameters[key] is previously del'ed to None, then swapin needs to create a new Parameter. Then it may leave uncollectable allocation on GPU after del this new'ed Parameter.
        '''
        # ç”±äºno_pin_modelå‚æ•°é»˜è®¤ä¸ºfalseï¼Œè¿™é‡Œçš„modelæŒ‡ä»£å­˜æ”¾åœ¨å›ºå®šå†…å­˜ä¸­çš„model
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()): # iterator over all modules in the network
            # print("{} <- {}".format(gpu_m, cpu_m))
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    # å°† CPU æ¨¡å‹ä¸­çš„å‚æ•°æ•°æ®å¤åˆ¶åˆ° GPU æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨ GPU ä¸Šåˆ†é…å†…å­˜
                    gpu_m._parameters[key].data = param.cuda(non_blocking=MEMCPY_NONBLK) # Returns a copy of this tensor object in CUDA memory.
                    # assert gpu_m._parameters[key].grad is None and (not gpu_m._parameters[key].requires_grad), "swapin requires no grad for both FP and BP"
                    # if not forward_only: 
                    #     gpu_m._parameters[key].requires_grad_(True)
                    # assert not param.is_cuda
                    # print("\t _parameter[{}]".format(key))
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    # if len(buf.shape) == 0: # Scalar buffer stays on CPU
                    #     gpu_m._buffers[key] = buf.clone().detach().requires_grad_(False)
                    # else:
                        gpu_m._buffers[key] = buf.cuda(non_blocking=MEMCPY_NONBLK) # Returns a copy of this tensor object in CUDA memory.
                    # assert not buf.is_cuda and (not gpu_m._buffers[key].requires_grad)
                    # print("\t _buffers[{}]".format(key))
        # print("[LocalModelGPU] rank{} swapin'ed params and bufs".format(self.rank))
        
        if not forward_only: # backward # move to here for 1) batching swap on GPU, 2) GPU CPU parallelism
            self.set_param_requiresgrad()
    
    # æ§åˆ¶æ¨¡å‹çš„å‚æ•°æ˜¯å¦éœ€è¦æ¢¯åº¦
    @torch.no_grad()
    def set_param_requiresgrad(self, requires_grad=True): 
        ''' From def swapin_param_buf() '''
        for gpu_m in self.model.modules(): # iterator over all modules in the network
            for key, param in gpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    param.requires_grad_(requires_grad)

    # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„tensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
    @torch.no_grad()
    def alloc_param_buf(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    gpu_m._parameters[key].data = torch.empty(param.shape,  dtype=param.dtype, device=self.rank, requires_grad=False)
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key] = torch.empty(buf.shape,  dtype=buf.dtype, device=self.rank, requires_grad=False)
    
    # å°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
    @torch.no_grad()
    def copyin_param_buf(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    gpu_m._parameters[key].data.copy_(param.data, non_blocking=MEMCPY_NONBLK) # inplace copy
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key].data.copy_(buf.data, non_blocking=MEMCPY_NONBLK) # inplace copy

    @torch.no_grad()
    def copyin_param_buf_blocking(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                   gpu_m._parameters[key].data.copy_(param.data, non_blocking=False) # inplace copy
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key].data.copy_(buf.data, non_blocking=False) # inplace copy

    def __call__(self, *input, **kwargs):
        return self.model(*input, **kwargs)
        # ref: https://github.com/pytorch/pytorch/blob/472be69a736c0b2aece4883be9f8b18e2f3dfbbd/torch/nn/modules/module.py#L487
    
    def average_grad(self, p2p_handler):
        p2p_handler.average_gradients(self.model)

    def average_buf(self, p2p_handler):
        p2p_handler.average_buffers(self.model)

    # 1.åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
    # 2.å°†tensoræ‹·è´åˆ°pinned memoryä¸­
    # 3.å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
    # 4.ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Šï¼Œå³æ‹·è´åˆ°shared_modelä¸Š
    # shared_modelçš„param.gradå®é™…ä¸Šåœ¨pinned memoryä¸Š
    @torch.no_grad()
    def swapout_grad(self):
        ''' in-place copy gradient from local model gpu to local .grad on cpu 
            (lazily allocate local .grad if not exists)
        '''
        for gpu_param, cpu_param in zip(self.model.parameters(), self.shared_model.parameters()):
            if gpu_param.grad is not None:
                # è‹¥shared memoryç‰ˆæœ¬æ¨¡å‹ï¼ˆæ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„gradå±æ€§ä¸ºç©ºï¼Œä¸ºå…¶åˆ†é…ä¸€ä¸ªpinned 
                # memoryä¸Šçš„tensorä½œä¸ºå€¼
                if cpu_param.grad is None:
                    assert cpu_param.requires_grad
                    # åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                    g = torch.zeros(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False)
                    # å°†tensoræ‹·è´åˆ°pinned memoryä¸­
                    if not self.no_pin_grad_buf:
                        g = g.pin_memory()
                    # å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
                    cpu_param.grad = g
                    # cpu_param.grad = torch.empty(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False, pin_memory=not self.no_pin_grad_buf) # NOTE: dont' use empty(pin_memory) with shared memory 
                    assert not cpu_param.grad.is_shared() # and cpu_param.grad.is_pinned()
                # ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Š
                cpu_param.grad.data.copy_(gpu_param.grad.data, non_blocking=MEMCPY_NONBLK) # in-place copies the elements from src tensor into self tensor (and returns self)
            else:
                print(ValueError("model has grad = None to swap"))
                cpu_param.grad = None
        # print("[LocalModelGPU] rank{} swapout'ed grad".format(self.rank))

    # shared_modelçš„param.gradå®é™…ä¸Šåœ¨pinned memoryä¸Š
    @torch.no_grad()
    def swapout_grad_blocking(self):
        ''' in-place copy gradient from local model gpu to local .grad on cpu 
            (lazily allocate local .grad if not exists)
        '''
        for gpu_param, cpu_param in zip(self.model.parameters(), self.shared_model.parameters()):
            if gpu_param.grad is not None:
                # è‹¥shared memoryç‰ˆæœ¬æ¨¡å‹ï¼ˆæ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„gradå±æ€§ä¸ºç©ºï¼Œä¸ºå…¶åˆ†é…ä¸€ä¸ªpinned 
                # memoryä¸Šçš„tensorä½œä¸ºå€¼
                if cpu_param.grad is None:
                    assert cpu_param.requires_grad
                    # åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                    g = torch.zeros(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False)
                    # å°†tensoræ‹·è´åˆ°pinned memoryä¸­
                    if not self.no_pin_grad_buf:
                        g = g.pin_memory()
                    # å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
                    cpu_param.grad = g
                    # cpu_param.grad = torch.empty(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False, pin_memory=not self.no_pin_grad_buf) # NOTE: dont' use empty(pin_memory) with shared memory 
                    assert not cpu_param.grad.is_shared() # and cpu_param.grad.is_pinned()
                # ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Š
                cpu_param.grad.data.copy_(gpu_param.grad.data, non_blocking=False) # in-place copies the elements from src tensor into self tensor (and returns self)
            else:
                print(ValueError("model has grad = None to swap"))
                cpu_param.grad = None
        # print("[LocalModelGPU] rank{} swapout'ed grad".format(self.rank))

    # 1.è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
    # 2.å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
    # æœ€ç»ˆè¿™ä¸ªpinned_bufä¼šä»¥æˆå‘˜å˜é‡çš„å½¢å¼æŒ‚åœ¨shared_modelä¸Š
    @torch.no_grad()
    def swapout_buf(self): 
        ''' in-place copy buffers from local model gpu to local pinned buf or shared buf on cpu 
            (lazily allocate local pinned buf if not exists)
        '''
        if self.no_pin_grad_buf:
            for gpu_buf, cpu_buf in zip(self.model.buffers(), self.shared_model.buffers()):
                if gpu_buf is not None:
                    cpu_buf.data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK)
                else:
                    assert cpu_buf is None, "local_model_gpu has unexpected None buffer to swap"
        else:
            if not hasattr(self.shared_model, "pinned_buf"):
                self.shared_model.pinned_buf = ODict()
                # è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                for name, buf in self.shared_model.named_buffers():
                    if buf is not None:
                        pin_buf = torch.zeros(buf.shape, dtype=buf.dtype, device="cpu", requires_grad=False).pin_memory()
                        # NOTE: dont' use torch.empty(pin_memory) with shared memory 
                        assert not pin_buf.is_shared() and pin_buf.is_pinned()
                    else:
                        pin_buf = None
                    self.shared_model.pinned_buf[name] = pin_buf
                for (gpu_name, gpu_buf), (pin_name, pin_buf) in zip(self.model.named_buffers(), self.shared_model.pinned_buf.items()):
                    assert gpu_name == pin_name
                    assert (gpu_buf is not None and pin_buf is not None) or \
                           (gpu_buf is None and pin_buf is None) 
            named_pin_buf = self.shared_model.pinned_buf
            # å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
            for name, gpu_buf in self.model.named_buffers():
                if gpu_buf is not None:
                    named_pin_buf[name].data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK)
                else:
                    assert named_pin_buf[name] is None, "local_model_gpu has unexpected None buffer to swap"

        # for gpu_buf, cpu_buf in zip(self.model.buffers(), self.pinned_model.buffers()):
        #     cpu_buf.data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK) # Copies the elements from src tensor into self tensor (and returns self); in-place copy (no new tensor created)
        # print("[LocalModelGPU] rank{} swapout'ed buf".format(self.rank))

    @torch.no_grad()
    def swapout_buf_blocking(self): 
        ''' in-place copy buffers from local model gpu to local pinned buf or shared buf on cpu 
            (lazily allocate local pinned buf if not exists)
        '''
        if self.no_pin_grad_buf:
            for gpu_buf, cpu_buf in zip(self.model.buffers(), self.shared_model.buffers()):
                if gpu_buf is not None:
                    cpu_buf.data.copy_(gpu_buf.data, non_blocking=False)
                else:
                    assert cpu_buf is None, "local_model_gpu has unexpected None buffer to swap"
        else:
            if not hasattr(self.shared_model, "pinned_buf"):
                self.shared_model.pinned_buf = ODict()
                # è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                for name, buf in self.shared_model.named_buffers():
                    if buf is not None:
                        pin_buf = torch.zeros(buf.shape, dtype=buf.dtype, device="cpu", requires_grad=False).pin_memory()
                        # NOTE: dont' use torch.empty(pin_memory) with shared memory 
                        assert not pin_buf.is_shared() and pin_buf.is_pinned()
                    else:
                        pin_buf = None
                    self.shared_model.pinned_buf[name] = pin_buf
                for (gpu_name, gpu_buf), (pin_name, pin_buf) in zip(self.model.named_buffers(), self.shared_model.pinned_buf.items()):
                    assert gpu_name == pin_name
                    assert (gpu_buf is not None and pin_buf is not None) or \
                           (gpu_buf is None and pin_buf is None) 
            named_pin_buf = self.shared_model.pinned_buf
            # å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
            for name, gpu_buf in self.model.named_buffers():
                if gpu_buf is not None:
                    named_pin_buf[name].data.copy_(gpu_buf.data, non_blocking=False)
                else:
                    assert named_pin_buf[name] is None, "local_model_gpu has unexpected None buffer to swap"

    def train(self):
        self.model.train()

    def eval(self):
        self.model.eval()

# ğŸ“
# 1.ä¸“ä¸ºworker5æä¾›çš„å®ç°ï¼Œå¯¹å½“å‰rankä¸è´Ÿè´£çš„layerï¼Œä¹Ÿæ²¡æœ‰åˆå§‹åŒ–çš„å¿…è¦äº†ï¼Œåˆå§‹åŒ–ä»…ä»…æ˜¯æŠŠself.modelæŒ‡å‘ä¸€ä¸ªç©ºçš„model(å±‚)
# 2.æ·»åŠ copyin_param_buf_from_pinned_bufferæ–¹æ³•ï¼šè¿›è¡Œä»å…±ç”¨çš„pinned bufferåˆ°gpuçš„å¤åˆ¶ï¼Œè€Œä¸æ˜¯åŸæ¥çš„layerçš„pinnedç‰ˆæœ¬åˆ°gpuçš„å¤åˆ¶
class LocalModelGPU_for_worker5(object):
    """ A wrapper class of process-local vlayer on GPU.
    """
    # 1.ä¿é™©æ“ä½œï¼šç¡®ä¿pinned_modelï¼ˆå°±æ˜¯ä¸€ä¸ªvlayerï¼‰çš„å‚æ•°å’Œbufferéƒ½åœ¨å›ºå®šå†…å­˜ä¸­
    # 2.å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model å¯¹åº”çš„layerçš„dataä¸Š
    #   å³ç°åœ¨empty_modelçš„è¯¥å±‚layerå­˜åœ¨äºGPUä¸Š
    # 3.åˆ é™¤GPUä¸Šå½“å‰layerçš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
    def __init__(self, cpu_layers, pinned_model, shared_model, empty_model, shared_model_nvme, layer_id_to_layer_idx, transformer_layer_idx_to_shape, id, X_names, Y_names, rank=-1, world_size=-1, no_pin_model=False, no_pin_grad_buf=False):
        """ Call this in subprocess """ 
        self.pinned_model = pinned_model
        self.shared_model = shared_model
        self.id = id # vlayer_id
        self.X_names = X_names
        self.Y_names = Y_names
        self.rank, self.world_size = rank, world_size
        assert self.rank == torch.cuda.current_device()
        self.no_pin_model = no_pin_model
        self.no_pin_grad_buf = no_pin_grad_buf

        # ğŸ“éœ€è¦è¯¥ç±»çš„_get_pinned_modelæ–¹æ³•å¾—åˆ°å¯¹åº”çš„pinned model
        self.shared_model_nvme_handler = shared_model_nvme
        # ğŸ“
        self.layer_id_to_layer_idx = layer_id_to_layer_idx
        # ğŸ“
        self.cpu_layers = cpu_layers
        self.transformer_layer_idx_to_shape = transformer_layer_idx_to_shape
        

        # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œè¿™é‡Œæ‰§è¡Œï¼š
        # ç¡®ä¿pinned_modelï¼ˆå°±æ˜¯ä¸€ä¸ªvlayerï¼‰çš„å‚æ•°å’Œbufferéƒ½åœ¨å›ºå®šå†…å­˜ä¸­
        if not self.no_pin_model and self.id in self.cpu_layers:
            # confirm pinned model is pinned, local, CPU, and no grad
            for param in self.pinned_model.parameters():
                assert param.data.is_pinned() and (not param.data.is_shared()) and (not param.data.is_cuda)
                assert (not param.requires_grad) and (param.grad is None)
            for buf in self.pinned_model.buffers():
                assert buf.data.is_pinned() and (not buf.data.is_shared()) and (not buf.data.is_cuda)
                assert (not buf.requires_grad) and (buf.grad is None)
        
        # use existing empty model one
        assert isinstance(empty_model, torch.nn.Module)
        # ä»£è¡¨GPUä¸Šçš„æ¨¡å‹
        self.model = empty_model
        
        # ğŸ“
        # è‹¥å½“å‰rankä¸è´Ÿè´£è¯¥layerï¼Œä»¥ä¸‹é€»è¾‘ä¹Ÿæ²¡æœ‰å­˜åœ¨çš„å¿…è¦
        if self.id in self.cpu_layers:
            # self.model.cuda() # Moves all model parameters and buffers to the GPU. (replace CPU params and buffers with newly alloacated GPU param and buffers) Return self module.
            # å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model å¯¹åº”çš„layerçš„dataä¸Š
            # å³ç°åœ¨empty_modelçš„è¯¥å±‚layerå­˜åœ¨äºGPUä¸Š
            self.swapin_param_buf(True)
            # initialize empty shell on GPU
            # åˆ é™¤empty_modelå½“å‰layerä¸Šçš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
            self.del_param_grad_buf(manual_gc=True)
            # print("[LocalModelGPU][id%d] rank%d: initialized local model on GPU (empty shell)"%(self.id, self.rank))
        
    # æ„Ÿè§‰local modelçš„åˆå§‹åŒ–ä¸­çš„è¿™ä¸¤è¡Œæ²¡å•¥ç”¨å•Šï¼Œæš‚æ—¶ä¸è¿›è¡Œre_init
    def re_init(self):
        self.swapin_param_buf(True)
        self.del_param_grad_buf(manual_gc=True)

    def del_param_grad_buf(self, manual_gc=False):
        # é€’å½’åœ°åˆ é™¤æ¨¡å—ï¼ˆåŒ…æ‹¬å­æ¨¡å—ï¼‰ä¸­çš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
        delete_param_grad_buf(self.model, manual_gc=manual_gc)
   
    # å°†å­˜æ”¾åœ¨cpuå›ºå®šå†…å­˜çš„vlayerçš„å‚æ•°å’Œbufferçš„dataï¼Œä½¿ç”¨.cuda()æ–¹æ³•å¤åˆ¶åˆ°GPUä¸Šï¼Œå¹¶èµ‹ç»™ empty_model
    # å³ç°åœ¨empty_modelå­˜åœ¨äºGPUä¸Š
    @torch.no_grad()
    def swapin_param_buf(self, forward_only=True): 
        ''' Recursively allocate and copy-in all params and buffers from cpu module to self.local_model_gpu
            
            Note: if gpu_m._parameters[key] is previously del'ed to None, then swapin needs to create a new Parameter. Then it may leave uncollectable allocation on GPU after del this new'ed Parameter.
        '''
        # ç”±äºno_pin_modelå‚æ•°é»˜è®¤ä¸ºfalseï¼Œè¿™é‡Œçš„modelæŒ‡ä»£å­˜æ”¾åœ¨å›ºå®šå†…å­˜ä¸­çš„model
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()): # iterator over all modules in the network
            # print("{} <- {}".format(gpu_m, cpu_m))
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    # å°† CPU æ¨¡å‹ä¸­çš„å‚æ•°æ•°æ®å¤åˆ¶åˆ° GPU æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨ GPU ä¸Šåˆ†é…å†…å­˜
                    gpu_m._parameters[key].data = param.cuda(non_blocking=MEMCPY_NONBLK) # Returns a copy of this tensor object in CUDA memory.
                    # assert gpu_m._parameters[key].grad is None and (not gpu_m._parameters[key].requires_grad), "swapin requires no grad for both FP and BP"
                    # if not forward_only: 
                    #     gpu_m._parameters[key].requires_grad_(True)
                    # assert not param.is_cuda
                    # print("\t _parameter[{}]".format(key))
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    # if len(buf.shape) == 0: # Scalar buffer stays on CPU
                    #     gpu_m._buffers[key] = buf.clone().detach().requires_grad_(False)
                    # else:
                    gpu_m._buffers[key] = buf.cuda(non_blocking=MEMCPY_NONBLK) # Returns a copy of this tensor object in CUDA memory.
                    # assert not buf.is_cuda and (not gpu_m._buffers[key].requires_grad)
                    # print("\t _buffers[{}]".format(key))
        # print("[LocalModelGPU] rank{} swapin'ed params and bufs".format(self.rank))
        
        if not forward_only: # backward # move to here for 1) batching swap on GPU, 2) GPU CPU parallelism
            self.set_param_requiresgrad()
    
    # æ§åˆ¶æ¨¡å‹çš„å‚æ•°æ˜¯å¦éœ€è¦æ¢¯åº¦
    @torch.no_grad()
    def set_param_requiresgrad(self, requires_grad=True): 
        ''' From def swapin_param_buf() '''
        for gpu_m in self.model.modules(): # iterator over all modules in the network
            for key, param in gpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    param.requires_grad_(requires_grad)

    # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„tensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
    @torch.no_grad()
    def alloc_param_buf(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        # print(f"rank:{self.rank}, self.model:{self.model}, cpu_model:{cpu_model}")
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    gpu_m._parameters[key].data = torch.empty(param.shape,  dtype=param.dtype, device=self.rank, requires_grad=False)
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key] = torch.empty(buf.shape,  dtype=buf.dtype, device=self.rank, requires_grad=False)
    
    # ğŸ“
    @torch.no_grad()
    def alloc_param_buf_2(self, vt_idx, layer_id): 
        # print(f"rank:{self.rank}, vt.idxï¼š{vt_idx}, layer_id:{layer_id}")
        # pinned_model = None
        # if layer_id in self.cpu_layers:
        #     pinned_model = self.pinned_model
        # else:
        #     layer_idx = self.layer_id_to_layer_idx[vt_idx][layer_id]
        #     pinned_model = self.shared_model_nvme_handler.get_pinned_buffer(layer_idx)

        cpu_model = self.shared_model

        global_param_idx = 0
        # print(f"rank:{self.rank}, self.modelï¼š{self.model}, pinned model:{pinned_model}")
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    gpu_m._parameters[key].data = torch.empty(self.transformer_layer_idx_to_shape[global_param_idx], dtype=param.dtype, device=self.rank, requires_grad=False)
                    # print(f"rank:{self.rank}, layer{layer_id}ï¼ŒæŸ¥çœ‹GPUä¸Šåˆšåˆšåˆ†é…çš„tensorå½¢çŠ¶:{gpu_m._parameters[key].shape}")
                    global_param_idx+=1
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key] = torch.empty(self.transformer_layer_idx_to_shape[global_param_idx], dtype=buf.dtype, device=self.rank, requires_grad=False)
                    # print(f"rank:{self.rank}, layer{layer_id}ï¼ŒæŸ¥çœ‹GPUä¸Šåˆšåˆšåˆ†é…çš„bufferå½¢çŠ¶:{gpu_m._buffers[key].shape}")
                    global_param_idx+=1

    # å°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
    @torch.no_grad()
    def copyin_param_buf(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    print(f"rank:{self.rank}, copy in cpuå±‚, {gpu_m._parameters[key].data.shape}, {param.data.shape}", flush=True)
                    gpu_m._parameters[key].data.copy_(param.data, non_blocking=MEMCPY_NONBLK) # inplace copy
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key].data.copy_(buf.data, non_blocking=MEMCPY_NONBLK) # inplace copy

    # ğŸ“å¹¶éä»nvmeå‘pinned bufferå¤åˆ¶,è€Œæ˜¯pinned bufferåˆ°gpu
    # ğŸ“æ•°æ®å·²ç»åœ¨pinned bufferä¸­äº†ï¼Œç›´æ¥copy
    @torch.no_grad()
    def copyin_param_buf_from_pinned_buffer(self, vt_idx, layer_id): 
        layer_idx = self.layer_id_to_layer_idx[vt_idx][layer_id]
        # å¾—åˆ°å½“å‰å±‚ä½¿ç”¨çš„pinned model(pinned buffer)
        pinned_model = self.shared_model_nvme_handler.get_pinned_buffer(layer_idx)

        ''' From def swapin_param_buf() '''
        # cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), pinned_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                    print(f"rank:{self.rank}, copy in 0 or last, {gpu_m._parameters[key].data.shape}, {param.data.shape}")
                    gpu_m._parameters[key].data.copy_(param.data, non_blocking=MEMCPY_NONBLK) # inplace copy
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    print(f"rank:{self.rank}, layer{layer_id} to gpu, {gpu_m._parameters[key].shape}, {param.shape}")
                    gpu_m._buffers[key].data.copy_(buf.data, non_blocking=MEMCPY_NONBLK) # inplace copy

    # ğŸ“ä¸Šä¸ªæ–¹æ³•çš„å®Œå–„ç‰ˆæœ¬ï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šæ˜¯ä»Pinned memoryè¿˜æ˜¯pinned bufferå‘gpuå¤åˆ¶
    @torch.no_grad()
    def copyin_param_buf_2(self, vt_idx, layer_id): 
        # print(f"rank:{self.rank}, å‡†å¤‡å¼€å§‹transformer layer{layer_id} çš„ cpu->gpu å¤åˆ¶", flush=True)
        # å¾—åˆ°å½“å‰å±‚ä½¿ç”¨çš„pinned model(pinned buffer)
        # pinned_model = self.pinned_model if self.cpu_layers else self.shared_model_nvme_handler.get_pinned_buffer(layer_idx)
        pinned_model = None
        if layer_id in self.cpu_layers:
            pinned_model = self.pinned_model
            for gpu_m, cpu_m in zip(self.model.modules(), pinned_model.modules()):
                for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                    if param is not None:
                        gpu_m._parameters[key].data.copy_(param.data, non_blocking=MEMCPY_NONBLK) # inplace copy
                for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                    if buf is not None:
                        gpu_m._buffers[key].data.copy_(buf.data, non_blocking=MEMCPY_NONBLK) # inplace copy 
        else:
            # ğŸ“å…ˆå°†pinned bufferçš„å„ä¸ªå‚æ•°æ¢å¤ä¸ºåŸæ¥çš„å½¢çŠ¶ï¼Œå†å¤åˆ¶åˆ°GPU
            layer_idx = self.layer_id_to_layer_idx[vt_idx][layer_id]
            pinned_model = self.shared_model_nvme_handler.get_pinned_buffer(layer_idx)
            
            global_param_idx = 0
            for gpu_m, cpu_m in zip(self.model.modules(), pinned_model.modules()):
                for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                    if param is not None:
                        # print(f"\trank:{self.rank}, å‡†å¤‡å¼€å§‹transformer layer{layer_id}-{global_param_idx} param çš„ cpu->gpu å¤åˆ¶", flush=True)
                        gpu_m._parameters[key].data.copy_(param.view(self.transformer_layer_idx_to_shape[global_param_idx]).data, non_blocking=MEMCPY_NONBLK) # inplace copy
                        global_param_idx+=1
                for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                    if buf is not None:
                        # print(f"\trank:{self.rank}, å‡†å¤‡å¼€å§‹transformer layer{layer_id}-{global_param_idx} buf çš„ cpu->gpu å¤åˆ¶", flush=True)
                        gpu_m._buffers[key].data.copy_(buf.view(self.transformer_layer_idx_to_shape[global_param_idx]).data, non_blocking=MEMCPY_NONBLK) # inplace copy 
                        global_param_idx+=1

    # ğŸ“åŒbufferç‰ˆæœ¬ï¼Œä¸ä¸Šä¸€ä¸ªæ–¹æ³•çš„åŒºåˆ«ï¼šéœ€æ ¹æ®buffer_idæ¥è·å–pinned model
    def copyin_param_buf_for_double_buffer(self, buffer_id, vt_idx, layer_id):
        if layer_id in self.cpu_layers:
            pinned_model = self.pinned_model
            for gpu_m, cpu_m in zip(self.model.modules(), pinned_model.modules()):
                for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                    if param is not None:
                        gpu_m._parameters[key].data.copy_(param.data, non_blocking=MEMCPY_NONBLK) # inplace copy
                for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                    if buf is not None:
                        gpu_m._buffers[key].data.copy_(buf.data, non_blocking=MEMCPY_NONBLK) # inplace copy 

        else:
            layer_idx = self.layer_id_to_layer_idx[vt_idx][layer_id]
            pinned_model = self.shared_model_nvme_handler.get_pinned_buffer(buffer_id, layer_idx)

            global_param_idx = 0
            for gpu_m, cpu_m in zip(self.model.modules(), pinned_model.modules()):
                for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                    if param is not None:
                        # print(f"\trank:{self.rank}, å‡†å¤‡å¼€å§‹transformer layer{layer_id}-{global_param_idx} param çš„ cpu->gpu å¤åˆ¶", flush=True)
                        gpu_m._parameters[key].data.copy_(param.view(self.transformer_layer_idx_to_shape[global_param_idx]).data, non_blocking=MEMCPY_NONBLK) # inplace copy
                        global_param_idx+=1
                for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                    if buf is not None:
                        # print(f"\trank:{self.rank}, å‡†å¤‡å¼€å§‹transformer layer{layer_id}-{global_param_idx} buf çš„ cpu->gpu å¤åˆ¶", flush=True)
                        gpu_m._buffers[key].data.copy_(buf.view(self.transformer_layer_idx_to_shape[global_param_idx]).data, non_blocking=MEMCPY_NONBLK) # inplace copy 
                        global_param_idx+=1

    @torch.no_grad()
    def copyin_param_buf_blocking(self): 
        ''' From def swapin_param_buf() '''
        cpu_model = self.shared_model if self.no_pin_model else self.pinned_model
        # model.modules()ï¼šè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œç”¨äºé€’å½’åœ°éå†æ¨¡å‹ model ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å‹æœ¬èº«ä»¥åŠå®ƒçš„å­æ¨¡å—
        # ğŸ“Œæ— éœ€æ‹…å¿ƒæ¨¡å‹æœ¬èº«ï¼Œå› ä¸ºæ¨¡å‹æœ¬èº«çš„_parametersçš„é•¿åº¦ä¸º0ï¼Œforå¾ªç¯ä¸ä¼šæ‰§è¡Œ
        for gpu_m, cpu_m in zip(self.model.modules(), cpu_model.modules()):
            for key, param in cpu_m._parameters.items(): # OrderedDict([('weight', Parameter), ('bias', Parameter)])
                if param is not None:
                   gpu_m._parameters[key].data.copy_(param.data, non_blocking=False) # inplace copy
            for key, buf in cpu_m._buffers.items(): # OrderedDict([('running_mean', tensor), ('running_var', tensor), ('num_batches_tracked', tensor)])
                if buf is not None:
                    gpu_m._buffers[key].data.copy_(buf.data, non_blocking=False) # inplace copy

    def __call__(self, *input, **kwargs):
        return self.model(*input, **kwargs)
        # ref: https://github.com/pytorch/pytorch/blob/472be69a736c0b2aece4883be9f8b18e2f3dfbbd/torch/nn/modules/module.py#L487
    
    def average_grad(self, p2p_handler):
        p2p_handler.average_gradients(self.model)

    def average_buf(self, p2p_handler):
        p2p_handler.average_buffers(self.model)

    # 1.åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
    # 2.å°†tensoræ‹·è´åˆ°pinned memoryä¸­
    # 3.å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
    # 4.ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Šï¼Œå³æ‹·è´åˆ°shared_modelä¸Š
    # shared_modelçš„param.gradå®é™…ä¸Šåœ¨pinned memoryä¸Š
    @torch.no_grad()
    def swapout_grad(self):
        ''' in-place copy gradient from local model gpu to local .grad on cpu 
            (lazily allocate local .grad if not exists)
        '''
        for gpu_param, cpu_param in zip(self.model.parameters(), self.shared_model.parameters()):
            if gpu_param.grad is not None:
                # è‹¥shared memoryç‰ˆæœ¬æ¨¡å‹ï¼ˆæ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„gradå±æ€§ä¸ºç©ºï¼Œä¸ºå…¶åˆ†é…ä¸€ä¸ªpinned 
                # memoryä¸Šçš„tensorä½œä¸ºå€¼
                if cpu_param.grad is None:
                    # print(f"rank:{self.rank}, +++++++++++++++++++++++++++++é‡æ–°åˆ›å»ºæ¢¯åº¦")
                    assert cpu_param.requires_grad
                    # åœ¨cpuä¸ŠæŒ‰ç…§paramçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                    g = torch.zeros(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False)
                    # å°†tensoræ‹·è´åˆ°pinned memoryä¸­
                    if not self.no_pin_grad_buf:
                        g = g.pin_memory()
                    # å°†åˆšåˆšåˆ†é…çš„tensorèµ‹ç»™ cpu_param.grad
                    cpu_param.grad = g
                    # cpu_param.grad = torch.empty(cpu_param.shape, dtype=cpu_param.dtype, device="cpu", requires_grad=False, pin_memory=not self.no_pin_grad_buf) # NOTE: dont' use empty(pin_memory) with shared memory 
                    assert not cpu_param.grad.is_shared() # and cpu_param.grad.is_pinned()
                # ä»¥éé˜»å¡çš„æ–¹å¼å°†gpuå‚æ•°çš„grad.data æ‹·è´åˆ°åˆšåˆšåˆ†é…çš„tensorä¸Š
                cpu_param.grad.data.copy_(gpu_param.grad.data, non_blocking=MEMCPY_NONBLK) # in-place copies the elements from src tensor into self tensor (and returns self)
            else:
                print(ValueError("model has grad = None to swap"))
                cpu_param.grad = None
        # print("[LocalModelGPU] rank{} swapout'ed grad".format(self.rank))

    # 1.è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
    # 2.å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
    # æœ€ç»ˆè¿™ä¸ªpinned_bufä¼šä»¥æˆå‘˜å˜é‡çš„å½¢å¼æŒ‚åœ¨shared_modelä¸Š
    @torch.no_grad()
    def swapout_buf(self): 
        ''' in-place copy buffers from local model gpu to local pinned buf or shared buf on cpu 
            (lazily allocate local pinned buf if not exists)
        '''
        if self.no_pin_grad_buf:
            for gpu_buf, cpu_buf in zip(self.model.buffers(), self.shared_model.buffers()):
                if gpu_buf is not None:
                    cpu_buf.data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK)
                else:
                    assert cpu_buf is None, "local_model_gpu has unexpected None buffer to swap"
        else:
            if not hasattr(self.shared_model, "pinned_buf"):
                self.shared_model.pinned_buf = ODict()
                # è‹¥æ¨¡å‹å­˜åœ¨bufferï¼Œåœ¨cpuä¸Šçš„å›ºå®šå†…å­˜æŒ‰ç…§bufferçš„shapeå’Œç±»å‹åˆ†é…ä¸€ä¸ªé›¶å€¼tensor
                for name, buf in self.shared_model.named_buffers():
                    if buf is not None:
                        pin_buf = torch.zeros(buf.shape, dtype=buf.dtype, device="cpu", requires_grad=False).pin_memory()
                        # NOTE: dont' use torch.empty(pin_memory) with shared memory 
                        assert not pin_buf.is_shared() and pin_buf.is_pinned()
                    else:
                        pin_buf = None
                    self.shared_model.pinned_buf[name] = pin_buf
                for (gpu_name, gpu_buf), (pin_name, pin_buf) in zip(self.model.named_buffers(), self.shared_model.pinned_buf.items()):
                    assert gpu_name == pin_name
                    assert (gpu_buf is not None and pin_buf is not None) or \
                           (gpu_buf is None and pin_buf is None) 
            named_pin_buf = self.shared_model.pinned_buf
            # å°†gpuä¸Šçš„buffer tensorä»¥éé˜»å¡çš„æ–¹å¼æ‹·è´åˆ°cpuå›ºå®šå†…å­˜ä¸Šåˆšåˆšåˆ†é…çš„é›¶å€¼tensor
            for name, gpu_buf in self.model.named_buffers():
                if gpu_buf is not None:
                    named_pin_buf[name].data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK)
                else:
                    assert named_pin_buf[name] is None, "local_model_gpu has unexpected None buffer to swap"

        # for gpu_buf, cpu_buf in zip(self.model.buffers(), self.pinned_model.buffers()):
        #     cpu_buf.data.copy_(gpu_buf.data, non_blocking=MEMCPY_NONBLK) # Copies the elements from src tensor into self tensor (and returns self); in-place copy (no new tensor created)
        # print("[LocalModelGPU] rank{} swapout'ed buf".format(self.rank))

    def train(self):
        self.model.train()

    def eval(self):
        self.model.eval()

# åˆ†æï¼šput_queue ç›¸å½“äºä¸€ä¸ªå¯åŠ¨æ¡ä»¶ï¼Œå³æ¥æ´»äº†ï¼Œå…¶ä¸­çš„å…ƒç´ æ˜¯å…¶ä»–çº¿ç¨‹æˆ–è‡ªå·±æ·»åŠ è¿›å»çš„ï¼Œè¿™ä¸ªé‡Œé¢å¾—æœ‰ä¸œè¥¿ï¼Œè‡ªèº«è¿™ä¸ªçº¿ç¨‹æ‰èƒ½å–å‡ºä¸œè¥¿ï¼Œå¹¶æ‰§è¡Œåé¢çš„æ­¥éª¤
#      ä¸ç„¶ä¼šä¸€ç›´é˜»å¡ï¼Œç›´åˆ°é‡Œé¢æœ‰ä¸œè¥¿èƒ½å¤Ÿå–å‡ºæ¥ã€‚å–å‡ºæ¥å°±æ„å‘³ç€å‰ææ¡ä»¶å·²ç»æ‰§è¡Œå®Œäº†ï¼ˆå¯ä»¥ç®€å•è¿™ä¹ˆç†è§£ï¼Œåœ¨è¿™ä¸ªç±»ä¸­å–å‡ºæ¥åè¿˜éœ€è¦æ˜¾å¼çš„ç­‰å¾…æ‰§è¡Œå®Œï¼‰
#      get_queue ä¸­è£…ç€(vt.idx,ev_swapin)ï¼Œè¡¨ç¤ºè¿™ä¸ªev_swapinäº‹ä»¶æ­£åœ¨æ‰§è¡Œæˆ–å·²ç»æ‰§è¡Œå®Œäº†ï¼Œ
""" Prefetch LocalModelGPU  """
class PrefetchLocalModelGPU(object):
    """ Handles flexible prefetch of local model (W,B) from pinned model in background thread.
        Step:
        1) main thread: allocate (W,B) on GPU
        2) prefetch thread: get sync'ed pinned model (W,B)
        3) prefetch thread: copy in (W,B) in swapin_cudaStream
        4) prefetch thread: turn on grad
        
        Assumption: 
        1) always in FIFO ordering. put each task, and get prefetched task. 
        2) use cuda event for unblocking next CPU ops

        NOTE: why main thread allocates? i) 'no memory reuse across different stream' -- PyTorch, ii) different threads use different CUDA address spaces
        NOTE: move vt.medium check back to runtime by reducing granularity to vLayer?
    """
    def __init__(self, syncpin_handler, local_model, rank, swapin_stream=None, compute_stream=None, nvprof=False):
        self.syncpin_handler = syncpin_handler
        self.local_model = local_model # list
        self.rank = rank
        # è‹¥æ²¡æœ‰ç»™å®šä¸€ä¸ªstreamï¼Œåˆ™åœ¨å½“å‰rankä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„CUDA stream
        # ç›®å‰æ¥çœ‹ï¼Œswapin_streamå°±æ˜¯ä¸€ä¸ªæ–°çš„æµ
        self.swapin_stream = swapin_stream if swapin_stream is not None else torch.cuda.Stream(device=rank)
        # è¯¥å‚æ•°æ˜¯ç»™å®šäº†çš„ï¼Œå…¶å®å’Œä¸ç»™å®šæ‰§è¡Œelseä¸€æ ·ï¼Œéƒ½æ˜¯cudaä¸Šçš„é»˜è®¤æµ
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        self.nvprof = nvprof
        assert rank == torch.cuda.current_device()
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [vt1, ...] # between main and prefetch thread
        self.get_queue = threadsafe_data_struct.Queue() # [vt1.idx, ...] # between prefetch and main thread
        self.prefetch_thread = threading.Thread(target=self._thread_func)
        self.prefetch_thread.daemon = True
        self.prefetch_thread.start()
        
        self.is_running = False
        
        # print("[PrefetchLocalModelGPU] rank{} started prefetch_thread".format(self.rank))

    # inputå°±æ˜¯å‘put_queueä¸­åŠ å…ƒç´ ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    #
    # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
    # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
    # 2.åœ¨é»˜è®¤æµä¸Šï¼ˆev_computeï¼‰è®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
    # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
    # 4.å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    def iput(self, vt):
        ''' Call by main thread. Blocking Alloc & Nonblocking CopyIn.
            Assumption: only one iput can be running. '''
        if vt is None:
            return
        # æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
        assert not self.is_running, "the prefetch is still running"
        # å°† self.is_running æ ‡å¿—è®¾ç½®ä¸º Trueï¼Œè¡¨ç¤ºå½“å‰æœ‰ iput æ“ä½œæ­£åœ¨æ‰§è¡Œã€‚
        self.is_running = True
        assert isinstance(vt, vTask)
        # record previous compute event for swapin stream to wait
        # åœ¨ compute_stream ï¼ˆé»˜è®¤æµï¼‰ä¸Šè®°å½•äº‹ä»¶ ev_computeï¼Œè‹¥å‚æ•°ä¸ºç©ºåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
        ev_compute = self.compute_stream.record_event()
        # allocation in main thread
        if self.nvprof: nvtx_range_push("task{}({}) Alloc(W,B)".format(vt.idx, vt.show_layers())) 

        # è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
        for l in vt.layers: # verified: vt.In['W'].keys==vt.In['B'].keys==vt.layers
            # è‹¥è¯¥å±‚çš„Wå’ŒBçš„åª’ä»‹ä¸ºSHM
            if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
                self.local_model[l].alloc_param_buf()
            elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                pass
            else: # P2P
                raise ValueError("Underdevelopment")
        # do the rest in background thread
        # å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­
        self.put_queue.add((vt,ev_compute))
        if self.nvprof: nvtx_range_pop() 

    # ä¸æ–­å°è¯•ä»put_queueä¸­æ‹¿å–(vt, ev_computeï¼ˆå‡†å¤‡å·¥ä½œçš„äº‹ä»¶ï¼Œå³åœ¨GPUä¸Šå…ˆåˆå§‹åŒ–Wå’ŒBï¼‰)ï¼Œæ‰§è¡Œï¼ˆè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ä¼šè¢«é˜»å¡ï¼‰ï¼š
    # 1.ä» put_queue é˜Ÿåˆ—ä¸­å¼¹å‡º (vt, ev_compute)ï¼Œè‹¥é˜Ÿåˆ—æ²¡æœ‰å…ƒç´ ä¼šè¢«é˜»å¡åœ¨è¿™
    # 2.è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
    # 3.åœ¨ CUDA æµ self.swapin_stream ä¸Šç­‰å¾…äº‹ä»¶ ev_compute çš„å®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
    #   å³ç­‰å¾…åœ¨GPUä¸Šåˆå§‹åŒ–vtæ‰€æœ‰å±‚çš„ Wå’ŒB(çš„tensor) çš„å®Œæˆ
    # 4.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 5.åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ ev_swapin
    # 6.å°† (idx(å½“å‰ä»»åŠ¡çš„id),ev_swapin) åŠ å…¥åˆ° get_queue ä¸­
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each incoming task 
            # ä» put_queue é˜Ÿé¦–å¼¹å‡ºä¸€ä¸ªå…ƒç´ ï¼Œè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ï¼Œä¼šè¢«é˜»å¡åœ¨è¿™é‡Œ
            vt, ev_compute = self.put_queue.remove() # blocking
            if self.nvprof: nvtx_range_push("__task{}({}) CopyIn(W,B)".format(vt.idx, vt.show_layers())) 
            # get sync'ed pinned model 
            # è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚
            # å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
            # ğŸ“Œæ ¹æ®syncpin_handlerä¸­çš„æ³¨é‡Šæ¥çœ‹ï¼Œä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„åŒæ­¥æ“ä½œæ˜¯é˜»å¡çš„ï¼ˆå°½ç®¡ä»£ç ä¸Šç›´è§‚æ¥çœ‹æ˜¯éé˜»å¡çš„ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¸€å®šæ‰§è¡Œå®Œäº†
            #   ä»å¦ä¸€ä¸ªè§’åº¦çœ‹ï¼Œè¿™ä¸ªget()æœ¬èº«å°±æ˜¯åœ¨ç­‰å¾…syncçº¿ç¨‹æ‰§è¡Œå®Œä¸€æ¬¡åŒæ­¥æ“ä½œ
            syncpin_vt_idx = self.syncpin_handler.get()
            # ç¡®ä¿å½“å‰åœ¨GPUä¸Šåˆå§‹åŒ–Wå’ŒBçš„ä»»åŠ¡å’Œå…±äº«å±‚åˆ°pinnedå±‚å¤åˆ¶ä»»åŠ¡çš„å±‚ï¼Œç›®æ ‡æ˜¯åŒä¸€ä¸ªvt
            # ä¸‹é¢å°±æ˜¯è¦æŠŠè¯¥ä»»åŠ¡å¯¹åº”çš„layer packçš„Wå’ŒB swap inåˆ°GPUä¸Š
            assert syncpin_vt_idx == vt.idx
            # let swapin stream waits for this compute event 
            # ç­‰å¾…äº‹ä»¶ ev_compute åœ¨ CUDA æµ self.swapin_stream ä¸Šå®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
            # å³ç­‰å¾…ä»»åŠ¡vtçš„æ‰€æœ‰å±‚å·²ç»æ‹¿åˆ°GPUä¸Š
            self.swapin_stream.wait_event(ev_compute) 
            # copy in and turn on grad
            # åœ¨swapin_streamä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
            # æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
            with torch.cuda.stream(self.swapin_stream): # context-manager selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.
                for l in vt.layers:
                    # è‹¥Wå’ŒBåœ¨çš„é€šè®¯åª’ä»‹ä¸ºå…±äº«å†…å­˜ï¼Œå°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
                    if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                        self.local_model[l].copyin_param_buf()
                        # è‹¥vtæ˜¯BWDä»»åŠ¡ï¼Œè¿˜éœ€å°†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()
                    # è‹¥Wå’ŒBè¢«pinåœ¨è®¾å¤‡ä¸Šï¼Œæ˜¾ç„¶ä¸ç”¨æ‰§è¡Œæ‹·è´æ“ä½œ
                    elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()
                    else: # P2P
                        raise ValueError("Underdevelopment")
            # record this swapin event for compute stream to wait
            # åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶
            ev_swapin = self.swapin_stream.record_event() 
            # if MEMCPY_NONBLK: self.swapin_stream.synchronize() # Wait for all the kernels in this stream to complete.
            # ready to use
            # å°†ä»»åŠ¡çš„idxå’Œ swap_in äº‹ä»¶ä»¥å…ƒç»„çš„å½¢å¼åŠ å…¥åˆ° get_queue ä¸­
            self.get_queue.add( (vt.idx,ev_swapin) )
            if self.nvprof: nvtx_range_pop() 

    # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
    # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    def _wait(self):
        ''' Wait for the running iput. Called in main thread.
            Assumption: only one iput can be running. '''
        assert self.is_running, "no running prefetch"
        self.is_running = False
        return self.get_queue.remove()

    # è¯¥å‡½æ•°é™„å¸¦ä¿é™©æ“ä½œï¼ˆå®é™…ä¸Šå¯èƒ½ä¹‹å‰å°±æ²¡è°ƒç”¨è¿‡inputæ¥ä¸ºå½“å‰çº¿ç¨‹æ·»åŠ é¢„å–ä»»åŠ¡ï¼‰ï¼š
    # æ‹¿å–get_queueï¼ˆé€»è¾‘ä¸Šå®Œæˆé¢„å–çš„ä»»åŠ¡é˜Ÿåˆ—ï¼‰ä¸­çš„é¦–ä¸ªå…ƒç´ ï¼Œå³ç­‰å¾…ä¸€ä¸ªä¹‹å‰å°±è§¦å‘çš„é¢„å–æ¨¡å‹ï¼ˆswapinï¼‰äº‹ä»¶ã€‚æ‹¿å–åªä»£è¡¨é€»è¾‘ä¸Šæ‰§è¡Œå®Œï¼Œ
    # å®é™…ä¸Šå¯èƒ½æ²¡æ‰§è¡Œå®Œï¼Œå› æ­¤éœ€è¦ç­‰å¾…äº‹ä»¶çš„å®Œæˆã€‚æœ€åè¿”å›æ‹¿å–å®Œæˆçš„é¦–ä¸ªå…ƒç´ ä¸­çš„vt_idx
    # ğŸ“Œåˆ†æï¼šinputå’Œgetæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œinputå°†is_runningç½®ä¸ºtrueï¼Œget(è°ƒç”¨wait)å°†is_runningç½®ä¸ºfalseã€‚ä¸è°ƒç”¨getï¼Œis_runningå°±ä¸å¯èƒ½ä¸ºfalse
    #         è‹¥å‘ç°is_runningä¸ä¸ºtrueï¼Œå°±è¯´æ˜ä¹‹å‰æ ¹æœ¬å°±æ²¡æ‰§è¡Œè¿‡layerçš„é¢„å–
    
    # 1.å‡†å¤‡å·¥ä½œ1ï¼šè°ƒç”¨syncpin_handlerå®ä¾‹çš„çº¿ç¨‹å°†vtä¸­çš„è¿™äº›åœ¨cpuå…±äº«å†…å­˜ä¸­çš„layerå¤åˆ¶åˆ°pinned memoryä¸Šï¼›
    # 2.å‡†å¤‡å·¥ä½œ2ï¼šåœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    #   åŒæ—¶ä¹Ÿæ˜¯å½“å‰PrefetchLocalModelGPUå®ä¾‹çš„çº¿ç¨‹çš„è§¦å‘å·¥ä½œï¼Œå°†ä¸œè¥¿æ”¾è¿›put_queueï¼Œè¿™æ„å‘³ç€çº¿ç¨‹å¼€å§‹æ‰§è¡Œ3
    # 3.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 4.è°ƒç”¨_waitå°†is_running ç½®ä¸ºfalseï¼Œè¿”å›get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    # 5.self.compute_stream.wait_event(ev_swapin)
    # 6.è‹¥suc_vtå‚æ•°ä¸ä¸ºç©ºï¼Œæ„å‘³ç€è¯¥å‡½æ•°ä¼šä¸ºæå‰æ‰§è¡Œä¸€éƒ¨åˆ†åç»§ä»»åŠ¡ï¼Œå³è°ƒç”¨self.syncpin_handler.iput(suc_vt)ï¼Œä¸1ç›¸åŒ
    def get(self, vt, suc_vt=None):
        ''' Call by main thread. Blocking wait until current prefetch finish. Then launch next sync pin. Return current vt.idx. '''
        assert isinstance(vt, vTask)
        # wait current one (if no current one, get this one)
        # è‹¥å½“å‰æ²¡æœ‰æ­£åœ¨GPUä¸Šåˆ†é…vtçš„æ‰€æœ‰å±‚
        if not self.is_running:
            # self.put_queue.add(vt)
            # è¿™æ„å‘³ç€ syncpin_handler è¿™ä¸ªçº¿ç¨‹å¼€å§‹æ‰§è¡Œvtçš„æ¨¡å‹çš„ä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„å¤åˆ¶
            self.syncpin_handler.iput(vt)
            # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
            # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
            # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
            # 
            # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
            # 2.åœ¨é»˜è®¤æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin_stream è¿™ä¸ªæµä¸Šç­‰å¾…
            # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
            # 4.å°† (vt, ev_compute) æ·»åŠ åˆ° put_queue ä¸­
            self.iput(vt)
        # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
        # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)ï¼Œå³æ­£åœ¨æˆ–å·²ç»æ‰§è¡Œç©çš„swap_inäº‹ä»¶ï¼Œ(vt.idx,ev_swapin)
        #   è‹¥ _thread_func, å³ swap_in æ²¡åœ¨swapin_streamä¸Šåˆ†é…å®Œï¼Œä¼šé˜»å¡åœ¨ remove() ä¸Š
        cur_vt_idx, ev_swapin = self._wait()
        # ç­‰å¾…è¯¥vtä¸Šæ‰€æœ‰çš„å±‚åœ¨GPUä¸Šå®Œæˆåˆ†é…ç©ºtensor
        self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
        # syncpin next one if exsits
        # è‹¥ç»™å®šäº†åç»§ä»»åŠ¡ï¼Œ
        if suc_vt is not None:
            self.syncpin_handler.iput(suc_vt)
        # è¿”å› cur_vt_idx
        return cur_vt_idx


# ================ my version =========================
class PrefetchLocalModelGPU_2(object):
    """ Handles flexible prefetch of local model (W,B) from pinned model in background thread.
        Step:
        1) main thread: allocate (W,B) on GPU
        2) prefetch thread: get sync'ed pinned model (W,B)
        3) prefetch thread: copy in (W,B) in swapin_cudaStream
        4) prefetch thread: turn on grad
        
        Assumption: 
        1) always in FIFO ordering. put each task, and get prefetched task. 
        2) use cuda event for unblocking next CPU ops

        NOTE: why main thread allocates? i) 'no memory reuse across different stream' -- PyTorch, ii) different threads use different CUDA address spaces
        NOTE: move vt.medium check back to runtime by reducing granularity to vLayer?
    """
    def __init__(self, syncpin_handler, local_model, rank, swapin_stream=None, compute_stream=None, nvprof=False):
        self.syncpin_handler = syncpin_handler
        self.local_model = local_model # list
        self.rank = rank
        # è‹¥æ²¡æœ‰ç»™å®šä¸€ä¸ªstreamï¼Œåˆ™åœ¨å½“å‰rankä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„CUDA stream
        # ç›®å‰æ¥çœ‹ï¼Œswapin_streamå°±æ˜¯ä¸€ä¸ªæ–°çš„æµ
        self.swapin_stream = swapin_stream if swapin_stream is not None else torch.cuda.Stream(device=rank)
        # è¯¥å‚æ•°æ˜¯ç»™å®šäº†çš„ï¼Œå…¶å®å’Œä¸ç»™å®šæ‰§è¡Œelseä¸€æ ·ï¼Œéƒ½æ˜¯cudaä¸Šçš„é»˜è®¤æµ
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        self.nvprof = nvprof
        assert rank == torch.cuda.current_device()
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [vt1, ...] # between main and prefetch thread
        self.get_queue = threadsafe_data_struct.Queue() # [vt1.idx, ...] # between prefetch and main thread
        self.prefetch_thread = threading.Thread(target=self._thread_func)
        self.prefetch_thread.daemon = True
        self.prefetch_thread.start()
        
        # self.is_running = False
        
        # print("[PrefetchLocalModelGPU] rank{} started prefetch_thread".format(self.rank))

    # inputå°±æ˜¯å‘put_queueä¸­åŠ å…ƒç´ ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    #
    # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
    # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
    # 2.åœ¨é»˜è®¤æµä¸Šï¼ˆev_computeï¼‰è®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
    # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
    # 4.å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    def iput(self, layer_id, vt):
        ''' Call by main thread. Blocking Alloc & Nonblocking CopyIn.
            Assumption: only one iput can be running. '''
        if layer_id is None:
            return
        # å°† self.is_running æ ‡å¿—è®¾ç½®ä¸º Trueï¼Œè¡¨ç¤ºå½“å‰æœ‰ iput æ“ä½œæ­£åœ¨æ‰§è¡Œã€‚
        # self.is_running = True
        # record previous compute event for swapin stream to wait
        # åœ¨ compute_stream ï¼ˆé»˜è®¤æµï¼‰ä¸Šè®°å½•äº‹ä»¶ ev_computeï¼Œè‹¥å‚æ•°ä¸ºç©ºåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
        ev_compute = self.compute_stream.record_event()
        # allocation in main thread
        if self.nvprof: nvtx_range_push("task{}({}) Alloc(W,B)".format(vt.idx, vt.show_layers())) 

        # è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
        # for l in vt.layers: # verified: vt.In['W'].keys==vt.In['B'].keys==vt.layers
        #     # è‹¥è¯¥å±‚çš„Wå’ŒBçš„åª’ä»‹ä¸ºSHM
        #     if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
        #         # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
        #         self.local_model[l].alloc_param_buf()
        #     elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
        #         pass
        #     else: # P2P
        #         raise ValueError("Underdevelopment")
        
        if vt.In['W'][layer_id].medium=='SHM' and vt.In['B'][layer_id].medium=='SHM':
            # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
            self.local_model[layer_id].alloc_param_buf()
        elif vt.In['W'][layer_id].medium=='PIN' and vt.In['B'][layer_id].medium=='PIN':
            pass
        else: # P2P
            raise ValueError("Underdevelopment")

        # do the rest in background thread
        # å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­
        self.put_queue.add((layer_id, vt, ev_compute))
        if self.nvprof: nvtx_range_pop() 

    # ä¸æ–­å°è¯•ä»put_queueä¸­æ‹¿å–(vt, ev_computeï¼ˆå‡†å¤‡å·¥ä½œçš„äº‹ä»¶ï¼Œå³åœ¨GPUä¸Šå…ˆåˆå§‹åŒ–Wå’ŒBï¼‰)ï¼Œæ‰§è¡Œï¼ˆè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ä¼šè¢«é˜»å¡ï¼‰ï¼š
    # 1.ä» put_queue é˜Ÿåˆ—ä¸­å¼¹å‡º (vt, ev_compute)ï¼Œè‹¥é˜Ÿåˆ—æ²¡æœ‰å…ƒç´ ä¼šè¢«é˜»å¡åœ¨è¿™
    # 2.è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
    # 3.åœ¨ CUDA æµ self.swapin_stream ä¸Šç­‰å¾…äº‹ä»¶ ev_compute çš„å®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
    #   å³ç­‰å¾…åœ¨GPUä¸Šåˆå§‹åŒ–vtæ‰€æœ‰å±‚çš„ Wå’ŒB(çš„tensor) çš„å®Œæˆ
    # 4.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 5.åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ ev_swapin
    # 6.å°† (idx(å½“å‰ä»»åŠ¡çš„id),ev_swapin) åŠ å…¥åˆ° get_queue ä¸­
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each incoming task 
            # ä» put_queue é˜Ÿé¦–å¼¹å‡ºä¸€ä¸ªå…ƒç´ ï¼Œè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ï¼Œä¼šè¢«é˜»å¡åœ¨è¿™é‡Œ
            layer_id, vt, ev_compute = self.put_queue.remove() # blocking
            if self.nvprof: nvtx_range_push("__task{}(L{}) call CopyIn(W,B)".format(vt.idx, layer_id)) 
            # get sync'ed pinned model 
            # è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚
            # å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
            # ğŸ“Œæ ¹æ®syncpin_handlerä¸­çš„æ³¨é‡Šæ¥çœ‹ï¼Œä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„åŒæ­¥æ“ä½œæ˜¯é˜»å¡çš„ï¼ˆå°½ç®¡ä»£ç ä¸Šç›´è§‚æ¥çœ‹æ˜¯éé˜»å¡çš„ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¸€å®šæ‰§è¡Œå®Œäº†
            #   ä»å¦ä¸€ä¸ªè§’åº¦çœ‹ï¼Œè¿™ä¸ªget()æœ¬èº«å°±æ˜¯åœ¨ç­‰å¾…syncçº¿ç¨‹æ‰§è¡Œå®Œä¸€æ¬¡åŒæ­¥æ“ä½œ
            syncpin_layer_id = self.syncpin_handler.get()
            # ç¡®ä¿å½“å‰åœ¨GPUä¸Šåˆå§‹åŒ–Wå’ŒBçš„ä»»åŠ¡å’Œå…±äº«å±‚åˆ°pinnedå±‚å¤åˆ¶ä»»åŠ¡çš„å±‚ï¼Œç›®æ ‡æ˜¯åŒä¸€ä¸ªvt
            # ä¸‹é¢å°±æ˜¯è¦æŠŠè¯¥ä»»åŠ¡å¯¹åº”çš„layer packçš„Wå’ŒB swap inåˆ°GPUä¸Š
            assert syncpin_layer_id == layer_id
            # let swapin stream waits for this compute event 
            # ç­‰å¾…äº‹ä»¶ ev_compute åœ¨ CUDA æµ self.swapin_stream ä¸Šå®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
            # å³ç­‰å¾…ä»»åŠ¡vtçš„æ‰€æœ‰å±‚å·²ç»æ‹¿åˆ°GPUä¸Š
            self.swapin_stream.wait_event(ev_compute) 
            # copy in and turn on grad
            # åœ¨swapin_streamä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
            # æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
            with torch.cuda.stream(self.swapin_stream): # context-manager selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.
                # è‹¥Wå’ŒBåœ¨çš„é€šè®¯åª’ä»‹ä¸ºå…±äº«å†…å­˜ï¼Œå°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
                # print(f"rank:{self.rank}, æ­£åœ¨é¢„å–layer{layer_id}")
                if vt.In['W'][layer_id].medium=='SHM' and vt.In['B'][layer_id].medium=='SHM':
                    self.local_model[layer_id].copyin_param_buf()
                    # è‹¥vtæ˜¯BWDä»»åŠ¡ï¼Œè¿˜éœ€å°†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
                    if vt.type == 'BWD':
                        self.local_model[layer_id].set_param_requiresgrad()
                # è‹¥Wå’ŒBè¢«pinåœ¨è®¾å¤‡ä¸Šï¼Œæ˜¾ç„¶ä¸ç”¨æ‰§è¡Œæ‹·è´æ“ä½œ
                elif vt.In['W'][layer_id].medium=='PIN' and vt.In['B'][layer_id].medium=='PIN':
                    if vt.type == 'BWD':
                        self.local_model[layer_id].set_param_requiresgrad()
                else: # P2P
                    raise ValueError("Underdevelopment")
            # record this swapin event for compute stream to wait
            # åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶
            ev_swapin = self.swapin_stream.record_event() 
            # if MEMCPY_NONBLK: self.swapin_stream.synchronize() # Wait for all the kernels in this stream to complete.
            # ready to use
            # å°†ä»»åŠ¡çš„idxå’Œ swap_in äº‹ä»¶ä»¥å…ƒç»„çš„å½¢å¼åŠ å…¥åˆ° get_queue ä¸­
            self.get_queue.add( (layer_id,ev_swapin) )
            if self.nvprof: nvtx_range_pop() 

    # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
    # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    def _wait(self):
        ''' Wait for the running iput. Called in main thread.
            Assumption: only one iput can be running. '''
        assert self.is_running, "no running prefetch"
        self.is_running = False
        return self.get_queue.remove()

    # è¯¥å‡½æ•°é™„å¸¦ä¿é™©æ“ä½œï¼ˆå®é™…ä¸Šå¯èƒ½ä¹‹å‰å°±æ²¡è°ƒç”¨è¿‡inputæ¥ä¸ºå½“å‰çº¿ç¨‹æ·»åŠ é¢„å–ä»»åŠ¡ï¼‰ï¼š
    # æ‹¿å–get_queueï¼ˆé€»è¾‘ä¸Šå®Œæˆé¢„å–çš„ä»»åŠ¡é˜Ÿåˆ—ï¼‰ä¸­çš„é¦–ä¸ªå…ƒç´ ï¼Œå³ç­‰å¾…ä¸€ä¸ªä¹‹å‰å°±è§¦å‘çš„é¢„å–æ¨¡å‹ï¼ˆswapinï¼‰äº‹ä»¶ã€‚æ‹¿å–åªä»£è¡¨é€»è¾‘ä¸Šæ‰§è¡Œå®Œï¼Œ
    # å®é™…ä¸Šå¯èƒ½æ²¡æ‰§è¡Œå®Œï¼Œå› æ­¤éœ€è¦ç­‰å¾…äº‹ä»¶çš„å®Œæˆã€‚æœ€åè¿”å›æ‹¿å–å®Œæˆçš„é¦–ä¸ªå…ƒç´ ä¸­çš„vt_idx
    # ğŸ“Œåˆ†æï¼šinputå’Œgetæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œinputå°†is_runningç½®ä¸ºtrueï¼Œget(è°ƒç”¨wait)å°†is_runningç½®ä¸ºfalseã€‚ä¸è°ƒç”¨getï¼Œis_runningå°±ä¸å¯èƒ½ä¸ºfalse
    #         è‹¥å‘ç°is_runningä¸ä¸ºtrueï¼Œå°±è¯´æ˜ä¹‹å‰æ ¹æœ¬å°±æ²¡æ‰§è¡Œè¿‡layerçš„é¢„å–
    
    # 1.å‡†å¤‡å·¥ä½œ1ï¼šè°ƒç”¨syncpin_handlerå®ä¾‹çš„çº¿ç¨‹å°†vtä¸­çš„è¿™äº›åœ¨cpuå…±äº«å†…å­˜ä¸­çš„layerå¤åˆ¶åˆ°pinned memoryä¸Šï¼›
    # 2.å‡†å¤‡å·¥ä½œ2ï¼šåœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    #   åŒæ—¶ä¹Ÿæ˜¯å½“å‰PrefetchLocalModelGPUå®ä¾‹çš„çº¿ç¨‹çš„è§¦å‘å·¥ä½œï¼Œå°†ä¸œè¥¿æ”¾è¿›put_queueï¼Œè¿™æ„å‘³ç€çº¿ç¨‹å¼€å§‹æ‰§è¡Œ3
    # 3.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 4.è‹¥suc_vtå‚æ•°ä¸ä¸ºç©ºï¼Œæ„å‘³ç€è¯¥å‡½æ•°ä¼šä¸ºæå‰æ‰§è¡Œä¸€éƒ¨åˆ†åç»§ä»»åŠ¡ï¼Œå³è°ƒç”¨self.syncpin_handler.iput(suc_vt)ï¼Œä¸1ç›¸åŒ
    def get(self, layer_id, vt):
        ''' Call by main thread. Blocking wait until current prefetch finish. Then launch next sync pin. Return current vt.idx. '''
        # wait current one (if no current one, get this one)
        # è‹¥å½“å‰æ²¡æœ‰æ­£åœ¨GPUä¸Šåˆ†é…vtçš„æ‰€æœ‰å±‚

        # self.put_queue.add(vt)
        # è¿™æ„å‘³ç€ syncpin_handler è¿™ä¸ªçº¿ç¨‹å¼€å§‹æ‰§è¡Œvtçš„æ¨¡å‹çš„ä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„å¤åˆ¶
        self.syncpin_handler.input_one_layer(layer_id, vt)
        # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
        # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
        # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
        # 
        # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
        # 2.åœ¨é»˜è®¤æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin_stream è¿™ä¸ªæµä¸Šç­‰å¾…
        # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
        # 4.å°† (vt, ev_compute) æ·»åŠ åˆ° put_queue ä¸­
        self.iput(layer_id, vt)
        # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
        # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)ï¼Œå³æ­£åœ¨æˆ–å·²ç»æ‰§è¡Œç©çš„swap_inäº‹ä»¶ï¼Œ(vt.idx,ev_swapin)
        #   è‹¥ _thread_func, å³ swap_in æ²¡åœ¨swapin_streamä¸Šåˆ†é…å®Œï¼Œä¼šé˜»å¡åœ¨ remove() ä¸Š
        # layer_id, ev_swapin = self._wait()
        # ç­‰å¾…è¯¥vtä¸Šæ‰€æœ‰çš„å±‚åœ¨GPUä¸Šå®Œæˆåˆ†é…ç©ºtensor
        # self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
        # è¿”å› cur_vt_idx

        layer_id, ev_swapin = self.get_queue.remove()
        return layer_id, ev_swapin, self.compute_stream
    

    def layer_waiting_futs(self):
        layer_id, ev_swapin = self.get_queue.remove()
        self.compute_stream.wait_event(ev_swapin)

# ç°å·²ä½¿ç”¨éåµŒå¥—çº¿ç¨‹ï¼Œå› æ­¤è¯¥ç±»æš‚æ—¶åºŸå¼ƒï¼Œä¸æ·»åŠ æ ¹æ®self.cpu_layer_idåˆ¤æ–­åŒåŒºåŸŸshared memoryçš„é€»è¾‘
# æ€ä¹ˆåµŒå¥—äº†ï¼šä½¿ç”¨çš„swap_in_cpu_handlerä¸­åˆåµŒå¥—äº†syncpin_handler
class PrefetchLocalModelGPU_for_worker5(object):
    """ Handles flexible prefetch of local model (W,B) from pinned model in background thread.
        Step:
        1) main thread: allocate (W,B) on GPU
        2) prefetch thread: get sync'ed pinned model (W,B)
        3) prefetch thread: copy in (W,B) in swapin_cudaStream
        4) prefetch thread: turn on grad
        
        Assumption: 
        1) always in FIFO ordering. put each task, and get prefetched task. 
        2) use cuda event for unblocking next CPU ops

        NOTE: why main thread allocates? i) 'no memory reuse across different stream' -- PyTorch, ii) different threads use different CUDA address spaces
        NOTE: move vt.medium check back to runtime by reducing granularity to vLayer?
    """
    def __init__(self, syncpin_handler, swap_in_cpu_handler, local_model, layer_num, rank, swapin_stream=None, compute_stream=None, nvprof=False):
        self.syncpin_handler = syncpin_handler
        self.local_model = local_model # list
        self.rank = rank
        # è‹¥æ²¡æœ‰ç»™å®šä¸€ä¸ªstreamï¼Œåˆ™åœ¨å½“å‰rankä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„CUDA stream
        # ç›®å‰æ¥çœ‹ï¼Œswapin_streamå°±æ˜¯ä¸€ä¸ªæ–°çš„æµ
        self.swapin_stream = swapin_stream if swapin_stream is not None else torch.cuda.Stream(device=rank)
        # è¯¥å‚æ•°æ˜¯ç»™å®šäº†çš„ï¼Œå…¶å®å’Œä¸ç»™å®šæ‰§è¡Œelseä¸€æ ·ï¼Œéƒ½æ˜¯cudaä¸Šçš„é»˜è®¤æµ
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        self.nvprof = nvprof
        assert rank == torch.cuda.current_device()
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [vt1, ...] # between main and prefetch thread
        self.get_queue = threadsafe_data_struct.Queue() # [vt1.idx, ...] # between prefetch and main thread
        self.prefetch_thread = threading.Thread(target=self._thread_func)
        self.prefetch_thread.daemon = True
        self.prefetch_thread.start()

        self.is_running = False

        # ğŸ“
        self.swap_in_cpu_handler = swap_in_cpu_handler
        self.layer_num = layer_num
        
        # print("[PrefetchLocalModelGPU] rank{} started prefetch_thread".format(self.rank))

    # inputå°±æ˜¯å‘put_queueä¸­åŠ å…ƒç´ ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    #
    # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
    # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
    # 2.åœ¨é»˜è®¤æµä¸Šï¼ˆev_computeï¼‰è®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
    # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
    # 4.å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    def iput(self, vt):
        ''' Call by main thread. Blocking Alloc & Nonblocking CopyIn.
            Assumption: only one iput can be running. '''
        if vt is None:
            return
        # æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
        assert not self.is_running, "the prefetch is still running"
        # å°† self.is_running æ ‡å¿—è®¾ç½®ä¸º Trueï¼Œè¡¨ç¤ºå½“å‰æœ‰ iput æ“ä½œæ­£åœ¨æ‰§è¡Œã€‚
        self.is_running = True
        assert isinstance(vt, vTask)
        # record previous compute event for swapin stream to wait
        # åœ¨ compute_stream ï¼ˆé»˜è®¤æµï¼‰ä¸Šè®°å½•äº‹ä»¶ ev_computeï¼Œè‹¥å‚æ•°ä¸ºç©ºåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
        ev_compute = self.compute_stream.record_event()
        # allocation in main thread
        if self.nvprof: nvtx_range_push("task{}({}) Alloc(W,B)".format(vt.idx, vt.show_layers())) 

        # è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
        for l in vt.layers: # verified: vt.In['W'].keys==vt.In['B'].keys==vt.layers
            # è‹¥è¯¥å±‚çš„Wå’ŒBçš„åª’ä»‹ä¸ºSHM
            if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
                # ğŸ“
                if vt.has_data and l == 0:
                    self.local_model[l].alloc_param_buf()
                    continue
                elif l == self.layer_num-3:
                    self.local_model[l].alloc_param_buf()
                    continue
                elif l == self.layer_num-2:
                    self.local_model[l].alloc_param_buf()
                    continue
                elif l == self.layer_num-1:
                    self.local_model[l].alloc_param_buf()
                    continue
                self.local_model[l].alloc_param_buf_2(vt.idx, l)
            elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                pass
            else: # P2P
                raise ValueError("Underdevelopment")
        # do the rest in background thread
        # å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­
        self.put_queue.add((vt,ev_compute))
        if self.nvprof: nvtx_range_pop() 

    # ä¸æ–­å°è¯•ä»put_queueä¸­æ‹¿å–(vt, ev_computeï¼ˆå‡†å¤‡å·¥ä½œçš„äº‹ä»¶ï¼Œå³åœ¨GPUä¸Šå…ˆåˆå§‹åŒ–Wå’ŒBï¼‰)ï¼Œæ‰§è¡Œï¼ˆè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ä¼šè¢«é˜»å¡ï¼‰ï¼š
    # 1.ä» put_queue é˜Ÿåˆ—ä¸­å¼¹å‡º (vt, ev_compute)ï¼Œè‹¥é˜Ÿåˆ—æ²¡æœ‰å…ƒç´ ä¼šè¢«é˜»å¡åœ¨è¿™
    # 2.è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
    # 3.åœ¨ CUDA æµ self.swapin_stream ä¸Šç­‰å¾…äº‹ä»¶ ev_compute çš„å®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
    #   å³ç­‰å¾…åœ¨GPUä¸Šåˆå§‹åŒ–vtæ‰€æœ‰å±‚çš„ Wå’ŒB(çš„tensor) çš„å®Œæˆ
    # 4.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 5.åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ ev_swapin
    # 6.å°† (idx(å½“å‰ä»»åŠ¡çš„id),ev_swapin) åŠ å…¥åˆ° get_queue ä¸­
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each incoming task 
            # ä» put_queue é˜Ÿé¦–å¼¹å‡ºä¸€ä¸ªå…ƒç´ ï¼Œè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ï¼Œä¼šè¢«é˜»å¡åœ¨è¿™é‡Œ
            vt, ev_compute = self.put_queue.remove() # blocking
            if self.nvprof: nvtx_range_push("__task{}({}) CopyIn(W,B)".format(vt.idx, vt.show_layers())) 
            # get sync'ed pinned model 
            # è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚
            # å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
            # ğŸ“Œæ ¹æ®syncpin_handlerä¸­çš„æ³¨é‡Šæ¥çœ‹ï¼Œä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„åŒæ­¥æ“ä½œæ˜¯é˜»å¡çš„ï¼ˆå°½ç®¡ä»£ç ä¸Šç›´è§‚æ¥çœ‹æ˜¯éé˜»å¡çš„ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¸€å®šæ‰§è¡Œå®Œäº†
            #   ä»å¦ä¸€ä¸ªè§’åº¦çœ‹ï¼Œè¿™ä¸ªget()æœ¬èº«å°±æ˜¯åœ¨ç­‰å¾…syncçº¿ç¨‹æ‰§è¡Œå®Œä¸€æ¬¡åŒæ­¥æ“ä½œ
            swapin_vt_idx = self.swap_in_cpu_handler.get()
            print(f"rank:{self.rank}, swap inçº¿ç¨‹å·²æ‰§è¡Œå®Œvt nvme->cpuçš„æ‹¿å–({vt.layers})", flush=True)
            # ç¡®ä¿å½“å‰åœ¨GPUä¸Šåˆå§‹åŒ–Wå’ŒBçš„ä»»åŠ¡å’Œå…±äº«å±‚åˆ°pinnedå±‚å¤åˆ¶ä»»åŠ¡çš„å±‚ï¼Œç›®æ ‡æ˜¯åŒä¸€ä¸ªvt
            # ä¸‹é¢å°±æ˜¯è¦æŠŠè¯¥ä»»åŠ¡å¯¹åº”çš„layer packçš„Wå’ŒB swap inåˆ°GPUä¸Š
            assert swapin_vt_idx == vt.idx

            # if vt.has_data:
            #     syncpin_layer_id = self.syncpin_handler.get()
            #     assert syncpin_layer_id == vt.layers[0]
            # elif vt.has_criterion:
            #     syncpin_layer_id = self.syncpin_handler.get()
            #     assert syncpin_layer_id == vt.layers[-2]
            
            # let swapin stream waits for this compute event 
            # ç­‰å¾…äº‹ä»¶ ev_compute åœ¨ CUDA æµ self.swapin_stream ä¸Šå®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
            # å³ç­‰å¾…ä»»åŠ¡vtçš„æ‰€æœ‰å±‚å·²ç»æ‹¿åˆ°GPUä¸Š
            self.swapin_stream.wait_event(ev_compute) 
            # copy in and turn on grad
            # åœ¨swapin_streamä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
            # æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
            with torch.cuda.stream(self.swapin_stream): # context-manager selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.
                for l in vt.layers:
                    # è‹¥Wå’ŒBåœ¨çš„é€šè®¯åª’ä»‹ä¸ºå…±äº«å†…å­˜ï¼Œå°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
                    if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                        
                        if vt.has_data and l == vt.layers[0]:
                            print(f"rank:{self.rank}, layer{l}, ç­‰å¾…syncpin_handler.get()")
                            syncpin_layer_id = self.syncpin_handler.get()
                            print(f"rank:{self.rank}, layer{l}, syncpin_handlerå®Œæˆ")
                            assert syncpin_layer_id == l
                            self.local_model[l].copyin_param_buf()
                        elif l == self.layer_num-3:
                            syncpin_layer_id = self.syncpin_handler.get()
                            assert syncpin_layer_id == l
                            self.local_model[l].copyin_param_buf()
                            print(f"rank:{self.rank}, layer{l}, å·²å®Œæˆcpu->gpuçš„æ‹·è´", flush=True)
                        elif l == self.layer_num-2:
                            syncpin_layer_id = self.syncpin_handler.get()
                            assert syncpin_layer_id == l
                            self.local_model[l].copyin_param_buf()
                            print(f"rank:{self.rank}, layer{l}, å·²å®Œæˆcpu->gpuçš„æ‹·è´", flush=True)
                        elif l == self.layer_num-1:
                            syncpin_layer_id = self.syncpin_handler.get()
                            assert syncpin_layer_id == l
                            self.local_model[l].copyin_param_buf()
                            print(f"rank:{self.rank}, layer{l}, å·²å®Œæˆcpu->gpuçš„æ‹·è´", flush=True)
                        else:
                            # ğŸ“ä»pinned bufferæ‹·è´åˆ°gpuä¸Š
                            print(f"rank:{self.rank}, layer{l}, å‡†å¤‡å¼€å§‹cpu->gpuçš„æ‹·è´", flush=True)
                            self.local_model[l].copyin_param_buf_2(vt.idx, l)
                            print(f"rank:{self.rank}, layer{l}, å·²å®Œæˆcpu->gpuçš„æ‹·è´", flush=True)

                        # è‹¥vtæ˜¯BWDä»»åŠ¡ï¼Œè¿˜éœ€å°†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()

                    # è‹¥Wå’ŒBè¢«pinåœ¨è®¾å¤‡ä¸Šï¼Œæ˜¾ç„¶ä¸ç”¨æ‰§è¡Œæ‹·è´æ“ä½œ
                    elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()
                    else: # P2P
                        raise ValueError("Underdevelopment")
            # record this swapin event for compute stream to wait
            # åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶
            ev_swapin = self.swapin_stream.record_event() 
            # if MEMCPY_NONBLK: self.swapin_stream.synchronize() # Wait for all the kernels in this stream to complete.
            # ready to use
            # å°†ä»»åŠ¡çš„idxå’Œ swap_in äº‹ä»¶ä»¥å…ƒç»„çš„å½¢å¼åŠ å…¥åˆ° get_queue ä¸­
            self.get_queue.add( (vt.idx,ev_swapin) )
            if self.nvprof: nvtx_range_pop() 

    # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
    # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    def _wait(self):
        ''' Wait for the running iput. Called in main thread.
            Assumption: only one iput can be running. '''
        assert self.is_running, "no running prefetch"
        self.is_running = False
        return self.get_queue.remove()

    # è¯¥å‡½æ•°é™„å¸¦ä¿é™©æ“ä½œï¼ˆå®é™…ä¸Šå¯èƒ½ä¹‹å‰å°±æ²¡è°ƒç”¨è¿‡inputæ¥ä¸ºå½“å‰çº¿ç¨‹æ·»åŠ é¢„å–ä»»åŠ¡ï¼‰ï¼š
    # æ‹¿å–get_queueï¼ˆé€»è¾‘ä¸Šå®Œæˆé¢„å–çš„ä»»åŠ¡é˜Ÿåˆ—ï¼‰ä¸­çš„é¦–ä¸ªå…ƒç´ ï¼Œå³ç­‰å¾…ä¸€ä¸ªä¹‹å‰å°±è§¦å‘çš„é¢„å–æ¨¡å‹ï¼ˆswapinï¼‰äº‹ä»¶ã€‚æ‹¿å–åªä»£è¡¨é€»è¾‘ä¸Šæ‰§è¡Œå®Œï¼Œ
    # å®é™…ä¸Šå¯èƒ½æ²¡æ‰§è¡Œå®Œï¼Œå› æ­¤éœ€è¦ç­‰å¾…äº‹ä»¶çš„å®Œæˆã€‚æœ€åè¿”å›æ‹¿å–å®Œæˆçš„é¦–ä¸ªå…ƒç´ ä¸­çš„vt_idx
    # ğŸ“Œåˆ†æï¼šinputå’Œgetæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œinputå°†is_runningç½®ä¸ºtrueï¼Œget(è°ƒç”¨wait)å°†is_runningç½®ä¸ºfalseã€‚ä¸è°ƒç”¨getï¼Œis_runningå°±ä¸å¯èƒ½ä¸ºfalse
    #         è‹¥å‘ç°is_runningä¸ä¸ºtrueï¼Œå°±è¯´æ˜ä¹‹å‰æ ¹æœ¬å°±æ²¡æ‰§è¡Œè¿‡layerçš„é¢„å–
    
    # 1.å‡†å¤‡å·¥ä½œ1ï¼šè°ƒç”¨syncpin_handlerå®ä¾‹çš„çº¿ç¨‹å°†vtä¸­çš„è¿™äº›åœ¨cpuå…±äº«å†…å­˜ä¸­çš„layerå¤åˆ¶åˆ°pinned memoryä¸Šï¼›
    # 2.å‡†å¤‡å·¥ä½œ2ï¼šåœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    #   åŒæ—¶ä¹Ÿæ˜¯å½“å‰PrefetchLocalModelGPUå®ä¾‹çš„çº¿ç¨‹çš„è§¦å‘å·¥ä½œï¼Œå°†ä¸œè¥¿æ”¾è¿›put_queueï¼Œè¿™æ„å‘³ç€çº¿ç¨‹å¼€å§‹æ‰§è¡Œ3
    # 3.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 4.è°ƒç”¨_waitå°†is_running ç½®ä¸ºfalseï¼Œè¿”å›get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    # 5.self.compute_stream.wait_event(ev_swapin)
    # 6.è‹¥suc_vtå‚æ•°ä¸ä¸ºç©ºï¼Œæ„å‘³ç€è¯¥å‡½æ•°ä¼šä¸ºæå‰æ‰§è¡Œä¸€éƒ¨åˆ†åç»§ä»»åŠ¡ï¼Œå³è°ƒç”¨self.syncpin_handler.iput(suc_vt)ï¼Œä¸1ç›¸åŒ
    def get(self, vt, suc_vt=None):
        ''' Call by main thread. Blocking wait until current prefetch finish. Then launch next sync pin. Return current vt.idx. '''
        assert isinstance(vt, vTask)
        # wait current one (if no current one, get this one)
        # è‹¥å½“å‰æ²¡æœ‰æ­£åœ¨GPUä¸Šåˆ†é…vtçš„æ‰€æœ‰å±‚
        print(f"rank:{self.rank}, å‡†å¤‡æ‰§è¡Œgetçš„vtä¸º{vt.layers}")
        if not self.is_running:

            # è‹¥vtåŒ…å«é¦–å±‚ï¼Œè¯¥å±‚å› ä¸ºä¸ä¼šå¸è½½åˆ°nvmeï¼Œéœ€è¦ä»shared memoryå¤åˆ¶åˆ°Pinned memory
            # if vt.has_data:
            #     self.syncpin_handler.input_one_layer(vt.layers[0], vt)
            # if vt.has_criterion:
            #     self.syncpin_handler.input_one_layer(vt.layers[-2], vt)
            # if 

            # ğŸ“ç°åœ¨æ— éœ€shared model->pinned modelçš„å¤åˆ¶ï¼Œç›´æ¥ä»NVMeæ‹¿
            self.swap_in_cpu_handler.iput(vt)


            # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
            # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
            # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
            # 
            # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
            # 2.åœ¨é»˜è®¤æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin_stream è¿™ä¸ªæµä¸Šç­‰å¾…
            # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
            # 4.å°† (vt, ev_compute) æ·»åŠ åˆ° put_queue ä¸­
            self.iput(vt)
        # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
        # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)ï¼Œå³æ­£åœ¨æˆ–å·²ç»æ‰§è¡Œç©çš„swap_inäº‹ä»¶ï¼Œ(vt.idx,ev_swapin)
        #   è‹¥ _thread_func, å³ swap_in æ²¡åœ¨swapin_streamä¸Šåˆ†é…å®Œï¼Œä¼šé˜»å¡åœ¨ remove() ä¸Š
        cur_vt_idx, ev_swapin = self._wait()
        # ç­‰å¾…è¯¥vtä¸Šæ‰€æœ‰çš„å±‚åœ¨GPUä¸Šå®Œæˆåˆ†é…ç©ºtensor
        self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
        # syncpin next one if exsits
        # è‹¥ç»™å®šäº†åç»§ä»»åŠ¡ï¼Œ
        if suc_vt is not None:
            # ğŸ“
            self.swap_in_cpu_handler.iput(suc_vt)
        # è¿”å› cur_vt_idx
        return cur_vt_idx


# ä¸Šä¸€ä¸ªé¢„å–ç±»ä½¿ç”¨äº†åµŒå¥—çš„çº¿ç¨‹æ‹¿å–æ¨¡å‹ï¼Œå³åœ¨è‡ªå·±å†™çš„swap_in_cpuç±»çš„çº¿ç¨‹ä¸­åµŒå¥—syncpinmodelinbkgd
# è¯¥ç±»åªä½¿ç”¨syncpinmodelinbkgdï¼Œå…¶ä¸­ç›´æ¥ä½¿ç”¨swap_in_cpuçº¿ç¨‹çš„åŠŸèƒ½
class PrefetchLocalModelGPU_for_worker5_2(object):
    """ Handles flexible prefetch of local model (W,B) from pinned model in background thread.
        Step:
        1) main thread: allocate (W,B) on GPU
        2) prefetch thread: get sync'ed pinned model (W,B)
        3) prefetch thread: copy in (W,B) in swapin_cudaStream
        4) prefetch thread: turn on grad
        
        Assumption: 
        1) always in FIFO ordering. put each task, and get prefetched task. 
        2) use cuda event for unblocking next CPU ops

        NOTE: why main thread allocates? i) 'no memory reuse across different stream' -- PyTorch, ii) different threads use different CUDA address spaces
        NOTE: move vt.medium check back to runtime by reducing granularity to vLayer?
    """
    def __init__(self, syncpin_handler, local_model, layer_num, cpu_layer_id, rank, swapin_stream=None, compute_stream=None, nvprof=False):
        self.syncpin_handler = syncpin_handler
        self.local_model = local_model # list
        self.rank = rank
        # è‹¥æ²¡æœ‰ç»™å®šä¸€ä¸ªstreamï¼Œåˆ™åœ¨å½“å‰rankä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„CUDA stream
        # ç›®å‰æ¥çœ‹ï¼Œswapin_streamå°±æ˜¯ä¸€ä¸ªæ–°çš„æµ
        self.swapin_stream = swapin_stream if swapin_stream is not None else torch.cuda.Stream(device=rank)
        # è¯¥å‚æ•°æ˜¯ç»™å®šäº†çš„ï¼Œå…¶å®å’Œä¸ç»™å®šæ‰§è¡Œelseä¸€æ ·ï¼Œéƒ½æ˜¯cudaä¸Šçš„é»˜è®¤æµ
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        self.nvprof = nvprof
        assert rank == torch.cuda.current_device()
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [vt1, ...] # between main and prefetch thread
        self.get_queue = threadsafe_data_struct.Queue() # [vt1.idx, ...] # between prefetch and main thread
        self.prefetch_thread = threading.Thread(target=self._thread_func)
        self.prefetch_thread.daemon = True
        self.prefetch_thread.start()
        
        self.is_running = False

        # ğŸ“
        self.layer_num = layer_num
        self.cpu_layer_id = cpu_layer_id
        # print("[PrefetchLocalModelGPU] rank{} started prefetch_thread".format(self.rank))

    # inputå°±æ˜¯å‘put_queueä¸­åŠ å…ƒç´ ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    #
    # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
    # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
    # 2.åœ¨é»˜è®¤æµä¸Šï¼ˆev_computeï¼‰è®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
    # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
    # 4.å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    def iput(self, vt):
        ''' Call by main thread. Blocking Alloc & Nonblocking CopyIn.
            Assumption: only one iput can be running. '''
        if vt is None:
            return
        # æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
        assert not self.is_running, "the prefetch is still running"
        # å°† self.is_running æ ‡å¿—è®¾ç½®ä¸º Trueï¼Œè¡¨ç¤ºå½“å‰æœ‰ iput æ“ä½œæ­£åœ¨æ‰§è¡Œã€‚
        self.is_running = True
        assert isinstance(vt, vTask)
        # record previous compute event for swapin stream to wait
        # åœ¨ compute_stream ï¼ˆé»˜è®¤æµï¼‰ä¸Šè®°å½•äº‹ä»¶ ev_computeï¼Œè‹¥å‚æ•°ä¸ºç©ºåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
        ev_compute = self.compute_stream.record_event()
        # allocation in main thread
        if self.nvprof: nvtx_range_push("task{}({}) Alloc(W,B)".format(vt.idx, vt.show_layers())) 

        # è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
        for l in vt.layers: # verified: vt.In['W'].keys==vt.In['B'].keys==vt.layers
            # è‹¥è¯¥å±‚çš„Wå’ŒBçš„åª’ä»‹ä¸ºSHM
            if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
                # ğŸ“
                if l in self.cpu_layer_id: #l == 0 or l == self.layer_num-3 or l == self.layer_num-2 or l == self.layer_num-1:
                    self.local_model[l].alloc_param_buf()
                    continue
                else:
                    self.local_model[l].alloc_param_buf_2(vt.idx, l)
            elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                pass
            else: # P2P
                raise ValueError("Underdevelopment")
        # do the rest in background thread
        # å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­
        self.put_queue.add((vt,ev_compute))
        if self.nvprof: nvtx_range_pop() 

    # ä¸æ–­å°è¯•ä»put_queueä¸­æ‹¿å–(vt, ev_computeï¼ˆå‡†å¤‡å·¥ä½œçš„äº‹ä»¶ï¼Œå³åœ¨GPUä¸Šå…ˆåˆå§‹åŒ–Wå’ŒBï¼‰)ï¼Œæ‰§è¡Œï¼ˆè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ä¼šè¢«é˜»å¡ï¼‰ï¼š
    # 1.ä» put_queue é˜Ÿåˆ—ä¸­å¼¹å‡º (vt, ev_compute)ï¼Œè‹¥é˜Ÿåˆ—æ²¡æœ‰å…ƒç´ ä¼šè¢«é˜»å¡åœ¨è¿™
    # 2.è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
    # 3.åœ¨ CUDA æµ self.swapin_stream ä¸Šç­‰å¾…äº‹ä»¶ ev_compute çš„å®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
    #   å³ç­‰å¾…åœ¨GPUä¸Šåˆå§‹åŒ–vtæ‰€æœ‰å±‚çš„ Wå’ŒB(çš„tensor) çš„å®Œæˆ
    # 4.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 5.åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ ev_swapin
    # 6.å°† (idx(å½“å‰ä»»åŠ¡çš„id),ev_swapin) åŠ å…¥åˆ° get_queue ä¸­
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each incoming task 
            # ä» put_queue é˜Ÿé¦–å¼¹å‡ºä¸€ä¸ªå…ƒç´ ï¼Œè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ï¼Œä¼šè¢«é˜»å¡åœ¨è¿™é‡Œ
            vt, ev_compute = self.put_queue.remove() # blocking
            if self.nvprof: nvtx_range_push("__task{}({}) CopyIn(W,B)".format(vt.idx, vt.show_layers())) 
            # get sync'ed pinned model 
            # è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚
            # å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
            # ğŸ“Œæ ¹æ®syncpin_handlerä¸­çš„æ³¨é‡Šæ¥çœ‹ï¼Œä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„åŒæ­¥æ“ä½œæ˜¯é˜»å¡çš„ï¼ˆå°½ç®¡ä»£ç ä¸Šç›´è§‚æ¥çœ‹æ˜¯éé˜»å¡çš„ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¸€å®šæ‰§è¡Œå®Œäº†
            #   ä»å¦ä¸€ä¸ªè§’åº¦çœ‹ï¼Œè¿™ä¸ªget()æœ¬èº«å°±æ˜¯åœ¨ç­‰å¾…syncçº¿ç¨‹æ‰§è¡Œå®Œä¸€æ¬¡åŒæ­¥æ“ä½œ
            syncpin_vt_idx = self.syncpin_handler.get()
            # ç¡®ä¿å½“å‰åœ¨GPUä¸Šåˆå§‹åŒ–Wå’ŒBçš„ä»»åŠ¡å’Œå…±äº«å±‚åˆ°pinnedå±‚å¤åˆ¶ä»»åŠ¡çš„å±‚ï¼Œç›®æ ‡æ˜¯åŒä¸€ä¸ªvt
            # ä¸‹é¢å°±æ˜¯è¦æŠŠè¯¥ä»»åŠ¡å¯¹åº”çš„layer packçš„Wå’ŒB swap inåˆ°GPUä¸Š
            assert syncpin_vt_idx == vt.idx
            # let swapin stream waits for this compute event 
            # ç­‰å¾…äº‹ä»¶ ev_compute åœ¨ CUDA æµ self.swapin_stream ä¸Šå®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
            # å³ç­‰å¾…ä»»åŠ¡vtçš„æ‰€æœ‰å±‚å·²ç»æ‹¿åˆ°GPUä¸Š
            self.swapin_stream.wait_event(ev_compute) 
            # copy in and turn on grad
            # åœ¨swapin_streamä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
            # æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
            with torch.cuda.stream(self.swapin_stream): # context-manager selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.
                for l in vt.layers:
                    # è‹¥Wå’ŒBåœ¨çš„é€šè®¯åª’ä»‹ä¸ºå…±äº«å†…å­˜ï¼Œå°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
                    if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                        # ğŸ“Œ
                        self.local_model[l].copyin_param_buf_2(vt.idx, l)
                        # è‹¥vtæ˜¯BWDä»»åŠ¡ï¼Œè¿˜éœ€å°†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()
                    # è‹¥Wå’ŒBè¢«pinåœ¨è®¾å¤‡ä¸Šï¼Œæ˜¾ç„¶ä¸ç”¨æ‰§è¡Œæ‹·è´æ“ä½œ
                    elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()
                    else: # P2P
                        raise ValueError("Underdevelopment")
            # record this swapin event for compute stream to wait
            # åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶
            ev_swapin = self.swapin_stream.record_event() 
            # if MEMCPY_NONBLK: self.swapin_stream.synchronize() # Wait for all the kernels in this stream to complete.
            # ready to use
            # å°†ä»»åŠ¡çš„idxå’Œ swap_in äº‹ä»¶ä»¥å…ƒç»„çš„å½¢å¼åŠ å…¥åˆ° get_queue ä¸­
            self.get_queue.add( (vt.idx,ev_swapin) )
            if self.nvprof: nvtx_range_pop() 

    # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
    # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    def _wait(self):
        ''' Wait for the running iput. Called in main thread.
            Assumption: only one iput can be running. '''
        assert self.is_running, "no running prefetch"
        self.is_running = False
        return self.get_queue.remove()

    # è¯¥å‡½æ•°é™„å¸¦ä¿é™©æ“ä½œï¼ˆå®é™…ä¸Šå¯èƒ½ä¹‹å‰å°±æ²¡è°ƒç”¨è¿‡inputæ¥ä¸ºå½“å‰çº¿ç¨‹æ·»åŠ é¢„å–ä»»åŠ¡ï¼‰ï¼š
    # æ‹¿å–get_queueï¼ˆé€»è¾‘ä¸Šå®Œæˆé¢„å–çš„ä»»åŠ¡é˜Ÿåˆ—ï¼‰ä¸­çš„é¦–ä¸ªå…ƒç´ ï¼Œå³ç­‰å¾…ä¸€ä¸ªä¹‹å‰å°±è§¦å‘çš„é¢„å–æ¨¡å‹ï¼ˆswapinï¼‰äº‹ä»¶ã€‚æ‹¿å–åªä»£è¡¨é€»è¾‘ä¸Šæ‰§è¡Œå®Œï¼Œ
    # å®é™…ä¸Šå¯èƒ½æ²¡æ‰§è¡Œå®Œï¼Œå› æ­¤éœ€è¦ç­‰å¾…äº‹ä»¶çš„å®Œæˆã€‚æœ€åè¿”å›æ‹¿å–å®Œæˆçš„é¦–ä¸ªå…ƒç´ ä¸­çš„vt_idx
    # ğŸ“Œåˆ†æï¼šinputå’Œgetæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œinputå°†is_runningç½®ä¸ºtrueï¼Œget(è°ƒç”¨wait)å°†is_runningç½®ä¸ºfalseã€‚ä¸è°ƒç”¨getï¼Œis_runningå°±ä¸å¯èƒ½ä¸ºfalse
    #         è‹¥å‘ç°is_runningä¸ä¸ºtrueï¼Œå°±è¯´æ˜ä¹‹å‰æ ¹æœ¬å°±æ²¡æ‰§è¡Œè¿‡layerçš„é¢„å–
    
    # 1.å‡†å¤‡å·¥ä½œ1ï¼šè°ƒç”¨syncpin_handlerå®ä¾‹çš„çº¿ç¨‹å°†vtä¸­çš„è¿™äº›åœ¨cpuå…±äº«å†…å­˜ä¸­çš„layerå¤åˆ¶åˆ°pinned memoryä¸Šï¼›
    # 2.å‡†å¤‡å·¥ä½œ2ï¼šåœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    #   åŒæ—¶ä¹Ÿæ˜¯å½“å‰PrefetchLocalModelGPUå®ä¾‹çš„çº¿ç¨‹çš„è§¦å‘å·¥ä½œï¼Œå°†ä¸œè¥¿æ”¾è¿›put_queueï¼Œè¿™æ„å‘³ç€çº¿ç¨‹å¼€å§‹æ‰§è¡Œ3
    # 3.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 4.è°ƒç”¨_waitå°†is_running ç½®ä¸ºfalseï¼Œè¿”å›get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    # 5.self.compute_stream.wait_event(ev_swapin)
    # 6.è‹¥suc_vtå‚æ•°ä¸ä¸ºç©ºï¼Œæ„å‘³ç€è¯¥å‡½æ•°ä¼šä¸ºæå‰æ‰§è¡Œä¸€éƒ¨åˆ†åç»§ä»»åŠ¡ï¼Œå³è°ƒç”¨self.syncpin_handler.iput(suc_vt)ï¼Œä¸1ç›¸åŒ
    def get(self, vt, suc_vt=None):
        ''' Call by main thread. Blocking wait until current prefetch finish. Then launch next sync pin. Return current vt.idx. '''
        assert isinstance(vt, vTask)
        # wait current one (if no current one, get this one)
        # è‹¥å½“å‰æ²¡æœ‰æ­£åœ¨GPUä¸Šåˆ†é…vtçš„æ‰€æœ‰å±‚
        if not self.is_running:
            # self.put_queue.add(vt)
            # è¿™æ„å‘³ç€ syncpin_handler è¿™ä¸ªçº¿ç¨‹å¼€å§‹æ‰§è¡Œvtçš„æ¨¡å‹çš„ä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„å¤åˆ¶
            self.syncpin_handler.iput(vt)
            # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
            # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
            # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
            # 
            # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
            # 2.åœ¨é»˜è®¤æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin_stream è¿™ä¸ªæµä¸Šç­‰å¾…
            # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
            # 4.å°† (vt, ev_compute) æ·»åŠ åˆ° put_queue ä¸­
            self.iput(vt)
        # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
        # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)ï¼Œå³æ­£åœ¨æˆ–å·²ç»æ‰§è¡Œç©çš„swap_inäº‹ä»¶ï¼Œ(vt.idx,ev_swapin)
        #   è‹¥ _thread_func, å³ swap_in æ²¡åœ¨swapin_streamä¸Šåˆ†é…å®Œï¼Œä¼šé˜»å¡åœ¨ remove() ä¸Š
        cur_vt_idx, ev_swapin = self._wait()
        # ç­‰å¾…è¯¥vtä¸Šæ‰€æœ‰çš„å±‚åœ¨GPUä¸Šå®Œæˆåˆ†é…ç©ºtensor
        self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
        # syncpin next one if exsits
        # è‹¥ç»™å®šäº†åç»§ä»»åŠ¡ï¼Œ
        if suc_vt is not None:
            self.syncpin_handler.iput(suc_vt)
        # è¿”å› cur_vt_idx
        return cur_vt_idx

class PrefetchLocalModelGPU_for_worker5_double_buffer(object):
    """ Handles flexible prefetch of local model (W,B) from pinned model in background thread.
        Step:
        1) main thread: allocate (W,B) on GPU
        2) prefetch thread: get sync'ed pinned model (W,B)
        3) prefetch thread: copy in (W,B) in swapin_cudaStream
        4) prefetch thread: turn on grad
        
        Assumption: 
        1) always in FIFO ordering. put each task, and get prefetched task. 
        2) use cuda event for unblocking next CPU ops

        NOTE: why main thread allocates? i) 'no memory reuse across different stream' -- PyTorch, ii) different threads use different CUDA address spaces
        NOTE: move vt.medium check back to runtime by reducing granularity to vLayer?
    """
    def __init__(self, syncpin_handler, shared_model_nvme, local_model, layer_num, cpu_layer_id, rank, swapin_stream=None, compute_stream=None, nvprof=False):
        self.syncpin_handler = syncpin_handler
        self.shared_model_nvme = shared_model_nvme
        self.local_model = local_model # list
        self.rank = rank
        # è‹¥æ²¡æœ‰ç»™å®šä¸€ä¸ªstreamï¼Œåˆ™åœ¨å½“å‰rankä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„CUDA stream
        # ç›®å‰æ¥çœ‹ï¼Œswapin_streamå°±æ˜¯ä¸€ä¸ªæ–°çš„æµ
        self.swapin_stream = swapin_stream if swapin_stream is not None else torch.cuda.Stream(device=rank)
        # è¯¥å‚æ•°æ˜¯ç»™å®šäº†çš„ï¼Œå…¶å®å’Œä¸ç»™å®šæ‰§è¡Œelseä¸€æ ·ï¼Œéƒ½æ˜¯cudaä¸Šçš„é»˜è®¤æµ
        self.compute_stream = compute_stream if compute_stream is not None else torch.cuda.default_stream(device=rank)
        self.nvprof = nvprof
        assert rank == torch.cuda.current_device()
        # initialize
        self.put_queue = threadsafe_data_struct.Queue() # [vt1, ...] # between main and prefetch thread
        self.get_queue = threadsafe_data_struct.Queue() # [vt1.idx, ...] # between prefetch and main thread
        self.prefetch_thread = threading.Thread(target=self._thread_func)
        self.prefetch_thread.daemon = True
        self.prefetch_thread.start()
        
        self.is_running = False

        # ğŸ“
        self.layer_num = layer_num
        self.cpu_layer_id = cpu_layer_id
        # print("[PrefetchLocalModelGPU] rank{} started prefetch_thread".format(self.rank))

    # inputå°±æ˜¯å‘put_queueä¸­åŠ å…ƒç´ ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    #
    # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
    # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
    # 2.åœ¨é»˜è®¤æµä¸Šï¼ˆev_computeï¼‰è®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
    # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
    # 4.å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­ï¼ŒğŸ“Œè¿™ä¹Ÿæ„å‘³ç€è¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
    def iput(self, vt):
        ''' Call by main thread. Blocking Alloc & Nonblocking CopyIn.
            Assumption: only one iput can be running. '''
        if vt is None:
            return
        # æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
        assert not self.is_running, "the prefetch is still running"
        # å°† self.is_running æ ‡å¿—è®¾ç½®ä¸º Trueï¼Œè¡¨ç¤ºå½“å‰æœ‰ iput æ“ä½œæ­£åœ¨æ‰§è¡Œã€‚
        self.is_running = True
        assert isinstance(vt, vTask)
        # record previous compute event for swapin stream to wait
        # åœ¨ compute_stream ï¼ˆé»˜è®¤æµï¼‰ä¸Šè®°å½•äº‹ä»¶ ev_computeï¼Œè‹¥å‚æ•°ä¸ºç©ºåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼Œç”¨äºåç»­åœ¨ swapin æµä¸Šç­‰å¾…
        ev_compute = self.compute_stream.record_event()
        # allocation in main thread
        if self.nvprof: nvtx_range_push("task{}({}) Alloc(W,B)".format(vt.idx, vt.show_layers())) 

        # è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Š(pinned memory)å‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
        for l in vt.layers: # verified: vt.In['W'].keys==vt.In['B'].keys==vt.layers
            # è‹¥è¯¥å±‚çš„Wå’ŒBçš„åª’ä»‹ä¸ºSHM
            if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                # åˆ†é…æ¨¡å‹çš„å‚æ•°å’Œbufferï¼Œå³æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensorï¼ˆåˆ†é…å†…å­˜ä¸åˆå§‹åŒ–å€¼ï¼‰
                # ğŸ“
                if l in self.cpu_layer_id:
                    self.local_model[l].alloc_param_buf()
                    continue
                else:
                    self.local_model[l].alloc_param_buf_2(vt.idx, l)
            elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                pass
            else: # P2P
                raise ValueError("Underdevelopment")
            
        # do the rest in background thread
        # å°†(vt, ev_compute)æ·»åŠ åˆ° put_queue ä¸­
        self.put_queue.add((vt,ev_compute))
        if self.nvprof: nvtx_range_pop() 

    # ä¸æ–­å°è¯•ä»put_queueä¸­æ‹¿å–(vt, ev_computeï¼ˆå‡†å¤‡å·¥ä½œçš„äº‹ä»¶ï¼Œå³åœ¨GPUä¸Šå…ˆåˆå§‹åŒ–Wå’ŒBï¼‰)ï¼Œæ‰§è¡Œï¼ˆè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ä¼šè¢«é˜»å¡ï¼‰ï¼š
    # 1.ä» put_queue é˜Ÿåˆ—ä¸­å¼¹å‡º (vt, ev_compute)ï¼Œè‹¥é˜Ÿåˆ—æ²¡æœ‰å…ƒç´ ä¼šè¢«é˜»å¡åœ¨è¿™
    # 2.è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
    # 3.åœ¨ CUDA æµ self.swapin_stream ä¸Šç­‰å¾…äº‹ä»¶ ev_compute çš„å®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
    #   å³ç­‰å¾…åœ¨GPUä¸Šåˆå§‹åŒ–vtæ‰€æœ‰å±‚çš„ Wå’ŒB(çš„tensor) çš„å®Œæˆ
    # 4.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 5.åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ ev_swapin
    # 6.å°† (idx(å½“å‰ä»»åŠ¡çš„id),ev_swapin) åŠ å…¥åˆ° get_queue ä¸­
    def _thread_func(self):
        """ This method is to be executed from a daemon thread. """
        while True: # for each incoming task 
            # ä» put_queue é˜Ÿé¦–å¼¹å‡ºä¸€ä¸ªå…ƒç´ ï¼Œè‹¥é˜Ÿåˆ—æ˜¯ç©ºçš„ï¼Œä¼šè¢«é˜»å¡åœ¨è¿™é‡Œ
            vt, ev_compute = self.put_queue.remove() # blocking
            if self.nvprof: nvtx_range_push("__task{}({}) CopyIn(W,B)".format(vt.idx, vt.show_layers())) 
            # get sync'ed pinned model 
            # è¿”å›syncpin_handlerå®ä¾‹çš„get_queueçš„é¦–ä¸ªå…ƒç´ ï¼ˆä»»åŠ¡çš„idxï¼‰ï¼Œå¹¶æŠŠå…¶ä»é˜Ÿåˆ—ä¸­åˆ é™¤ã€‚
            # å³è¿”å›å·²ç»å®Œæˆä»å…±äº«æ¨¡å‹åˆ°å›ºå®šå†…å­˜æ¨¡å‹å¤åˆ¶W,Bçš„ä»»åŠ¡ï¼ˆå³åŒæ­¥ï¼‰
            # ğŸ“Œæ ¹æ®syncpin_handlerä¸­çš„æ³¨é‡Šæ¥çœ‹ï¼Œä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„åŒæ­¥æ“ä½œæ˜¯é˜»å¡çš„ï¼ˆå°½ç®¡ä»£ç ä¸Šç›´è§‚æ¥çœ‹æ˜¯éé˜»å¡çš„ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¸€å®šæ‰§è¡Œå®Œäº†
            #   ä»å¦ä¸€ä¸ªè§’åº¦çœ‹ï¼Œè¿™ä¸ªget()æœ¬èº«å°±æ˜¯åœ¨ç­‰å¾…syncçº¿ç¨‹æ‰§è¡Œå®Œä¸€æ¬¡åŒæ­¥æ“ä½œ
            syncpin_vt_idx, buffer_id = self.syncpin_handler.get()
            # ç¡®ä¿å½“å‰åœ¨GPUä¸Šåˆå§‹åŒ–Wå’ŒBçš„ä»»åŠ¡å’Œå…±äº«å±‚åˆ°pinnedå±‚å¤åˆ¶ä»»åŠ¡çš„å±‚ï¼Œç›®æ ‡æ˜¯åŒä¸€ä¸ªvt
            # ä¸‹é¢å°±æ˜¯è¦æŠŠè¯¥ä»»åŠ¡å¯¹åº”çš„layer packçš„Wå’ŒB swap inåˆ°GPUä¸Š
            assert syncpin_vt_idx == vt.idx
            # let swapin stream waits for this compute event 
            # ç­‰å¾…äº‹ä»¶ ev_compute åœ¨ CUDA æµ self.swapin_stream ä¸Šå®Œæˆï¼Œç„¶åå†ç»§ç»­æ‰§è¡Œåç»­çš„æ“ä½œï¼ˆåç»­æäº¤ç»™è¯¥stramçš„æ‰€æœ‰workéƒ½å¾—ç­‰ï¼‰
            # å³ç­‰å¾…ä»»åŠ¡vtçš„æ‰€æœ‰å±‚å·²ç»æ‹¿åˆ°GPUä¸Š
            self.swapin_stream.wait_event(ev_compute) 
            # copy in and turn on grad
            # åœ¨swapin_streamä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
            # æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
            with torch.cuda.stream(self.swapin_stream): # context-manager selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.
                for l in vt.layers:
                    # è‹¥Wå’ŒBåœ¨çš„é€šè®¯åª’ä»‹ä¸ºå…±äº«å†…å­˜ï¼Œå°†cpuå†…å­˜modelä¸­çš„å‚æ•°å’Œç¼“å†²åŒºæ•°æ®æ‹·è´åˆ°gpu modelä¸Š
                    if vt.In['W'][l].medium=='SHM' and vt.In['B'][l].medium=='SHM':
                        # ğŸ“Œ
                        print(f"rank{self.rank}, vt.idx:{vt.idx}({vt.layers}), {vt.type}, {l}, å¼€å§‹cpu->gpuçš„å¤åˆ¶")
                        self.local_model[l].copyin_param_buf_for_double_buffer(buffer_id, vt.idx, l)
                        # è‹¥vtæ˜¯BWDä»»åŠ¡ï¼Œè¿˜éœ€å°†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä¸ºéœ€è¦æ¢¯åº¦
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()
                    # è‹¥Wå’ŒBè¢«pinåœ¨è®¾å¤‡ä¸Šï¼Œæ˜¾ç„¶ä¸ç”¨æ‰§è¡Œæ‹·è´æ“ä½œ
                    elif vt.In['W'][l].medium=='PIN' and vt.In['B'][l].medium=='PIN':
                        if vt.type == 'BWD':
                            self.local_model[l].set_param_requiresgrad()
                    else: # P2P
                        raise ValueError("Underdevelopment")
            # record this swapin event for compute stream to wait
            # åœ¨å½“å‰ CUDA æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶
            ev_swapin = self.swapin_stream.record_event() 

            # ğŸ“Œå®Œæˆå¤åˆ¶å, FWDå³å¯é‡Šæ”¾pinned buffer
            if vt.type == 'FWD':
                print(f"rank:{self.rank}, vt.layers:{vt.layers}, vt.Out:{vt.Out}")
                # all_pin = all(vt.Out['W'][l].medium=='PIN' and vt.Out['B'][l].medium=='PIN' for l in vt.layers)
                not_all_pin = all(not (l in vt.Out['W']) and not (l in vt.Out['B']) for l in vt.layers)
                if not_all_pin:
                    self.shared_model_nvme.release_buffer(buffer_id)
                    print(f"rank:{self.rank}, ---------------------FWDæˆåŠŸé‡Šæ”¾buffer_id:{buffer_id}")
                else:
                    if all(vt.Out['W'][l].medium=='PIN' and vt.Out['B'][l].medium=='PIN' for l in vt.layers):
                        print(f"rank:{self.rank}, ---------------------FWDä½¿ç”¨PINåª’ä»‹, ä¿ç•™buffer_id:{buffer_id}")
                    
            # if MEMCPY_NONBLK: self.swapin_stream.synchronize() # Wait for all the kernels in this stream to complete.
            # ready to use
            # å°†ä»»åŠ¡çš„idxå’Œ swap_in äº‹ä»¶ä»¥å…ƒç»„çš„å½¢å¼åŠ å…¥åˆ° get_queue ä¸­
            self.get_queue.add( (vt.idx,ev_swapin) )
            if self.nvprof: nvtx_range_pop() 

    # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
    # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    def _wait(self):
        ''' Wait for the running iput. Called in main thread.
            Assumption: only one iput can be running. '''
        assert self.is_running, "no running prefetch"
        self.is_running = False
        return self.get_queue.remove()

    # è¯¥å‡½æ•°é™„å¸¦ä¿é™©æ“ä½œï¼ˆå®é™…ä¸Šå¯èƒ½ä¹‹å‰å°±æ²¡è°ƒç”¨è¿‡inputæ¥ä¸ºå½“å‰çº¿ç¨‹æ·»åŠ é¢„å–ä»»åŠ¡ï¼‰ï¼š
    # æ‹¿å–get_queueï¼ˆé€»è¾‘ä¸Šå®Œæˆé¢„å–çš„ä»»åŠ¡é˜Ÿåˆ—ï¼‰ä¸­çš„é¦–ä¸ªå…ƒç´ ï¼Œå³ç­‰å¾…ä¸€ä¸ªä¹‹å‰å°±è§¦å‘çš„é¢„å–æ¨¡å‹ï¼ˆswapinï¼‰äº‹ä»¶ã€‚æ‹¿å–åªä»£è¡¨é€»è¾‘ä¸Šæ‰§è¡Œå®Œï¼Œ
    # å®é™…ä¸Šå¯èƒ½æ²¡æ‰§è¡Œå®Œï¼Œå› æ­¤éœ€è¦ç­‰å¾…äº‹ä»¶çš„å®Œæˆã€‚æœ€åè¿”å›æ‹¿å–å®Œæˆçš„é¦–ä¸ªå…ƒç´ ä¸­çš„vt_idx
    # ğŸ“Œåˆ†æï¼šinputå’Œgetæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œinputå°†is_runningç½®ä¸ºtrueï¼Œget(è°ƒç”¨wait)å°†is_runningç½®ä¸ºfalseã€‚ä¸è°ƒç”¨getï¼Œis_runningå°±ä¸å¯èƒ½ä¸ºfalse
    #         è‹¥å‘ç°is_runningä¸ä¸ºtrueï¼Œå°±è¯´æ˜ä¹‹å‰æ ¹æœ¬å°±æ²¡æ‰§è¡Œè¿‡layerçš„é¢„å–
    
    # 1.å‡†å¤‡å·¥ä½œ1ï¼šè°ƒç”¨syncpin_handlerå®ä¾‹çš„çº¿ç¨‹å°†vtä¸­çš„è¿™äº›åœ¨cpuå…±äº«å†…å­˜ä¸­çš„layerå¤åˆ¶åˆ°pinned memoryä¸Šï¼›
    # 2.å‡†å¤‡å·¥ä½œ2ï¼šåœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
    #   åŒæ—¶ä¹Ÿæ˜¯å½“å‰PrefetchLocalModelGPUå®ä¾‹çš„çº¿ç¨‹çš„è§¦å‘å·¥ä½œï¼Œå°†ä¸œè¥¿æ”¾è¿›put_queueï¼Œè¿™æ„å‘³ç€çº¿ç¨‹å¼€å§‹æ‰§è¡Œ3
    # 3.åœ¨ swapin_stream ä¸­å°†Wå’ŒBä»cpu memoryï¼ˆé»˜è®¤åœ¨å›ºå®šå†…å­˜ä¸Šï¼‰ä¸Šæ‹·è´åˆ°gpuçš„modelä¸Šï¼Œè‹¥vtçš„ç±»å‹ä¸ºBWDï¼Œè¿˜éœ€è¦
    #   æ˜¾ç¤ºçš„è®¾ç½®å‚æ•° param çš„ requires_grad å±æ€§ä¸º True
    # 4.è°ƒç”¨_waitå°†is_running ç½®ä¸ºfalseï¼Œè¿”å›get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)
    # 5.self.compute_stream.wait_event(ev_swapin)
    # 6.è‹¥suc_vtå‚æ•°ä¸ä¸ºç©ºï¼Œæ„å‘³ç€è¯¥å‡½æ•°ä¼šä¸ºæå‰æ‰§è¡Œä¸€éƒ¨åˆ†åç»§ä»»åŠ¡ï¼Œå³è°ƒç”¨self.syncpin_handler.iput(suc_vt)ï¼Œä¸1ç›¸åŒ
    def get(self, vt, suc_vt=None):
        ''' Call by main thread. Blocking wait until current prefetch finish. Then launch next sync pin. Return current vt.idx. '''
        assert isinstance(vt, vTask)
        # wait current one (if no current one, get this one)
        # è‹¥å½“å‰æ²¡æœ‰æ­£åœ¨GPUä¸Šåˆ†é…vtçš„æ‰€æœ‰å±‚
        if not self.is_running:
            # self.put_queue.add(vt)
            # è¿™æ„å‘³ç€ syncpin_handler è¿™ä¸ªçº¿ç¨‹å¼€å§‹æ‰§è¡Œvtçš„æ¨¡å‹çš„ä»å…±äº«å†…å­˜åˆ°å›ºå®šå†…å­˜çš„å¤åˆ¶
            self.syncpin_handler.iput(vt)
            # ä¸ºå‘GPUå¤åˆ¶Wã€Båšå‡†å¤‡å·¥ä½œï¼š
            # åœ¨é»˜è®¤è®¡ç®—æµä¸Šï¼ŒæŒ‰ç…§Pinned memoryä¸­layerçš„W,Bçš„å¤§å°ã€ç±»å‹ï¼Œä¸ºç»™å®švtåœ¨GPU(çš„å¯¹åº”å±‚)ä¸Šçš„æ‰€æœ‰layeråˆå§‹åŒ–Wå’ŒBï¼ˆå€¼æ˜¯éšæœºçš„ï¼‰
            # å°†(vt,ev_computeï¼ˆå°±æ˜¯è¯¥åˆå§‹åŒ–äº‹ä»¶ï¼‰)åŠ å…¥åˆ° put_queueä¸­ï¼Œè¿™è¡¨ç¤ºè¯¥å‡½æ•°æ‰€åœ¨å®ä¾‹(PrefetchLocalModelGPU)çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œswap in
            # 
            # 1.æ–­è¨€å½“å‰æ²¡æœ‰å…¶ä»–çš„ iput æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª iput æ“ä½œå¯ä»¥è¿›è¡Œ
            # 2.åœ¨é»˜è®¤æµä¸Šè®°å½•ä¸€ä¸ªäº‹ä»¶ï¼ˆåˆ†é…ä¸€ä¸ªæ–°çš„eventï¼‰ï¼Œå³ev_computeï¼Œç”¨äºåç»­åœ¨ swapin_stream è¿™ä¸ªæµä¸Šç­‰å¾…
            # 3.è‹¥ç»™å®švtä¸Šçš„æ‰€æœ‰layerçš„Wå’ŒBçš„åª’ä»‹ä¸ºSHMï¼Œåˆ™æŒ‰ç…§cpuä¸Šå‚æ•°å’Œbufferçš„å¤§å°å’Œç±»å‹ï¼Œä¸ºgpuä¸Šçš„dataåˆ†é…ä¸€ä¸ªå¤§å°å’Œç±»å‹ç›¸åŒçš„ç©ºtensor
            # 4.å°† (vt, ev_compute) æ·»åŠ åˆ° put_queue ä¸­
            self.iput(vt)
        # 1.åœ¨inputæ“ä½œæ­£åœ¨æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå°† is_running ç½®ä¸ºfalseï¼Œè¡¨ç¤ºæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„prefetchæ“ä½œ
        # 2.è¿”å›çº¿ç¨‹å®‰å…¨é˜Ÿåˆ— get_queue çš„é¦–ä¸ª (vt.idxï¼Œev_swapin)ï¼Œå³æ­£åœ¨æˆ–å·²ç»æ‰§è¡Œç©çš„swap_inäº‹ä»¶ï¼Œ(vt.idx,ev_swapin)
        #   è‹¥ _thread_func, å³ swap_in æ²¡åœ¨swapin_streamä¸Šåˆ†é…å®Œï¼Œä¼šé˜»å¡åœ¨ remove() ä¸Š
        cur_vt_idx, ev_swapin = self._wait()
        # ç­‰å¾…è¯¥vtä¸Šæ‰€æœ‰çš„å±‚åœ¨GPUä¸Šå®Œæˆåˆ†é…ç©ºtensor
        self.compute_stream.wait_event(ev_swapin) # Makes all future work submitted to compute stream wait for this swapin event
        # syncpin next one if exsits
        # è‹¥ç»™å®šäº†åç»§ä»»åŠ¡ï¼Œ
        if suc_vt is not None:
            self.syncpin_handler.iput(suc_vt)
        # è¿”å› cur_vt_idx
        return cur_vt_idx
