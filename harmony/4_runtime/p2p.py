# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import os
import threading
import gc
from collections import OrderedDict as ODict

import torch
import torch.distributed as dist
from torch.autograd import Variable

from torch.cuda.nvtx import range_push as nvtx_range_push 
from torch.cuda.nvtx import range_pop as nvtx_range_pop 

from prof_data_struct import ConstMeta, TensorMeta


class P2PX(object):
    # ä½¿ç”¨NCCLå¤„ç† X/dx çš„P2På‘é€å’Œæ¥æ”¶
    """ Handle P2P send-recv for X/dX of vPP with NCCL

        Feature: 
            0) Stateless recv (GPU memory are free'ed each microbatch) and Stateful prerecv (with double buffering)
            1) NCCL broadcast emulated send-recv
            2) Everything in the "Main" thread 
            3) Send-recv rank pairs use their own groups/communicators/cudaStreams
                --> Deadlock free, Full Duplex, Unblocking send and recv
            4) Support concurrent isend/recv/irecv for multiple tensors 
                *Plan-A: use multiple isend unblockingly (catch: PyTorch send-recv only works for a single tensor --> blocking)
                Plan-B: move multiple isend-recv to background thread, just like Gloo simpleCommHandler (bad: P2P(NCCL) is not thread-safe! )
                Plan-C: concatenate multiple tensors into one big tensor, then isend (catch: extra memory/time for concatenate/split)
            5) Support microbatch size conversion by using 'UBatchSizeConverterP2P'
                last-FWD sending to 1st-BWD needs converting microbatch sizes on GPUs. (catch: extra memory/time for concatenate/split)
            6) Built-in cuda event:
                <cudaEventRecord & cudaStreamWaitEvent> -> 
                launch nccl kernel (broadcast) -> 
                <cudaEventRecord, cudaEventCreateWithFlags, cudaEventRecord, cudaStreamWaitEvent> (ireq.wait)

        Assumption:
            0) distributed environment has already been initialized with gloo
            1) during send-recv, X/dX has no grad (after send-recv, can set them to requires_grad for BWD)
    """
    # 1.è®¿é—®æ¯ä¸€ä¸ªrankï¼Œåœ¨æ¯ä¸ªrankå’Œå…¶ä¸‹ä¸€ä¸ªranké—´å»ºç«‹ä¸€ä¸ªNCCLé€šä¿¡ç»„ã€‚è‹¥å½“å‰rankåŒ…å«åœ¨æ­£åœ¨å»ºç«‹çš„é€šä¿¡ç»„ä¸­ï¼Œ
    #   å°±ä¸ºå­—å…¸ self.groups æ·»åŠ ä¸€ä¸ªå€¼ï¼š{ "r1->r2": dist.group_obj }
    # 2.
    #   2.1.åˆ›å»ºä¸€ä¸ªåŒ…å«å•ä¸ªå…ƒç´ çš„å¼ é‡ï¼Œç”¨äºåˆå§‹åŒ– NCCL é€šä¿¡å™¨
    #   2.2.å°†å½“å‰rankæ‰€åœ¨çš„é€šä¿¡ç»„å–å‡ºï¼Œè¿›è¡Œä¸€æ¬¡r1->r2çš„ç‚¹å¯¹ç‚¹é€šä¿¡
    def __init__(self, rank, world_size, reverse_bwd=True, verbose=False, nvprof=False):
        assert dist.get_backend() == "gloo"
        self.rank = rank
        self.world_size = world_size
        self.verbose = verbose
        self.nvprof = nvprof
        assert self.rank == torch.cuda.current_device() # torch.cuda.set_device(rank)
        # build two-process group for NCCL broadcast (r1, r2)  
        self.groups = ODict() # { "r1->r2": dist.group_obj }
        # in-order round-robin
        # è®¿é—®æ¯ä¸€ä¸ªrankï¼Œåœ¨æ¯ä¸ªrankå’Œå…¶ä¸‹ä¸€ä¸ªranké—´å»ºç«‹ä¸€ä¸ªNCCLé€šä¿¡ç»„ã€‚è‹¥å½“å‰rankåŒ…å«åœ¨æ­£åœ¨å»ºç«‹çš„é€šä¿¡ç»„ä¸­ï¼Œ
        # å°±ä¸ºå­—å…¸ self.groups æ·»åŠ ä¸€ä¸ªå€¼ï¼š{ "r1->r2": dist.group_obj }
        for r1 in range(self.world_size):
            # r2å³r1åé¢é‚£ä¸ªrank
            r2 = (r1+1) % self.world_size
            pgroup = dist.new_group(ranks=[r1, r2], backend='nccl')
            # This function requires that 1) all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group; 2) groups should be created in the same order in all processes.
            # This new_group only creates empty NCCL groups (communicator and cudaStream are not initialized yet)
            if self.rank in [r1,r2]:
                self.groups["{}->{}".format(r1,r2)] = pgroup
        # reverse round-robin
        # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œä¸ç”¨ç®¡
        if reverse_bwd:
            for r1 in range(self.world_size):
                r2 = (r1-1) % self.world_size
                pgroup = dist.new_group(ranks=[r1, r2], backend='nccl')
                if self.rank in [r1,r2]:
                    self.groups["{}->{}".format(r1,r2)] = pgroup

        ################################################
        ############## æ‰‹åŠ¨è®¾ç½®PPé€šä¿¡ç»„ #################
        # r1 = 0
        # r2 = 3
        # pgroup = dist.new_group(ranks=[r1, r2], backend='nccl')
        # if self.rank in [r1,r2]:
        #     self.groups["{}->{}".format(r1,r2)] = pgroup
        ################################################

        # print("[P2P] rank={}, world_size={}, self.groups = {}".format(self.rank, self.world_size, self.groups))
        # initialize NCCL communicator and its cudaStream in mainthread
        # åˆ›å»ºä¸€ä¸ªåŒ…å«å•ä¸ªå…ƒç´ çš„å¼ é‡ï¼Œç”¨äºåˆå§‹åŒ– NCCL é€šä¿¡å™¨
        tensor = torch.tensor(1.0, dtype=torch.float32, device="cuda:%d"%(self.rank))
        # å°†å½“å‰rankæ‰€åœ¨çš„é€šä¿¡ç»„å–å‡ºï¼Œè¿›è¡Œä¸€æ¬¡r1->r2çš„ç‚¹å¯¹ç‚¹é€šä¿¡
        for key, group in self.groups.items():
            dist.broadcast(tensor, group=group, src=int(key.split("->")[0])) # init communicator should be in-order
            # print("[P2P] rank={} init'ed NCCL communicator[{}] and its cudaStream".format(self.rank, key))
        # clean up
        del tensor; gc.collect(); torch.cuda.empty_cache()
        # for pre-recv
        self.is_irecving = False
        self.ubatch_idx = int(0)
        self.double_bufs = [None, None]
        # for bytes counter
        if self.verbose: 
            self.send_byte_cnt = 0
            self.recv_byte_cnt = 0

    # éé˜»å¡çš„å°† tensor å‘é€åˆ°ç›®æ ‡rankçš„GPUä¸Šï¼Œè¿”å›ä¸€ä¸ªå¼‚æ­¥workå¥æŸ„
    @torch.no_grad()
    def _isend_tensor(self, tensor, dst):
        """ Non-Blocking send a tensor via NCCL broadcast """
        # print("[P2P]\trank{}: _isend_tensor({},{}) to dst:{}".format(self.rank, tensor.shape, tensor.dtype, dst))
        assert tensor.is_cuda
        group_key = "{}->{}".format(self.rank,dst)
        ireq = dist.broadcast(tensor, src=self.rank, group=self.groups[group_key], async_op=True)
        # print("[P2P]\trank{}: _isend_tensor'ed".format(self.rank))
        # tensor.nelement():è¿”å›å¼ é‡ä¸­å…ƒç´ çš„æ€»æ•°
        # tensor.element_size():è¿”å›å¼ é‡ä¸­æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚æ•°
        if self.verbose: self.send_byte_cnt += tensor.nelement()*tensor.element_size()
        return ireq
    
    # 1.è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensorï¼Œç”¨äºä¿å­˜æ¥æ”¶åˆ°çš„tensor
    # 2.å–å‡ºä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ï¼Œæ¥æ”¶ä»æºèŠ‚ç‚¹srcå‘é€åˆ°å½“å‰èŠ‚ç‚¹çš„tensorï¼Œå¹¶ä¸”ä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
    # è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
    @torch.no_grad()
    def _irecv_tensor(self, tensor=None, shape=None, dtype=torch.float32, src=-1):
        """ Non-Blocking recv a tensor via NCCL broadcast.
            If tensor is None, then its shape (e.g. () or (1,) or (2,2)) must be given to create a tensor, to receive, and to return this GPU tensor. 
            Else, return filled GPU tensor.

            case-1: _irecv_tensor(shape=(1,2), src=123) # create new
            case-2: _irecv_tensor(tensor=cuda_tensor, src=123) # reuse existing
        """
        assert (tensor is None and shape is not None) or (tensor is not None and shape is None)
        # è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensor
        tensor = torch.empty(shape, dtype=dtype, device="cuda:%d"%self.rank) if tensor is None else tensor
        assert tensor.is_cuda
        # print("[P2P]\trank{}: _irecv_tensor({},{}) from src:{}".format(self.rank, tensor.shape, tensor.dtype, src))
        # å–å‡ºç‚¹å¯¹ç‚¹çš„å¹¿æ’­é€šä¿¡ç»„
        group_key = "{}->{}".format(src, self.rank)
        # æ¥æ”¶ä»æºèŠ‚ç‚¹srcå‘é€åˆ°å½“å‰èŠ‚ç‚¹çš„tensorï¼Œå¹¶ä¸”ä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
        # è‹¥å½“å‰rankæ˜¯æ¥æ”¶rankï¼Œè¿™ä¸ªtensorå°±æ˜¯ä¿å­˜æ¥æ”¶æ•°æ®çš„tensor
        ireq = dist.broadcast(tensor, src=src, group=self.groups[group_key], async_op=True)
        # ireq.wait() # blocking
        # print("[P2P]\trank{}: _irecv_tensor'ed".format(self.rank))
        if self.verbose: self.recv_byte_cnt += tensor.nelement()*tensor.element_size()
        return tensor, ireq
    
    @torch.no_grad()
    def _isend_const(self, const, dst):
        """ Non-Blocking send a const int/float via NCCL """
        # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
        assert isinstance(const, (int,float))
        tensor = torch.tensor(const, device="cuda:%d"%self.rank)
        ireq = self._isend_tensor(tensor, dst) # """ nccl unblocking isend works better than gloo """
        del tensor # It works
        # print("[P2PHandler] rank={}: dist.broadcast'ed".format(self.rank))
        return ireq

    @torch.no_grad()
    def _irecv_const(self, tensor=None, const=None, src=-1):
        """ Non-Blocking send a const scalar via NCCL.
            If tensor is None, then const must be given (int/float) to create a tensor, to receive, and to return this GPU tensor.
            Else, return filled GPU tensor.
            
            case-1: _irecv_const(const=123, src=123) # create new
            case-2: _irecv_const(tensor=cuda_tensor, src=123) # reuse existing
        """
        assert (tensor is None and isinstance(const, (int,float))) or (tensor is not None and const is None)
        # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
        tensor = torch.tensor(const, device="cuda:%d"%self.rank) if tensor is None else tensor
        tensor, ireq = self._irecv_tensor(tensor=tensor, src=src)
        # ireq.wait() # blocking
        # print("[rank{}]\tP2PX._irecv_const: irecv'ed {}".format(self.rank, tensor))
        return tensor, ireq # Need tensor.item() to convert a 0-dim tensor to a python number, once received
        
    # @torch.no_grad() # Deprecated
    # def _isend_const(self, const, dst, tag=7777777):
    #     """ Non-Blocking send a const int/float via Gloo """
    #     # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
    #     assert isinstance(const, (int,float))
    #     ireq = dist.isend(torch.tensor(const), dst=dst, tag=tag)
    #     ireq.wait() 
    #     """ must blocking for const, otherwise the CPU const can be deleted too soon, causing irecv wait forever """
    #     """ But if really blocks, it causes P2P deadlock. """
    #     # print("[P2PHandler] rank={}: dist.broadcast'ed".format(self.rank))
    #     return ireq

    # @torch.no_grad() # Deprecated
    # def _irecv_const(self, const, src, tag=7777777):
    #     """ Non-Blocking send a const int/float via Gloo """
    #     # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
    #     assert isinstance(const, (int,float))
    #     tensor = torch.tensor(const, device="cpu", pin_memory=True)
    #     ireq = dist.irecv(tensor, src=src, tag=tag)
    #     # ireq.wait() # blocking
    #     # print("[rank{}]\tP2PX._irecv_const: irecv'ed {}".format(self.rank, tensor))
    #     return tensor, ireq # Need tensor.item() to convert a 0-dim tensor to a python number
    
    # éé˜»å¡çš„å°† tensor å‘é€åˆ°ç›®æ ‡rankçš„GPUä¸Šï¼Œè¿”å›ä¸€ä¸ªå¼‚æ­¥workå¥æŸ„
    def isend(self, named_tensors, dst):
        ''' Call by main thread. Nonblocking send. '''    
        # print("[P2P]\trank{}: isend entered".format(self.rank))
        for name,tensor in named_tensors.items(): # { name: tensor/const, name: [tensors] }
            if isinstance(tensor, (torch.Tensor,Variable)):
                # ç¡®ä¿tensoråœ¨GPUä¸Šï¼Œä¸”ä¸éœ€è¦æ¢¯åº¦
                assert tensor.is_cuda and not tensor.requires_grad
                # éé˜»å¡çš„å°† tensor å‘é€åˆ°ç›®æ ‡rankçš„GPUä¸Šï¼Œè¿”å›ä¸€ä¸ªå¼‚æ­¥workå¥æŸ„
                self._isend_tensor(tensor, dst)
            elif isinstance(tensor, (float,int)):
                self._isend_const(tensor, dst)
            elif isinstance(tensor, list): # output tuple of bert pretrainhead 
                for t in tensor:
                    assert t.is_cuda and not t.requires_grad
                    self._isend_tensor(t, dst)
            else:
                raise ValueError("unknown tensor={}".format(tensor))
            # print("[P2P]\trank{}: isend'ed {}:{} to dst:{}".format(self.rank, name, type(tensor), dst))
    
    # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œ
    # 2 ç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œå³å·²ç»æ¥æ”¶åˆ°srcå‘é€è¿‡æ¥çš„tensor
    # 3.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰
    def recv(self, named_metas, src=-1):
        ''' Call by main thread. Blocking recv. 
            Assumption: 
                0) Stateless: always create allocate new tensor, and let it delete by runtime (no double buffering)
                1) Blocking compute stream by cuda events
            Argument: named_metas = { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      src = src rank
            Return: recved named_tensors. '''    
        # print("[P2P]\trank{}: recv entered".format(self.rank))
        named_tensors = ODict()
        named_ireq = ODict()

        # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œ
        for name, meta in named_metas.items():
            if isinstance(meta, TensorMeta):
                # 1.è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensorï¼Œç”¨äºä¿å­˜æ¥æ”¶åˆ°çš„tensor
                # 2.å–å‡ºä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ï¼Œæ¥æ”¶ä»srcå‘é€è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
                # è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
                named_tensors[name], named_ireq[name] = self._irecv_tensor(shape=meta.shape, dtype=meta.dtype, src=src)
            elif isinstance(meta, ConstMeta):
                named_tensors[name], named_ireq[name] = self._irecv_const(const=meta.const, src=src)
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                tmp_tensor, tmp_ireq = [], []
                for m in meta:
                    tensor, ireq = self._irecv_tensor(shape=m.shape, dtype=m.dtype, src=src)
                    tmp_tensor.append(tensor); tmp_ireq.append(ireq)
                named_tensors[name] = tmp_tensor
                named_ireq[name] = tmp_ireq
            else:
                raise ValueError("unknown meta={}".format(meta))
            # print("[P2P]\trank{}: recv's irecv'ed {}:{}".format(self.rank, name, meta))

        # wait all tensors recved (by built-in cuda event)
        # 2.ç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œå³å·²ç»æ¥æ”¶åˆ°srcå‘é€è¿‡æ¥çš„tensor
        for name, meta in named_metas.items():
            if isinstance(meta, TensorMeta):
                named_ireq[name].wait()
            elif isinstance(meta, ConstMeta):
                named_ireq[name].wait()
                named_tensors[name] = named_tensors[name].item() # convert a 0-dim cpu/cuda tensor to a python number # let python do the gc on cuda tensor
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                # for ireq, tensor in zip(named_ireq[name],named_tensors[name]):
                #     ireq.wait()
                [ ireq.wait() for ireq in named_ireq[name] ]  
            else:
                raise ValueError("unknown meta={}".format(meta))
            # print("[P2P]\trank{}: recv's ireq.waited {}:{} from src:{}".format(self.rank, name, meta, src))
        # print("[P2P]\trank{}: recv's all ireqs waited".format(self.rank))
        # clean up

        # 3.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰
        return named_tensors
    
    # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œã€‚è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
    # 2.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰å’Œå¼‚æ­¥å·¥ä½œå¥æŸ„
    def _irecv(self, named_metas, src, buffer=None): 
        ''' Non-Blocking recv. 
            Assumption: only one irecv can be running.
            Argument: named_metas = { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      src = src rank
                      buffer = None or (named_tensors, named_ireq)
            Return: created or input (named_tensors, named_ireq)
        '''
        assert not self.is_irecving, "the irecv is still running"
        self.is_irecving = True
        # print("[P2P]\trank{}: _irecv'ing".format(self.rank))

        # è‹¥æ²¡æœ‰ç»™å®šbufferï¼Œåœ¨_irecv_tensorå‡½æ•°çš„æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œåœ¨GPUä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„tensorç”¨äºæ¥æ”¶å‘è¿‡æ¥çš„tensor
        if buffer is None: # allocate new tensors
            if self.nvprof: nvtx_range_push("P2PIn Alloc & iBcast") 
            named_tensors = ODict()
            named_ireq = ODict()
            for name, meta in named_metas.items():
                if isinstance(meta, TensorMeta):
                    # 1.è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensorï¼Œç”¨äºä¿å­˜æ¥æ”¶åˆ°çš„tensor
                    # 2.å–å‡ºä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ï¼Œæ¥æ”¶ä»srcå‘é€è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
                    # è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
                    named_tensors[name], named_ireq[name] = self._irecv_tensor(shape=meta.shape, dtype=meta.dtype, src=src)
                elif isinstance(meta, ConstMeta):
                    named_tensors[name], named_ireq[name] = self._irecv_const(const=meta.const, src=src)
                elif isinstance(meta, list): # output tuple of bert pretrainhead 
                    tmp_tensor, tmp_ireq = [], []
                    for m in meta:
                        tensor, ireq = self._irecv_tensor(shape=m.shape, dtype=m.dtype, src=src)
                        tmp_tensor.append(tensor); tmp_ireq.append(ireq)
                    named_tensors[name] = tmp_tensor
                    named_ireq[name] = tmp_ireq
                else:
                    raise ValueError("unknown meta={}".format(meta))
            if self.nvprof: nvtx_range_pop() 
            # print("[P2P]\trank{}: _irecv allocated new tensors and requested all".format(self.rank))

        # è‹¥ç»™å®šäº†bufferï¼Œåˆ™ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„tensor(å·²ç»æ¥æ”¶è¿‡æ•°æ®äº†)æ¥æ”¶å‘è¿‡æ¥çš„tensor
        else: # reuse buffered tensors
            named_tensors, named_ireq = buffer
            for name, meta in named_metas.items():
                if isinstance(meta, TensorMeta):
                    named_tensors[name], named_ireq[name] = self._irecv_tensor(tensor=named_tensors[name], src=src)
                elif isinstance(meta, ConstMeta):
                    named_tensors[name], named_ireq[name] = self._irecv_const(tensor=named_tensors[name], src=src)
                elif isinstance(meta, list): # output tuple of bert pretrainhead 
                    assert isinstance(named_tensors[name], list) and isinstance(named_ireq[name], list)
                    for i in range(len(meta)):
                        named_tensors[name][i], named_ireq[name][i] = self._irecv_tensor(tensor=named_tensors[name][i], src=src)
                else:
                    raise ValueError("unknown meta={}".format(meta))
            # print("[P2P]\trank{}: _irecv reused buffered tensors and requested all".format(self.rank))
        return named_tensors, named_ireq
        
    # è°ƒç”¨bufferä¸­ä¿å­˜çš„å¼‚æ­¥å¥æŸ„çš„waitå‡½æ•°ï¼Œç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œä»¥named_tensorçš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensor
    def _wait_irecv(self, named_metas, buffer=None): 
        ''' Wait for the running irecv. 
            Assumption: only one irecv can be running.
            Arguments: the same as _irecv, except buffer is not None
            Return: recved named_tensors.
        '''
        assert self.is_irecving, "no running irecv"
        self.is_irecving = False
        # wait all tensors recved
        assert buffer is not None
        named_tensors, named_ireq = buffer
        recved_named_tensors = ODict() # ref to buffer
        for name, meta in named_metas.items():
            if isinstance(meta, TensorMeta):
                named_ireq[name].wait()
                recved_named_tensors[name] = named_tensors[name] # ref
            elif isinstance(meta, ConstMeta):
                named_ireq[name].wait()
                # named_tensors[name] = named_tensors[name].item() # convert a 0-dim cpu/cuda tensor to a python number
                recved_named_tensors[name] = named_tensors[name].item()
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                tmp_tensor = []
                for tensor, ireq in zip(named_tensors[name], named_ireq[name]):
                    ireq.wait()
                    tmp_tensor.append(tensor)
                recved_named_tensors[name] = tmp_tensor # ref
            else:
                raise ValueError("unknown meta={}".format(meta))
        return recved_named_tensors 

    # is_endï¼šè‹¥å½“å‰æ˜¯vtçš„æœ€åä¸€ä¸ªmicro batchï¼Œå°±ä¸ºTrue
    #
    # 1.è‹¥æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œåˆ™ä»src rankä¸Šæ¥æ”¶å‘è¿‡æ¥çš„tensor
    #   --å¦åˆ™ï¼Œè‹¥æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œç›´æ¥æ‰§è¡Œ2
    # 2.è°ƒç”¨å¼‚æ­¥å¥æŸ„çš„waitå‡½æ•°ï¼Œç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œä»¥named_tensorçš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensor
    # 3.è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªMicro batchï¼Œå¼‚æ­¥çš„æ¥æ”¶ä¸‹ä¸€ä¸ªmicrobatchè¿è¡Œæ—¶éœ€è¦çš„X
    def prerecv(self, named_metas, src, is_end=False): 
        ''' Call by main thread. Blocking recv current one and unblocking pre-recv next one. 
            Assumption: 
                    1) use double buffering for parallel compute and irecv
                    2) double buffering doesn't change in shape TODO: fix
                    3) use cudaEvent to sync with compute stream, and event calls are still on CPU async w.r.t GPU streams
                    4) no prerecv for successor group
            Argument: named_metas = { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      src = src rank
                      is_end = whether ends prerecv after current recv
            Return: recved current named_tensors. '''    
        #
        # 0
        cur_buf_idx = self.ubatch_idx % 2 # for compute
        # 1
        next_buf_idx = (self.ubatch_idx+1) % 2 # for pre irecv

        # wait current one (if no current one, _irecv & _wait_irecv one)
        # è‹¥æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œåˆ™ä»src rankä¸Šæ¥æ”¶å‘è¿‡æ¥çš„tensor
        # ğŸ“Œä»ç¬¬äºŒä¸ªmicrobatchå¼€å§‹ï¼Œè¿™é‡Œæ‰ä¼šç•¥è¿‡ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªMicorbatché¢„å–äº†ç¬¬äºŒä¸ªçš„X
        # å› æ­¤ifä¸ºfalseï¼Œç›´æ¥æ‰§è¡Œä¸‹é¢çš„waitç­‰å¾…æ¥æ”¶å®Œæ¯•ï¼ˆä¹Ÿæœ‰å¯èƒ½å·²ç»æ¥æ”¶å®Œäº†ï¼‰
        if not self.is_irecving:
            # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œã€‚è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
            # 2.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰å’Œå¼‚æ­¥å·¥ä½œå¥æŸ„
            self.double_bufs[cur_buf_idx] = self._irecv(named_metas, src, None)
        # è°ƒç”¨bufferä¸­ä¿å­˜çš„å¼‚æ­¥å¥æŸ„çš„waitå‡½æ•°ï¼Œç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œä»¥named_tensorçš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensor
        cur_named_tensors = self._wait_irecv(named_metas, self.double_bufs[cur_buf_idx])
        # irecv next one if exists
        # è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªMicro batchï¼Œ
        if not is_end:
            # â“æ˜¯ä¸æ˜¯ä»£ç å†™é”™äº†ï¼Œè‹¥ç¬¬äºŒä¸ªå‚æ•°çš„ä¸‹æ ‡æ˜¯next_buf_idxï¼Œå°±ä¸ç®—å¤ç”¨å·²æœ‰çš„tensoräº†
            # è‚¯å®šå†™é”™äº†ï¼Œself.double_bufs[next_buf_idx]ä¸­æ­¤æ—¶æ²¡æœ‰tesnorï¼Œä¸ºç©ºåœ¨_irecv_tensorä¸­å°±å¾—ç”Ÿæˆä¸€ä¸ªç©ºtensor
            # ä½†è°ƒç”¨ _irecv_tensor æ—¶æ ¹æœ¬æ²¡ç»™shape
            # æ­¤å¤–ï¼Œè¿™é‡Œåº”è¯¥æ¥æ”¶ä¸‹ä¸€ä¸ª named_metasï¼Œè¿™é‡Œä¼ è¿›å»çš„è¿˜æ˜¯å½“å‰çš„ named_metasï¼Œå¯¹åŒä¸€ä¸ªæ•°æ®é‡æ–°æ¥æ”¶äº†ä¸€é
            # ğŸ“Œè¿™é‡Œç¬¬2ä¸ªå‚æ•°ä¼ è¿›å»çš„å®é™…è¿˜æ˜¯Noneï¼Œå†…éƒ¨è¿˜æ˜¯ä¼šæ­£ç¡®çš„æ‰§è¡Œ
            # ğŸ“Œ24/7/9ï¼šæ²¡å†™é”™ï¼Œè¿™é‡Œä¸æ˜¯ä¸ºä¸‹ä¸€ä¸ªvtæ¥æ”¶è¾“å…¥Xï¼Œè€Œæ˜¯ä¸ºä¸‹ä¸€ä¸ªmicrobatchçš„æ‰§è¡Œæ¥æ”¶éœ€è¦çš„X
            self.double_bufs[next_buf_idx] = self._irecv(named_metas, src, self.double_bufs[next_buf_idx])
            self.ubatch_idx += 1
        else: # clean up
            self.ubatch_idx = 0
            del self.double_bufs 
            self.double_bufs = [None, None]            
        return cur_named_tensors # reference only; to be deleted by runtime

    # suc_infoï¼š
    # ä¸ºåç»§ä»»åŠ¡å‡†å¤‡è¾“å…¥ä¿¡æ¯ï¼Œåç»§ä¸ºFWDåˆ™å‡†å¤‡è¾“å…¥Xï¼Œåç»§ä¸ºBWDåˆ™å‡†å¤‡è¾“å…¥dY
    # ä¸¤ç§æƒ…å†µï¼š
    # 1.åç»§ä»»åŠ¡æ˜¯FWDä»»åŠ¡ï¼Œæˆ–ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ï¼ˆåŒ…å«è®¡ç®—losså±‚ï¼‰ï¼Œä¸ºå…¶å‡†å¤‡è¾“å…¥Xçš„å…ƒæ•°æ®ä»¥åŠï¼ˆæ¥æºï¼‰åª’ä»‹
    #   ä¸¤ç§æƒ…å†µï¼š
    #   1.1.åç»§ä»»åŠ¡æ¥æ”¶Xçš„åª’ä»‹ä¸ä¸ºP2Pï¼Œç›´æ¥è¿”å›None
    #   1.2.å¦åˆ™ï¼Œè¿”å› (suc_vtè¾“å…¥Xçš„metaï¼Œsuc_vtæ¥æ”¶Xçš„src_rank)
    # 2.åç»§ä»»åŠ¡æ˜¯BWDä»»åŠ¡ï¼Œåˆ™è¿”å›åç»§BWDä»»åŠ¡dYçš„å…ƒæ•°æ®ä»¥åŠï¼ˆæ¥æºï¼‰åª’ä»‹
    #   ä¸¤ç§æƒ…å†µï¼š
    #   2.1.è‹¥å½“å‰BWDçš„åç»§BWDä»»åŠ¡æ¥æ”¶dYçš„åª’ä»‹ä¸ä¸ºP2Pï¼Œè¿”å›None
    #   2.2.å¦åˆ™ï¼Œè¿”å›suc_vtçš„å‰é©±BWDä»»åŠ¡é¦–å±‚çš„æ¥æ”¶Xçš„å…ƒæ•°æ®ï¼Œ{ name:TensorMeta }ï¼Œæ¥ä»£è¡¨suc_vtæœ€åä¸€å±‚æ¥æ”¶dYçš„å…ƒæ•°æ®

    # æå‰ä½¿ç”¨P2Pæ–¹æ³•æ¥æ”¶åç»§ä»»åŠ¡çš„è¾“å…¥X/dY
    def prerecv_suc(self, suc_info): 
        ''' Prerecv successor group's 1st ubatch if exists. Call by main thread. 
            Assumption: same 1) 3) above
            Argument: suc_info = None or successor group's (named_metas, src_rank)
        '''  
        # è‹¥åç»§ä»»åŠ¡çš„ä¿¡æ¯ä¸ºNoneï¼Œç›´æ¥è¿”å›
        if suc_info is None:
            return
        # (suc_vtè¾“å…¥Xçš„metaï¼Œsuc_vté¦–å±‚çš„media)
        suc_named_metas, suc_src = suc_info
        # print("\trank{}: P2P.prerecv_suc({}, src{})".format(self.rank, suc_named_metas, suc_src))
        # must after is_end
        assert self.ubatch_idx == 0 and self.double_bufs == [None, None]
        # record previous compute event (built-in)
        # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œã€‚è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
        # 2.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰å’Œå¼‚æ­¥å·¥ä½œå¥æŸ„
        self.double_bufs[0] = self._irecv(suc_named_metas, suc_src, buffer=None)



class P2PX_2(object):
    # ä½¿ç”¨NCCLå¤„ç† X/dx çš„P2På‘é€å’Œæ¥æ”¶
    """ Handle P2P send-recv for X/dX of vPP with NCCL

        Feature: 
            0) Stateless recv (GPU memory are free'ed each microbatch) and Stateful prerecv (with double buffering)
            1) NCCL broadcast emulated send-recv
            2) Everything in the "Main" thread 
            3) Send-recv rank pairs use their own groups/communicators/cudaStreams
                --> Deadlock free, Full Duplex, Unblocking send and recv
            4) Support concurrent isend/recv/irecv for multiple tensors 
                *Plan-A: use multiple isend unblockingly (catch: PyTorch send-recv only works for a single tensor --> blocking)
                Plan-B: move multiple isend-recv to background thread, just like Gloo simpleCommHandler (bad: P2P(NCCL) is not thread-safe! )
                Plan-C: concatenate multiple tensors into one big tensor, then isend (catch: extra memory/time for concatenate/split)
            5) Support microbatch size conversion by using 'UBatchSizeConverterP2P'
                last-FWD sending to 1st-BWD needs converting microbatch sizes on GPUs. (catch: extra memory/time for concatenate/split)
            6) Built-in cuda event:
                <cudaEventRecord & cudaStreamWaitEvent> -> 
                launch nccl kernel (broadcast) -> 
                <cudaEventRecord, cudaEventCreateWithFlags, cudaEventRecord, cudaStreamWaitEvent> (ireq.wait)

        Assumption:
            0) distributed environment has already been initialized with gloo
            1) during send-recv, X/dX has no grad (after send-recv, can set them to requires_grad for BWD)
    """
    # 1.è®¿é—®æ¯ä¸€ä¸ªrankï¼Œåœ¨æ¯ä¸ªrankå’Œå…¶ä¸‹ä¸€ä¸ªranké—´å»ºç«‹ä¸€ä¸ªNCCLé€šä¿¡ç»„ã€‚è‹¥å½“å‰rankåŒ…å«åœ¨æ­£åœ¨å»ºç«‹çš„é€šä¿¡ç»„ä¸­ï¼Œ
    #   å°±ä¸ºå­—å…¸ self.groups æ·»åŠ ä¸€ä¸ªå€¼ï¼š{ "r1->r2": dist.group_obj }
    # 2.
    #   2.1.åˆ›å»ºä¸€ä¸ªåŒ…å«å•ä¸ªå…ƒç´ çš„å¼ é‡ï¼Œç”¨äºåˆå§‹åŒ– NCCL é€šä¿¡å™¨
    #   2.2.å°†å½“å‰rankæ‰€åœ¨çš„é€šä¿¡ç»„å–å‡ºï¼Œè¿›è¡Œä¸€æ¬¡r1->r2çš„ç‚¹å¯¹ç‚¹é€šä¿¡
    def __init__(self, rank, world_size, reverse_bwd=True, verbose=False, nvprof=False):
        assert dist.get_backend() == "gloo"
        self.rank = rank
        self.world_size = world_size
        self.verbose = verbose
        self.nvprof = nvprof
        assert self.rank == torch.cuda.current_device() # torch.cuda.set_device(rank)
        # build two-process group for NCCL broadcast (r1, r2)  
        self.groups = ODict() # { "r1->r2": dist.group_obj }
        # in-order round-robin
        # è®¿é—®æ¯ä¸€ä¸ªrankï¼Œåœ¨æ¯ä¸ªrankå’Œå…¶ä¸‹ä¸€ä¸ªranké—´å»ºç«‹ä¸€ä¸ªNCCLé€šä¿¡ç»„ã€‚è‹¥å½“å‰rankåŒ…å«åœ¨æ­£åœ¨å»ºç«‹çš„é€šä¿¡ç»„ä¸­ï¼Œ
        # å°±ä¸ºå­—å…¸ self.groups æ·»åŠ ä¸€ä¸ªå€¼ï¼š{ "r1->r2": dist.group_obj }
        for r1 in range(self.world_size):
            # r2å³r1åé¢é‚£ä¸ªrank
            r2 = (r1+1) % self.world_size
            pgroup = dist.new_group(ranks=[r1, r2], backend='nccl')
            # This function requires that 1) all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group; 2) groups should be created in the same order in all processes.
            # This new_group only creates empty NCCL groups (communicator and cudaStream are not initialized yet)
            if self.rank in [r1,r2]:
                self.groups["{}->{}".format(r1,r2)] = pgroup
        # reverse round-robin
        # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œä¸ç”¨ç®¡
        if reverse_bwd:
            for r1 in range(self.world_size):
                r2 = (r1-1) % self.world_size
                pgroup = dist.new_group(ranks=[r1, r2], backend='nccl')
                if self.rank in [r1,r2]:
                    self.groups["{}->{}".format(r1,r2)] = pgroup
        # print("[P2P] rank={}, world_size={}, self.groups = {}".format(self.rank, self.world_size, self.groups))
        # initialize NCCL communicator and its cudaStream in mainthread
        # åˆ›å»ºä¸€ä¸ªåŒ…å«å•ä¸ªå…ƒç´ çš„å¼ é‡ï¼Œç”¨äºåˆå§‹åŒ– NCCL é€šä¿¡å™¨
        tensor = torch.tensor(1.0, dtype=torch.float32, device="cuda:%d"%(self.rank))
        # å°†å½“å‰rankæ‰€åœ¨çš„é€šä¿¡ç»„å–å‡ºï¼Œè¿›è¡Œä¸€æ¬¡r1->r2çš„ç‚¹å¯¹ç‚¹é€šä¿¡
        for key, group in self.groups.items():
            dist.broadcast(tensor, group=group, src=int(key.split("->")[0])) # init communicator should be in-order
            # print("[P2P] rank={} init'ed NCCL communicator[{}] and its cudaStream".format(self.rank, key))
        # clean up
        del tensor; gc.collect(); torch.cuda.empty_cache()
        # for pre-recv
        self.is_irecving = False
        self.ubatch_idx = int(0)
        self.triple_bufs = [None, None, None]
        # for bytes counter
        if self.verbose: 
            self.send_byte_cnt = 0
            self.recv_byte_cnt = 0

    # éé˜»å¡çš„å°† tensor å‘é€åˆ°ç›®æ ‡rankçš„GPUä¸Šï¼Œè¿”å›ä¸€ä¸ªå¼‚æ­¥workå¥æŸ„
    @torch.no_grad()
    def _isend_tensor(self, tensor, dst):
        """ Non-Blocking send a tensor via NCCL broadcast """
        # print("[P2P]\trank{}: _isend_tensor({},{}) to dst:{}".format(self.rank, tensor.shape, tensor.dtype, dst))
        assert tensor.is_cuda
        group_key = "{}->{}".format(self.rank,dst)
        # print(f"rank:{self.rank}, å‘é€çš„tensorä¸º:{tensor}")
        ireq = dist.broadcast(tensor, src=self.rank, group=self.groups[group_key], async_op=True)
        # print("[P2P]\trank{}: _isend_tensor'ed".format(self.rank))
        # tensor.nelement():è¿”å›å¼ é‡ä¸­å…ƒç´ çš„æ€»æ•°
        # tensor.element_size():è¿”å›å¼ é‡ä¸­æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚æ•°
        if self.verbose: self.send_byte_cnt += tensor.nelement()*tensor.element_size()
        return ireq
    
    # 1.è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensorï¼Œç”¨äºä¿å­˜æ¥æ”¶åˆ°çš„tensor
    # 2.å–å‡ºä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ï¼Œæ¥æ”¶ä»æºèŠ‚ç‚¹srcå‘é€åˆ°å½“å‰èŠ‚ç‚¹çš„tensorï¼Œå¹¶ä¸”ä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
    # è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
    @torch.no_grad()
    def _irecv_tensor(self, tensor=None, shape=None, dtype=torch.float32, src=-1):
        """ Non-Blocking recv a tensor via NCCL broadcast.
            If tensor is None, then its shape (e.g. () or (1,) or (2,2)) must be given to create a tensor, to receive, and to return this GPU tensor. 
            Else, return filled GPU tensor.

            case-1: _irecv_tensor(shape=(1,2), src=123) # create new
            case-2: _irecv_tensor(tensor=cuda_tensor, src=123) # reuse existing
        """
        assert (tensor is None and shape is not None) or (tensor is not None and shape is None)
        # è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensor
        tensor = torch.empty(shape, dtype=dtype, device="cuda:%d"%self.rank) if tensor is None else tensor
        assert tensor.is_cuda
        # print("[P2P]\trank{}: _irecv_tensor({},{}) from src:{}".format(self.rank, tensor.shape, tensor.dtype, src))
        # å–å‡ºç‚¹å¯¹ç‚¹çš„å¹¿æ’­é€šä¿¡ç»„
        group_key = "{}->{}".format(src, self.rank)
        # æ¥æ”¶ä»æºèŠ‚ç‚¹srcå‘é€åˆ°å½“å‰èŠ‚ç‚¹çš„tensorï¼Œå¹¶ä¸”ä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
        # è‹¥å½“å‰rankæ˜¯æ¥æ”¶rankï¼Œè¿™ä¸ªtensorå°±æ˜¯ä¿å­˜æ¥æ”¶æ•°æ®çš„tensor
        ireq = dist.broadcast(tensor, src=src, group=self.groups[group_key], async_op=True)
        # åŠ ä¸Šè¿™ä¸ªæ²¡ç”¨ï¼Œä¸€æ ·æ”¶ä¸åˆ°ï¼Œä¸åƒæ˜¯è¢«è¦†ç›–çš„é—®é¢˜
        # ireq.wait() # blocking
        # print(f"rank:{self.rank}, æ”¶åˆ°çš„tensorä¸º:{tensor}")
        # print("[P2P]\trank{}: _irecv_tensor'ed".format(self.rank))
        if self.verbose: self.recv_byte_cnt += tensor.nelement()*tensor.element_size()
        return tensor, ireq
    
    @torch.no_grad()
    def _isend_const(self, const, dst):
        """ Non-Blocking send a const int/float via NCCL """
        # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
        assert isinstance(const, (int,float))
        tensor = torch.tensor(const, device="cuda:%d"%self.rank)
        ireq = self._isend_tensor(tensor, dst) # """ nccl unblocking isend works better than gloo """
        del tensor # It works
        # print("[P2PHandler] rank={}: dist.broadcast'ed".format(self.rank))
        return ireq

    @torch.no_grad()
    def _irecv_const(self, tensor=None, const=None, src=-1):
        """ Non-Blocking send a const scalar via NCCL.
            If tensor is None, then const must be given (int/float) to create a tensor, to receive, and to return this GPU tensor.
            Else, return filled GPU tensor.
            
            case-1: _irecv_const(const=123, src=123) # create new
            case-2: _irecv_const(tensor=cuda_tensor, src=123) # reuse existing
        """
        assert (tensor is None and isinstance(const, (int,float))) or (tensor is not None and const is None)
        # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
        tensor = torch.tensor(const, device="cuda:%d"%self.rank) if tensor is None else tensor
        tensor, ireq = self._irecv_tensor(tensor=tensor, src=src)
        # ireq.wait() # blocking
        # print("[rank{}]\tP2PX._irecv_const: irecv'ed {}".format(self.rank, tensor))
        return tensor, ireq # Need tensor.item() to convert a 0-dim tensor to a python number, once received
        
    # @torch.no_grad() # Deprecated
    # def _isend_const(self, const, dst, tag=7777777):
    #     """ Non-Blocking send a const int/float via Gloo """
    #     # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
    #     assert isinstance(const, (int,float))
    #     ireq = dist.isend(torch.tensor(const), dst=dst, tag=tag)
    #     ireq.wait() 
    #     """ must blocking for const, otherwise the CPU const can be deleted too soon, causing irecv wait forever """
    #     """ But if really blocks, it causes P2P deadlock. """
    #     # print("[P2PHandler] rank={}: dist.broadcast'ed".format(self.rank))
    #     return ireq

    # @torch.no_grad() # Deprecated
    # def _irecv_const(self, const, src, tag=7777777):
    #     """ Non-Blocking send a const int/float via Gloo """
    #     # print("[P2PHandler] rank={}: isend(tensor={} with shape={})".format(self.rank, tensor, tensor.shape))
    #     assert isinstance(const, (int,float))
    #     tensor = torch.tensor(const, device="cpu", pin_memory=True)
    #     ireq = dist.irecv(tensor, src=src, tag=tag)
    #     # ireq.wait() # blocking
    #     # print("[rank{}]\tP2PX._irecv_const: irecv'ed {}".format(self.rank, tensor))
    #     return tensor, ireq # Need tensor.item() to convert a 0-dim tensor to a python number
    
    # éé˜»å¡çš„å°† tensor å‘é€åˆ°ç›®æ ‡rankçš„GPUä¸Šï¼Œè¿”å›ä¸€ä¸ªå¼‚æ­¥workå¥æŸ„
    def isend(self, named_tensors, dst):
        ''' Call by main thread. Nonblocking send. '''    
        # print("[P2P]\trank{}: isend entered".format(self.rank))
        for name,tensor in named_tensors.items(): # { name: tensor/const, name: [tensors] }
            if isinstance(tensor, (torch.Tensor,Variable)):
                # ç¡®ä¿tensoråœ¨GPUä¸Šï¼Œä¸”ä¸éœ€è¦æ¢¯åº¦
                assert tensor.is_cuda and not tensor.requires_grad
                # éé˜»å¡çš„å°† tensor å‘é€åˆ°ç›®æ ‡rankçš„GPUä¸Šï¼Œè¿”å›ä¸€ä¸ªå¼‚æ­¥workå¥æŸ„
                self._isend_tensor(tensor, dst)
            elif isinstance(tensor, (float,int)):
                self._isend_const(tensor, dst)
            elif isinstance(tensor, list): # output tuple of bert pretrainhead 
                for t in tensor:
                    assert t.is_cuda and not t.requires_grad
                    self._isend_tensor(t, dst)
            else:
                raise ValueError("unknown tensor={}".format(tensor))
            # print("[P2P]\trank{}: isend'ed {}:{} to dst:{}".format(self.rank, name, type(tensor), dst))
    
    # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œ
    # 2 ç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œå³å·²ç»æ¥æ”¶åˆ°srcå‘é€è¿‡æ¥çš„tensor
    # 3.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰
    def recv(self, named_metas, src=-1):
        ''' Call by main thread. Blocking recv. 
            Assumption: 
                0) Stateless: always create allocate new tensor, and let it delete by runtime (no double buffering)
                1) Blocking compute stream by cuda events
            Argument: named_metas = { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      src = src rank
            Return: recved named_tensors. '''    
        # print("[P2P]\trank{}: recv entered".format(self.rank))
        named_tensors = ODict()
        named_ireq = ODict()

        # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œ
        for name, meta in named_metas.items():
            if isinstance(meta, TensorMeta):
                # 1.è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensorï¼Œç”¨äºä¿å­˜æ¥æ”¶åˆ°çš„tensor
                # 2.å–å‡ºä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ï¼Œæ¥æ”¶ä»srcå‘é€è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
                # è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
                named_tensors[name], named_ireq[name] = self._irecv_tensor(shape=meta.shape, dtype=meta.dtype, src=src)
            elif isinstance(meta, ConstMeta):
                named_tensors[name], named_ireq[name] = self._irecv_const(const=meta.const, src=src)
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                tmp_tensor, tmp_ireq = [], []
                for m in meta:
                    tensor, ireq = self._irecv_tensor(shape=m.shape, dtype=m.dtype, src=src)
                    tmp_tensor.append(tensor); tmp_ireq.append(ireq)
                named_tensors[name] = tmp_tensor
                named_ireq[name] = tmp_ireq
            else:
                raise ValueError("unknown meta={}".format(meta))
            # print("[P2P]\trank{}: recv's irecv'ed {}:{}".format(self.rank, name, meta))

        # wait all tensors recved (by built-in cuda event)
        # 2.ç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œå³å·²ç»æ¥æ”¶åˆ°srcå‘é€è¿‡æ¥çš„tensor
        for name, meta in named_metas.items():
            if isinstance(meta, TensorMeta):
                named_ireq[name].wait()
            elif isinstance(meta, ConstMeta):
                named_ireq[name].wait()
                named_tensors[name] = named_tensors[name].item() # convert a 0-dim cpu/cuda tensor to a python number # let python do the gc on cuda tensor
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                # for ireq, tensor in zip(named_ireq[name],named_tensors[name]):
                #     ireq.wait()
                [ ireq.wait() for ireq in named_ireq[name] ]  
            else:
                raise ValueError("unknown meta={}".format(meta))
            # print("[P2P]\trank{}: recv's ireq.waited {}:{} from src:{}".format(self.rank, name, meta, src))
        # print("[P2P]\trank{}: recv's all ireqs waited".format(self.rank))
        # clean up

        # 3.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰
        # for name, meta in named_metas.items():
        #     print(f"rank:{self.rank}, name:{name}, æ”¶åˆ°çš„tensorä¸º:{named_tensors[name]}")
        return named_tensors
    
    # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œã€‚è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
    # 2.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰å’Œå¼‚æ­¥å·¥ä½œå¥æŸ„
    def _irecv(self, named_metas, src, buffer=None): 
        ''' Non-Blocking recv. 
            Assumption: only one irecv can be running.
            Argument: named_metas = { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      src = src rank
                      buffer = None or (named_tensors, named_ireq)
            Return: created or input (named_tensors, named_ireq)
        '''
        assert not self.is_irecving, "the irecv is still running"
        self.is_irecving = True
        # print("[P2P]\trank{}: _irecv'ing".format(self.rank))

        # è‹¥æ²¡æœ‰ç»™å®šbufferï¼Œåœ¨_irecv_tensorå‡½æ•°çš„æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œåœ¨GPUä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„tensorç”¨äºæ¥æ”¶å‘è¿‡æ¥çš„tensor
        if buffer is None: # allocate new tensors
            if self.nvprof: nvtx_range_push("P2PIn Alloc & iBcast") 
            named_tensors = ODict()
            named_ireq = ODict()
            for name, meta in named_metas.items():
                if isinstance(meta, TensorMeta):
                    # 1.è‹¥æ²¡æœ‰ç»™å®štensorï¼Œåœ¨å½“å‰GPUä¸ŠæŒ‰ç…§ç»™å®šçš„shapeå’Œdtypeåˆ›å»ºä¸€ä¸ªç©ºtensorï¼Œç”¨äºä¿å­˜æ¥æ”¶åˆ°çš„tensor
                    # 2.å–å‡ºä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ï¼Œæ¥æ”¶ä»srcå‘é€è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œæ“ä½œ
                    # è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
                    named_tensors[name], named_ireq[name] = self._irecv_tensor(shape=meta.shape, dtype=meta.dtype, src=src)
                elif isinstance(meta, ConstMeta):
                    named_tensors[name], named_ireq[name] = self._irecv_const(const=meta.const, src=src)
                elif isinstance(meta, list): # output tuple of bert pretrainhead 
                    tmp_tensor, tmp_ireq = [], []
                    for m in meta:
                        tensor, ireq = self._irecv_tensor(shape=m.shape, dtype=m.dtype, src=src)
                        tmp_tensor.append(tensor); tmp_ireq.append(ireq)
                    named_tensors[name] = tmp_tensor
                    named_ireq[name] = tmp_ireq
                else:
                    raise ValueError("unknown meta={}".format(meta))
            if self.nvprof: nvtx_range_pop() 
            # print("[P2P]\trank{}: _irecv allocated new tensors and requested all".format(self.rank))

        # è‹¥ç»™å®šäº†bufferï¼Œåˆ™ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„tensor(å·²ç»æ¥æ”¶è¿‡æ•°æ®äº†)æ¥æ”¶å‘è¿‡æ¥çš„tensor
        else: # reuse buffered tensors
            named_tensors, named_ireq = buffer
            for name, meta in named_metas.items():
                if isinstance(meta, TensorMeta):
                    named_tensors[name], named_ireq[name] = self._irecv_tensor(tensor=named_tensors[name], src=src)
                elif isinstance(meta, ConstMeta):
                    named_tensors[name], named_ireq[name] = self._irecv_const(tensor=named_tensors[name], src=src)
                elif isinstance(meta, list): # output tuple of bert pretrainhead 
                    assert isinstance(named_tensors[name], list) and isinstance(named_ireq[name], list)
                    for i in range(len(meta)):
                        named_tensors[name][i], named_ireq[name][i] = self._irecv_tensor(tensor=named_tensors[name][i], src=src)
                else:
                    raise ValueError("unknown meta={}".format(meta))
            # print("[P2P]\trank{}: _irecv reused buffered tensors and requested all".format(self.rank))
        return named_tensors, named_ireq
        
    # è°ƒç”¨bufferä¸­ä¿å­˜çš„å¼‚æ­¥å¥æŸ„çš„waitå‡½æ•°ï¼Œç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œä»¥named_tensorçš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensor
    def _wait_irecv(self, named_metas, buffer=None): 
        ''' Wait for the running irecv. 
            Assumption: only one irecv can be running.
            Arguments: the same as _irecv, except buffer is not None
            Return: recved named_tensors.
        '''
        assert self.is_irecving, "no running irecv"
        self.is_irecving = False
        # wait all tensors recved
        assert buffer is not None
        named_tensors, named_ireq = buffer
        recved_named_tensors = ODict() # ref to buffer
        for name, meta in named_metas.items():
            if isinstance(meta, TensorMeta):
                named_ireq[name].wait()
                recved_named_tensors[name] = named_tensors[name] # ref
            elif isinstance(meta, ConstMeta):
                named_ireq[name].wait()
                # named_tensors[name] = named_tensors[name].item() # convert a 0-dim cpu/cuda tensor to a python number
                recved_named_tensors[name] = named_tensors[name].item()
            elif isinstance(meta, list): # output tuple of bert pretrainhead 
                tmp_tensor = []
                for tensor, ireq in zip(named_tensors[name], named_ireq[name]):
                    ireq.wait()
                    tmp_tensor.append(tensor)
                recved_named_tensors[name] = tmp_tensor # ref
            else:
                raise ValueError("unknown meta={}".format(meta))
        return recved_named_tensors 

    # is_endï¼šè‹¥å½“å‰æ˜¯vtçš„æœ€åä¸€ä¸ªmicro batchï¼Œå°±ä¸ºTrue
    #
    # 1.è‹¥æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œåˆ™ä»src rankä¸Šæ¥æ”¶å‘è¿‡æ¥çš„tensor
    #   --å¦åˆ™ï¼Œè‹¥æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œç›´æ¥æ‰§è¡Œ2
    # 2.è°ƒç”¨å¼‚æ­¥å¥æŸ„çš„waitå‡½æ•°ï¼Œç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œä»¥named_tensorçš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensor
    # 3.è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªMicro batchï¼Œå¼‚æ­¥çš„æ¥æ”¶ä¸‹ä¸€ä¸ªmicrobatchè¿è¡Œæ—¶éœ€è¦çš„X
    def prerecv(self, named_metas, src, is_end=False): 
        ''' Call by main thread. Blocking recv current one and unblocking pre-recv next one. 
            Assumption: 
                    1) use double buffering for parallel compute and irecv
                    2) double buffering doesn't change in shape TODO: fix
                    3) use cudaEvent to sync with compute stream, and event calls are still on CPU async w.r.t GPU streams
                    4) no prerecv for successor group
            Argument: named_metas = { name:TensorMeta, name:ConstMeta, name:[TensorMeta,TensorMeta] }
                      src = src rank
                      is_end = whether ends prerecv after current recv
            Return: recved current named_tensors. '''    
        #
        cur_buf_idx = self.ubatch_idx % 3  # for compute
        next_buf_idx = (self.ubatch_idx + 1) % 3  # for pre irecv
        prev_buf_idx = (self.ubatch_idx + 2) % 3  # buffer to free

        # wait current one (if no current one, _irecv & _wait_irecv one)
        # è‹¥æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„æ¥æ”¶æ“ä½œï¼Œåˆ™ä»src rankä¸Šæ¥æ”¶å‘è¿‡æ¥çš„tensor
        # ğŸ“Œä»ç¬¬äºŒä¸ªmicrobatchå¼€å§‹ï¼Œè¿™é‡Œæ‰ä¼šç•¥è¿‡ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªMicorbatché¢„å–äº†ç¬¬äºŒä¸ªçš„X
        # å› æ­¤ifä¸ºfalseï¼Œç›´æ¥æ‰§è¡Œä¸‹é¢çš„waitç­‰å¾…æ¥æ”¶å®Œæ¯•ï¼ˆä¹Ÿæœ‰å¯èƒ½å·²ç»æ¥æ”¶å®Œäº†ï¼‰
        if not self.is_irecving:
            # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œã€‚è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
            # 2.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰å’Œå¼‚æ­¥å·¥ä½œå¥æŸ„
            self.triple_bufs[cur_buf_idx] = self._irecv(named_metas, src, None)
        # è°ƒç”¨bufferä¸­ä¿å­˜çš„å¼‚æ­¥å¥æŸ„çš„waitå‡½æ•°ï¼Œç­‰å¾…P2Pé€šä¿¡å®Œæˆï¼Œä»¥named_tensorçš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensor
        cur_named_tensors = self._wait_irecv(named_metas, self.triple_bufs[cur_buf_idx])
        # irecv next one if exists
        # è‹¥å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªMicro batchï¼Œ
        if not is_end:
            # â“æ˜¯ä¸æ˜¯ä»£ç å†™é”™äº†ï¼Œè‹¥ç¬¬äºŒä¸ªå‚æ•°çš„ä¸‹æ ‡æ˜¯next_buf_idxï¼Œå°±ä¸ç®—å¤ç”¨å·²æœ‰çš„tensoräº†
            # è‚¯å®šå†™é”™äº†ï¼Œself.double_bufs[next_buf_idx]ä¸­æ­¤æ—¶æ²¡æœ‰tesnorï¼Œä¸ºç©ºåœ¨_irecv_tensorä¸­å°±å¾—ç”Ÿæˆä¸€ä¸ªç©ºtensor
            # ä½†è°ƒç”¨ _irecv_tensor æ—¶æ ¹æœ¬æ²¡ç»™shape
            # æ­¤å¤–ï¼Œè¿™é‡Œåº”è¯¥æ¥æ”¶ä¸‹ä¸€ä¸ª named_metasï¼Œè¿™é‡Œä¼ è¿›å»çš„è¿˜æ˜¯å½“å‰çš„ named_metasï¼Œå¯¹åŒä¸€ä¸ªæ•°æ®é‡æ–°æ¥æ”¶äº†ä¸€é
            # ğŸ“Œè¿™é‡Œç¬¬2ä¸ªå‚æ•°ä¼ è¿›å»çš„å®é™…è¿˜æ˜¯Noneï¼Œå†…éƒ¨è¿˜æ˜¯ä¼šæ­£ç¡®çš„æ‰§è¡Œ
            # ğŸ“Œ24/7/9ï¼šæ²¡å†™é”™ï¼Œè¿™é‡Œä¸æ˜¯ä¸ºä¸‹ä¸€ä¸ªvtæ¥æ”¶è¾“å…¥Xï¼Œè€Œæ˜¯ä¸ºä¸‹ä¸€ä¸ªmicrobatchçš„æ‰§è¡Œæ¥æ”¶éœ€è¦çš„X
            self.triple_bufs[next_buf_idx] = self._irecv(named_metas, src, self.triple_bufs[next_buf_idx])
            self.ubatch_idx += 1
        else: # clean up
            self.ubatch_idx = 0
            del self.triple_bufs 
            self.triple_bufs = [None, None, None]            
        return cur_named_tensors # reference only; to be deleted by runtime

    # suc_infoï¼š
    # ä¸ºåç»§ä»»åŠ¡å‡†å¤‡è¾“å…¥ä¿¡æ¯ï¼Œåç»§ä¸ºFWDåˆ™å‡†å¤‡è¾“å…¥Xï¼Œåç»§ä¸ºBWDåˆ™å‡†å¤‡è¾“å…¥dY
    # ä¸¤ç§æƒ…å†µï¼š
    # 1.åç»§ä»»åŠ¡æ˜¯FWDä»»åŠ¡ï¼Œæˆ–ç¬¬ä¸€ä¸ªBWDä»»åŠ¡ï¼ˆåŒ…å«è®¡ç®—losså±‚ï¼‰ï¼Œä¸ºå…¶å‡†å¤‡è¾“å…¥Xçš„å…ƒæ•°æ®ä»¥åŠï¼ˆæ¥æºï¼‰åª’ä»‹
    #   ä¸¤ç§æƒ…å†µï¼š
    #   1.1.åç»§ä»»åŠ¡æ¥æ”¶Xçš„åª’ä»‹ä¸ä¸ºP2Pï¼Œç›´æ¥è¿”å›None
    #   1.2.å¦åˆ™ï¼Œè¿”å› (suc_vtè¾“å…¥Xçš„metaï¼Œsuc_vtæ¥æ”¶Xçš„src_rank)
    # 2.åç»§ä»»åŠ¡æ˜¯BWDä»»åŠ¡ï¼Œåˆ™è¿”å›åç»§BWDä»»åŠ¡dYçš„å…ƒæ•°æ®ä»¥åŠï¼ˆæ¥æºï¼‰åª’ä»‹
    #   ä¸¤ç§æƒ…å†µï¼š
    #   2.1.è‹¥å½“å‰BWDçš„åç»§BWDä»»åŠ¡æ¥æ”¶dYçš„åª’ä»‹ä¸ä¸ºP2Pï¼Œè¿”å›None
    #   2.2.å¦åˆ™ï¼Œè¿”å›suc_vtçš„å‰é©±BWDä»»åŠ¡é¦–å±‚çš„æ¥æ”¶Xçš„å…ƒæ•°æ®ï¼Œ{ name:TensorMeta }ï¼Œæ¥ä»£è¡¨suc_vtæœ€åä¸€å±‚æ¥æ”¶dYçš„å…ƒæ•°æ®

    # æå‰ä½¿ç”¨P2Pæ–¹æ³•æ¥æ”¶åç»§ä»»åŠ¡çš„è¾“å…¥X/dY
    def prerecv_suc(self, suc_info): 
        ''' Prerecv successor group's 1st ubatch if exists. Call by main thread. 
            Assumption: same 1) 3) above
            Argument: suc_info = None or successor group's (named_metas, src_rank)
        '''  
        # è‹¥åç»§ä»»åŠ¡çš„ä¿¡æ¯ä¸ºNoneï¼Œç›´æ¥è¿”å›
        if suc_info is None:
            return
        # (suc_vtè¾“å…¥Xçš„metaï¼Œsuc_vté¦–å±‚çš„media)
        suc_named_metas, suc_src = suc_info
        # print("\trank{}: P2P.prerecv_suc({}, src{})".format(self.rank, suc_named_metas, suc_src))
        # must after is_end
        assert self.ubatch_idx == 0 and self.triple_bufs == [None, None, None]
        # record previous compute event (built-in)
        # 1.åœ¨ä»¥srcä½œä¸ºå‘å°„æºçš„é€šä¿¡ç»„ä¸Šï¼Œæ¥æ”¶ä»srcå‘é€ï¼ˆå¹¿æ’­ï¼‰è¿‡æ¥çš„tensorï¼Œä»¥éé˜»å¡çš„æ–¹å¼æ‰§è¡Œã€‚è¿”å› tensorï¼Œireqï¼ˆå¼‚æ­¥çš„å·¥ä½œå¥æŸ„ï¼‰
        # 2.ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ¥æ”¶åˆ°çš„tensorï¼ˆnamed_tensorsï¼‰å’Œå¼‚æ­¥å·¥ä½œå¥æŸ„
        self.triple_bufs[0] = self._irecv(suc_named_metas, suc_src, buffer=None)


# ç›´æ¥æš‚å­˜è¾“å‡ºçš„Y/dXï¼Œç»™åŒä¸€ä¸ªGPUä¸Šçš„ä¸‹ä¸€ä¸ªvtç”¨
# ğŸ“Œä¸ç”¨ç®¡ubatshsizeconverterï¼Œè¿™ä¸ªæ˜¯ç»™ç›¸é‚»çš„FWDvt/BWDvtç”¨çš„ï¼Œè·ŸFWD->BWDçš„stashXæ²¡å…³ç³»
class CacheX(object):
    def __init__(self, rank, world_size, reverse_bwd=True, verbose=False, nvprof=False):
        self.rank = rank
        self.world_size = world_size
        self.verbose = verbose
        self.nvprof = nvprof
        assert self.rank == torch.cuda.current_device() # torch.cuda.set_device(rank)

        # åº•å±‚å­˜å‚¨ç»“æ„
        self.self_dict = ODict()

    def fetch(self, layer_id):
        # return self.self_dict.pop(layer_id)
        print(f"rank:{self.rank},å–å‡ºlayer{layer_id}å¯¹åº”çš„cuda tensor")
        return self.self_dict[layer_id].pop(0)

    def put(self, layer_id, cuda_named_tensors):
        # self.self_dict[layer_id] = cuda_named_tensors
        if layer_id not in self.self_dict:
            self.self_dict[layer_id] = []
        print(f"rank:{self.rank}ï¼Œä¸ºlayer{layer_id}è£…å…¥äº†ä¸€ä¸ªcuda tensor")
        self.self_dict[layer_id].append(cuda_named_tensors)

    # def init_layer_ids(self, layer_ids): # always ascending
    #     assert isinstance(layer_ids,list)
    #     for id in sorted(layer_ids): 
    #         self.odict[id] = []
    #     self.layer_ids = list(self.odict.keys())



class P2PModel(object):
    """ Handle P2P allreduce for dW/B of vDP with NCCL 
        
        Feature: 
            0) Stateless (GPU memory are free'ed each microbatch)
            1) Everything in the "Main" thread 
            2) Use a new group/communicator/cudaStream
            3) Blocking
        
        Assumption:
            0) distributed environment has already been initialized with gloo
    """
    def __init__(self, rank, world_size, verbose=False):
        assert dist.get_backend() == "gloo"
        self.rank = rank
        self.world_size = world_size
        self.verbose = verbose
        assert self.rank == torch.cuda.current_device() # torch.cuda.set_device(rank)
        # build a world group for NCCL collectives
        tensor = torch.tensor(1.0, dtype=torch.float32, device="cuda:%d"%(self.rank))
        self.world_group = dist.new_group(ranks=list(range(self.world_size)), backend='nccl')
        dist.all_reduce(tensor, group=self.world_group, op=dist.ReduceOp.SUM)
        # print("[P2PM] rank={} init'ed world communicator of NCCL and its cudaStream".format(self.rank))
        del tensor; gc.collect(); torch.cuda.empty_cache()
        if self.verbose: self.allreduce_byte_cnt = 0

    @torch.no_grad()
    def average_gradients(self, model):
        """ GPU Gradient averaging. """
        size = float(self.world_size)
        for param in model.parameters():
            dist.all_reduce(param.grad.data, group=self.world_group, op=dist.ReduceOp.SUM)
            param.grad.data /= size
            if self.verbose: self.allreduce_byte_cnt += param.grad.data.nelement()*param.grad.data.element_size()
    
    @torch.no_grad()
    def average_buffers(self, model):
        """ Buffer averaging. """
        size = float(self.world_size)
        for buf in model.buffers():
            if isinstance(buf.data, torch.Tensor) and buf.data.dtype in [torch.float16, torch.float32, torch.float64]: 
                if buf.is_cuda:
                    dist.all_reduce(buf.data, group=self.world_group, op=dist.ReduceOp.SUM) # NCCL
                else: 
                    dist.all_reduce(buf.data, op=dist.ReduceOp.SUM) # Gloo
                buf.data /= size
                if self.verbose: self.allreduce_byte_cnt += buf.data.nelement()*buf.data.element_size()
