# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import threading
from collections import OrderedDict as ODict

import torch

from torch.autograd import Variable
from torch.cuda.nvtx import range_push as nvtx_range_push 
from torch.cuda.nvtx import range_pop as nvtx_range_pop 

import threadsafe_data_struct

# å®é™…è°ƒç”¨çš„è¿˜æ˜¯MSGstashXçš„çº¿ç¨‹å‘é€tensorï¼Œå³è°ƒç”¨MSGstashXçš„isendæ–¹æ³•ï¼Œè¯¥ç±»åªç›¸å½“äºåœ¨æ­£å¸¸çš„ä¸­é—´ç¯èŠ‚ä¸­æ·»åŠ ä¸€ä¸ªé¢å¤–çš„æ­¥éª¤ï¼Œ
# ç”¨äºå°†æ›´å¤§çš„FWDç”¨microbatchæ‹†åˆ†ä¸ºBWDç”¨microbatchã€‚
# åˆ†æï¼šå‰å‘microbatch sizeé»˜è®¤æ˜¯ä¸èƒ½è¶…è¿‡bwd microbatchsizeçš„ï¼Œé“ç†å¾ˆç®€å•ã€‚è‹¥fwdå°äºbwdï¼Œè¯¥microbatchå‹æ ¹ä¸èƒ½ç”¨äºBWDçš„æµç¨‹
# åªèƒ½è¢«å½“ä½œå‰©ä½™æ•°æ®ç­‰å¾…ä¸‹ä¸€ä¸ªiterationçš„æ–°æ•°æ®è¿›æ¥ï¼Œç»„åˆæˆä¸€ä¸ªè¶³å¤Ÿå¤§çš„tensorï¼Œåœ¨æ­¤ä¹‹å‰BWDæ— æ³•è¿›è¡Œï¼Œæ ¹æœ¬å°±æ— æ³•è¿›è¡Œæ­£å¸¸è®­ç»ƒ
class UBatchSizeConverter(object):
    """ Convert different microbatch sizes from forward to backward tasks.
        E.g. stashing X in vPP/vDP (including X to last layer pack in vDP)
        Assumption:
            0) know D, Ufwd, Ubwd in advance
            1) only CPU tensor and no grad
    """
    # u_fwdï¼šfwd micro batchçš„å¤§å°
    # ubatchszs_fwdï¼šfwd microbatchsizeçš„åˆ—è¡¨
    # u_bwdï¼šbwd micro batchçš„å¤§å°
    # ubatchszs_bwdï¼šbwd microbatchsizeçš„åˆ—è¡¨
    def __init__(self, rank, data_batchsize, u_fwd, ubatchszs_fwd, u_bwd, ubatchszs_bwd, output_method, pack_ordering=True, pin_memory=True, nvprof=False):
        self.rank = rank
        self.data_batchsize = data_batchsize
        self.u_fwd = u_fwd
        self.ubatchszs_fwd = ubatchszs_fwd
        self.u_bwd = u_bwd
        self.ubatchszs_bwd = ubatchszs_bwd
        # ä¿é™©æ“ä½œï¼Œå‰åå‘microbatchsizeä¸ä¸€æ ·æ‰ä¼šå®ä¾‹åŒ–è¯¥ä¾‹ï¼Œå› æ­¤ä¸€æ ·çš„è¯è¿™é‡Œä¼šè¾“å‡ºä¸€ä¸ªwarningä¿¡æ¯
        if u_fwd == u_bwd: # assert u_fwd != u_bwd
            print("[UBatchSizeConverter] --- Warning: Ufwd = Ubwd ! ---") 
        assert data_batchsize >= u_fwd and data_batchsize >= u_bwd
        self.pin_memory = pin_memory
        self.nvprof = nvprof
        
        # 
        self._initialize(output_method, pack_ordering)
        self._start_helper_thread()

        # print("[UBatchSizeConverter] __init__: rank {} has D={}, Ufwd={} ({}), Ubwd={} ({})".format(self.rank, self.data_batchsize, self.u_fwd, self.ubatchszs_fwd, self.u_bwd, self.ubatchszs_bwd))

    # åˆå§‹åŒ–ä¸€äº›æ•°æ®ç»“æ„
    def _initialize(self, output_method, pack_ordering=True):
        """
        Initialize state needed for sub-thread. 
        Argument: output_method(layer_id, named_tensor)
                  pack_ordering = bool : whether convert in layer or pack ordering (this ordering is self contained)
        """
        # å®ä¾‹åŒ–ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„æ ˆ
        self.input_queue = threadsafe_data_struct.Queue()
        self.residual = ODict() # { layer_id: named_tensors (with ubatchsize < Ubwd) }
        self.cnt_converted_ubatch = ODict() # { layer_id: cnt }
        self.output_method = output_method # can be dict
        assert self.output_method is not None
        self.pack_ordering = pack_ordering
        assert isinstance(pack_ordering, bool)

    def _start_helper_thread(self):
        helper_thread = threading.Thread(target=self._helper_thread)
        helper_thread.daemon = True
        helper_thread.start()
        # print("[UBatchSizeConverter] rank{} started converter helper thread".format(self.rank))
    
    def _helper_thread(self):
        """ This method is to be executed from a helper daemon thread. """
        if self.pack_ordering: # output in pack ordering: [X0:#1, X1:#1], [X0:#2, X1:#2]
            # print("[UBatchSizeConverter] uses pack ordering")
            while True:
                packed_named_tensors, is_convert = self.input_queue.remove() # [ [ layer_id, named_tensors ] ] # a pack at an ubatch
                if not is_convert:
                    for layer_id, named_tensors in packed_named_tensors:
                        self.output_method(layer_id, named_tensors)
                    continue
                # converting
                layer_converted = ODict() # { layer_id: [#1 cvt_named_tensor, #2 cvt_named_tensor] } 
                for layer_id, named_tensors in packed_named_tensors:
                    converted = self._convert_ubatchsize(layer_id, named_tensors) # this layer's [ { cvt_named_tensor } ]
                    if converted == []:
                        # print("[UBatchSizeConverter] rank{}: converted is empty".format(self.rank))
                        continue
                    else:
                        layer_converted[layer_id] = converted
                #
                if layer_converted:
                    num_ubwds = set()
                    for converted in layer_converted.values():
                        num_ubwds.add(len(converted))
                    assert len(num_ubwds) == 1, "layers in a pack must have equal num of ubwd to send"
                    for idx in range(list(num_ubwds)[0]):
                        for layer_id, converted in layer_converted.items():
                            self.output_method(layer_id, converted[idx])
                            # print("[UBatchSizeConverter] rank{}: outputed L{}".format(self.rank, layer_id))

        # æ‰§è¡Œè¿™ä¸ª
        else: # output in layer ordering: X0:[#1,#2,#3] then X1:[#1,#2,#3]
            # print("[UBatchSizeConverter] uses layer ordering")
            while True:
                # isendæ”¾å…¥input_queueä¸­çš„is_converté»˜è®¤ä¸ºtrue
                layer_id, named_tensors, is_convert = self.input_queue.remove() # a layer at an ubatch
                # æ‰€ä»¥è¿™é‡Œä¹Ÿä¸ä¼šæ‰§è¡Œ
                if not is_convert:
                    self.output_method(layer_id, named_tensors)
                    continue
                # converting
                if self.nvprof: nvtx_range_push("__L{} ConvertU(X)".format(layer_id)) 
                # 1.å°†ä¼ è¿›æ¥çš„stashXè¿™ä¸ªtensoræŒ‰ç…§BWDéœ€è¦çš„microbatchå¤§å°è¿›è¡Œåˆ†å—ï¼Œï¼ˆå½“ç„¶ï¼Œæœ€åä¸€ä¸ªå—å¯èƒ½æ²¡æœ‰u_bwdçš„å¤§å°ï¼‰ã€‚
                #   è‹¥self.residualä¸­å­˜åœ¨ layer_id è¿™ä¸ªé”®ï¼Œè¯´æ˜ä¸Šä¸€æ¬¡iterationä¸­ï¼Œè¯¥layer_idçš„è¾“å…¥Xæ²¡æœ‰å…¨éƒ¨ä½¿ç”¨ï¼Œå­˜åœ¨å‰©ä½™
                #   æ•°æ®ï¼Œå³æœ€åä¸€ä¸ªæ‹†åˆ†å‡ºæ¥çš„tensorå—çš„å¤§å°å°äºåå‘æ—¶éœ€è¦çš„microbatchçš„å¤§å°ï¼Œä¸èƒ½ç›´æ¥è¢«BWDä»»åŠ¡ä½¿ç”¨ã€‚æ­¤æ—¶ï¼Œ
                #   å°†å‰©ä½™æ•°æ®æ‹¼æ¥åˆ°å½“å‰è¾“å…¥tensorçš„åé¢ 
                # 2.å¯¹æ‹†åˆ†åçš„æ¯ä¸€ä¸ªtensorå—è¿›è¡Œæ£€æŸ¥ï¼Œæ ¹æ®å…¶å¤§å°æ˜¯å¦ç­‰äºåå‘ä¼ æ’­æ—¶microbatchçš„å¤§å°ï¼Œå†³å®šå°†å…¶æ”¾å…¥convertedåˆ—è¡¨ï¼Œ
                #   è¿˜æ˜¯self.residualä¸­ï¼Œå³ç”¨äºä¿å­˜å‰©ä½™æ•°æ®çš„å­—å…¸{layer_id: not_ready(ä¸€ä¸ªå­—å…¸{name:tensor})}
                # 3.æœ€ç»ˆè¿”å›convertedåˆ—è¡¨ï¼Œå³æ‰€æœ‰ç¬¦åˆæ ‡å‡†ï¼Œå¤§å°ä¸åå‘Microbatchsizeç›¸åŒçš„tesnorå—çš„åˆ—è¡¨ 
                converted = self._convert_ubatchsize(layer_id, named_tensors) # this layer's [ { named_tensors of Ubwd } ]
                if converted == []:
                    # print("[UBatchSizeConverter] rank{}: converted is empty".format(self.rank))
                    if self.nvprof: nvtx_range_pop() 
                    continue
                else:
                    # å°†convertå¥½çš„tensoråˆ—è¡¨åŠ å…¥åˆ°MSGstashXçš„send_ditcå­—å…¸ä¸­ï¼Œè¿™ä¹Ÿæ„å‘³ç€MSGstashXå®ä¾‹çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œ
                    # å‘ç›®æ ‡rankçš„å‘é€ä»»åŠ¡ã€‚å³æœ€ç»ˆè¿˜æ˜¯ç”¨MSGstashXå®ä¾‹çš„isendæ–¹æ³•å°†convertåçš„tensorå‘é€åˆ°ç›®æ ‡rank
                    for cvt_named_tensor in converted:
                        self.output_method(layer_id, cvt_named_tensor)
                        # print("[UBatchSizeConverter] rank{}: outputed L{}".format(self.rank, layer_id))
                    if self.nvprof: nvtx_range_pop() 

    # 1.å°†ä¼ è¿›æ¥çš„stashXè¿™ä¸ªtensoræŒ‰ç…§BWDéœ€è¦çš„microbatchå¤§å°è¿›è¡Œåˆ†å—ï¼Œï¼ˆå½“ç„¶ï¼Œæœ€åä¸€ä¸ªå—å¯èƒ½æ²¡æœ‰u_bwdçš„å¤§å°ï¼‰ã€‚
    #   è‹¥self.residualä¸­å­˜åœ¨ layer_id è¿™ä¸ªé”®ï¼Œè¯´æ˜ä¸Šä¸€æ¬¡iterationä¸­ï¼Œè¯¥layer_idçš„è¾“å…¥Xæ²¡æœ‰å…¨éƒ¨ä½¿ç”¨ï¼Œå­˜åœ¨å‰©ä½™
    #   æ•°æ®ï¼Œå³æœ€åä¸€ä¸ªæ‹†åˆ†å‡ºæ¥çš„tensorå—çš„å¤§å°å°äºåå‘æ—¶éœ€è¦çš„microbatchçš„å¤§å°ï¼Œä¸èƒ½ç›´æ¥è¢«BWDä»»åŠ¡ä½¿ç”¨ã€‚æ­¤æ—¶ï¼Œ
    #   å°†å‰©ä½™æ•°æ®æ‹¼æ¥åˆ°å½“å‰è¾“å…¥tensorçš„å‰é¢ ï¼Œè€Œåå†è¿›è¡Œåˆ†å—
    # 2.å¯¹æ‹†åˆ†åçš„æ¯ä¸€ä¸ªtensorå—è¿›è¡Œæ£€æŸ¥ï¼Œæ ¹æ®å…¶å¤§å°æ˜¯å¦ç­‰äºåå‘ä¼ æ’­æ—¶microbatchçš„å¤§å°ï¼Œå†³å®šå°†å…¶æ”¾å…¥convertedåˆ—è¡¨ï¼Œ
    #   è¿˜æ˜¯self.residualä¸­ï¼Œå³ç”¨äºä¿å­˜å‰©ä½™æ•°æ®çš„å­—å…¸{layer_id: not_ready(ä¸€ä¸ªå­—å…¸{name:tensor})}
    # 3.æœ€ç»ˆè¿”å›convertedåˆ—è¡¨ï¼Œå³æ‰€æœ‰ç¬¦åˆæ ‡å‡†ï¼Œå¤§å°ä¸åå‘Microbatchsizeç›¸åŒçš„tesnorå—çš„åˆ—è¡¨     
    def _convert_ubatchsize(self, layer_id, named_tensors):
        """
        Helper thread converts one layer's tensors from Ufwd to Ubwd.
        Use previously stored residual tensors (not sufficient for Ubwd) for each convert call.
        Store residual tensors of this convert call for the next one.
        Return converted = [ { named_tensors of Ubwd } ] or []

        Note: the actually residual memory in pytorch == a residual Ubwd + an extra Ufwd == the concat'ed size 
              (i.e., _concat_tensors create an atomic big tensor. Even if a split of it gets deleted, the entire concat'ed memory is still there. Unless all splits gets deleted.)
        """
        # find new split
        named_split = ODict() # { name: (t1,t2), name: (c1,c2), name: [ (t1,t2), (t1,t2) ] }
        num_split = set()
        # self.residual = ODict() # { layer_id: named_tensors (with ubatchsize < Ubwd) }
        for name,tensor in named_tensors.items(): # { name: tensor/const, name: [tensors] }
            if isinstance(tensor, (torch.Tensor,Variable)):
                # assert not tensor.is_cuda and not tensor.requires_grad
                # è‹¥self.residualä¸­å­˜åœ¨layer_idè¿™ä¸ªé”®ï¼Œè¯´æ˜ä¸Šä¸€æ¬¡iterationä¸­ï¼Œè¯¥layer_idçš„è¾“å…¥Xæ²¡æœ‰å…¨éƒ¨ä½¿ç”¨ï¼Œå­˜åœ¨å‰©ä½™
                # æ•°æ®ï¼Œå³æœ€åä¸€ä¸ªæ‹†åˆ†å‡ºæ¥çš„tensorå—çš„å¤§å°å°äºåå‘æ—¶éœ€è¦çš„microbatchçš„å¤§å°ï¼Œä¸èƒ½ç›´æ¥è¢«BWDä»»åŠ¡ä½¿ç”¨ã€‚æ­¤æ—¶ï¼Œ
                # å°†å‰©ä½™æ•°æ®æ‹¼æ¥åˆ°å½“å‰è¾“å…¥tensorçš„å‰é¢
                if layer_id in self.residual: # and name in self.residual[layer_id]:
                    # åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šæ‹¼æ¥tensorï¼Œå¹¶æ”¾åˆ°pinned memoryä¸­
                    concat_tensor = self._concat_tensors((self.residual[layer_id][name],tensor))
                else:
                    concat_tensor = tensor
                # åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šï¼Œå°†tensoræ‹†åˆ†ä¸ºå¤šä¸ªå¿«ï¼Œ æ¯ä¸ªåˆ†å—çš„å¤§å°ä¸º u_bwd (åå‘ä¼ æ’­çš„microbatch size)ï¼Œ
                # å¹¶å°†è¿™äº›tensorä»¥tupleå½¢å¼è¿”å›ã€‚ï¼ˆå½“ç„¶ï¼Œæœ€åä¸€ä¸ªå—å¯èƒ½æ²¡æœ‰u_bwdçš„å¤§å°ï¼‰
                named_split[name] = self._split_tensor(concat_tensor, self.u_bwd) # (t1,t2) or (t1,res) or (t1,) or (res,)
                # å°†å½“å‰tensoråˆ†å—çš„æ•°é‡æ·»åŠ åˆ°setä¸­
                num_split.add(len(named_split[name]))
            elif isinstance(tensor, int):
                assert tensor in self.ubatchszs_fwd, "convert ubatchsize on unknown int value={}".format(tensor) # TODO: can use repeated int const
                if layer_id in self.residual: # and name in self.residual[layer_id]:
                    concat_tensor = self._concat_const_ubatchsizes((self.residual[layer_id][name],tensor))
                else:
                    concat_tensor = tensor
                named_split[name] = self._split_const_ubatchsize(concat_tensor, self.u_bwd) # (c1,c2) or (c1,res) or (c1,) or (res,)
                num_split.add(len(named_split[name]))
            elif isinstance(tensor, list): # output tuple of bert pretrainhead 
                tmp = []
                for i,t in enumerate(tensor):
                    # assert not t.is_cuda and not t.requires_grad
                    if layer_id in self.residual: # and name in self.residual[layer_id]:
                        concat_t = self._concat_tensors((self.residual[layer_id][name][i],t))
                    else:
                        concat_t = t
                    tmp.append(self._split_tensor(concat_t, self.u_bwd)) # (t1,t2) or (t1,res) or (t1,) or (res,)
                    num_split.add(len(tmp[-1]))
                named_split[name] = tmp
            else:
                raise ValueError("unknown tensor type to convert ={}".format(type(tensor)))
        # save residual and return converted
        assert len(num_split) == 1, "num_split must be unique"
        # ä»self.residualä¸­åˆ é™¤å½“å‰layer_idçš„é”®å€¼å¯¹ï¼Œå› ä¸ºå‰©ä½™æ•°æ®ï¼Œå³ä¸€ä¸ªtensorå—ï¼Œå·²ç»æ‹¼æ¥åˆ°å½“å‰tensorä¸Šäº†
        if layer_id in self.residual: # { layer_id: named_tensors (with ubatchsize < Ubwd) }
            del self.residual[layer_id] 
        # è‹¥å½“å‰è¿™ä¸ªlayer_idè¿˜æ²¡å¤„ç†è¿‡ï¼Œå³è¿˜æ²¡è¿›è¡Œè¿‡è½¬åŒ–ï¼Œåˆå§‹åŒ–å…¶è½¬åŒ–ä¸º0
        if not layer_id in self.cnt_converted_ubatch: # { layer_id: cnt }
            self.cnt_converted_ubatch[layer_id] = 0
        # ä»¥è¯¥layerçš„å·²è½¬åŒ–æ¬¡æ•°ä¸ºä¸‹æ ‡ï¼Œä»ubatchszs_bwdä¸­å–å‡ºbwd microbatchçš„å¤§å°
        u_bwd = self.ubatchszs_bwd[self.cnt_converted_ubatch[layer_id]]
        converted = []
        # å¯¹æ‹†åˆ†åçš„æ¯ä¸€ä¸ªtensorå—è¿›è¡Œæ£€æŸ¥ï¼Œæ ¹æ®å…¶å¤§å°æ˜¯å¦ç­‰äºåå‘ä¼ æ’­æ—¶microbatchçš„å¤§å°ï¼Œå†³å®šå°†å…¶æ”¾å…¥convertedåˆ—è¡¨ï¼Œ
        # è¿˜æ˜¯self.residualä¸­ï¼Œå³ç”¨äºä¿å­˜å‰©ä½™æ•°æ®çš„å­—å…¸{layer_id: not_ready(ä¸€ä¸ªå­—å…¸{name:tensor})}
        # éå†æ¯ä¸€ä¸ªtensorå—ï¼Œæ‰§è¡Œï¼š
        # 1.å¯¹æ‹†åˆ†åçš„æ¯ä¸€ä¸ªtensorå—è¿›è¡Œæ£€æŸ¥ï¼š
        #   1.1.è‹¥å½“å‰tensorå—çš„å¤§å°å’Œåå‘ä¼ æ’­çš„å¤§å°ç›¸åŒï¼Œåˆ™è¡¨æ˜è¯¥tensorå—å¯ä»¥ç›´æ¥ç”¨ï¼Œæ”¾è¿›readyå­—å…¸ä¸­{nameï¼štensor}
        #   1.2.å¦åˆ™ï¼Œä¸èƒ½ç›´æ¥ç»™åå‘ä»»åŠ¡ç”¨ï¼Œæ”¾è¿›not_readyå­—å…¸ä¸­
        # 2.
        # 2.1.è‹¥readyå­—å…¸ä¸ä¸ºç©ºï¼Œè¯´æ˜å½“å‰åˆ†å‰²å‡ºæ¥çš„tensorå—çš„å¤§å°ç­‰äºåå‘ä¼ æ’­æ—¶microbatchçš„å¤§å°ï¼Œå°†è¯¥å­—å…¸æ·»åŠ åˆ°convertedåˆ—è¡¨ä¸­ï¼Œ
        #     å¹¶å°†è¯¥layer_idçš„è®¡æ•°+1ï¼Œè¡¨ç¤ºæ‰§è¡Œäº†ä¸€æ¬¡microbatchå¤§å°çš„è½¬æ¢ã€‚è‹¥æ‰€æœ‰çš„microbatchå·²ç»è½¬æ¢å®Œäº†ï¼Œå°†è¯¥å±‚çš„è®¡æ•°ç½®0ï¼Œä»¥ä¾¿
        #     ä¸‹ä¸€æ¬¡iterationæ­£ç¡®æ‰§è¡Œ
        # 2.2.è‹¥not_readyå­—å…¸ä¸ä¸ºç©ºï¼Œè¯´æ˜æœ€åä¸€ä¸ªtensorå—çš„å¤§å°å°äºåå‘ä¼ æ’­æ—¶microbatchçš„å¤§å°ï¼Œä¸èƒ½ç›´æ¥ç”¨
        #     ç›´æ¥å°†å…¶ä¿å­˜åˆ°self.residualè¿™ä¸ªå‰©ä½™æ•°æ®å­—å…¸ä¸­{layer_id: not_ready(ä¸€ä¸ªå­—å…¸{name:tensor})}
        #
        # ä¸€æ¬¡å¾ªç¯åªå¤„ç†ä¸€ä¸ªæ‹†åˆ†åçš„tensorï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€æ¬¡å¾€convertedä¸­è£…ä¸€ä¸ª name:tensorï¼Œæœ€ç»ˆconvertedä¸­è¿™å‡ ä¸ªé”®å€¼å¯¹nameéƒ½æ˜¯ä¸€æ ·çš„
        for j in range(list(num_split)[0]):
            ready = ODict() # { name: t1, name: c1, name: [t1,t1] }
            not_ready = ODict() 
            # ğŸ“Œè™½ç„¶ç”¨çš„æ˜¯forå¾ªç¯ï¼Œnamed_splitè¿™ä¸ªå­—å…¸åªæœ‰ä¸€ä¸ªé”®å€¼å¯¹ï¼Œå› æ­¤forå¾ªç¯åªæ‰§è¡Œäº†ä¸€æ¬¡
            for name, split in named_split.items(): # { name: (t1,t2), name: (c1,c2), name: [ (t1,t2), (t1,t2) ] }
                # print("[UBatchSizeConverter] rank{}'s named_split has {}:{}".format(self.rank, name, split)) 
                if isinstance(split,tuple) and isinstance(split[j], (torch.Tensor,Variable)):
                    tensor = split[j]
                    # è‹¥å½“å‰tensorå—çš„å¤§å°å’Œåå‘ä¼ æ’­çš„å¤§å°ç›¸åŒï¼Œåˆ™è¡¨æ˜è¯¥tensorå—å¯ä»¥ç›´æ¥ç”¨ï¼Œæ”¾è¿›readyå­—å…¸ä¸­{nameï¼štensor}
                    if tensor.size(0) == u_bwd: # 0-dim matches desired ubatchsize
                        ready[name] = tensor
                    # å¦åˆ™ï¼Œä¸èƒ½ç›´æ¥ç»™åå‘ä»»åŠ¡ç”¨ï¼Œæ”¾è¿›not_readyå­—å…¸ä¸­
                    elif tensor.size(0) < u_bwd: # residual
                        not_ready[name] = tensor
                    else:
                        raise ValueError
                elif isinstance(split,tuple) and isinstance(split[j], int):
                    tensor = split[j]
                    if tensor == u_bwd: # 0-dim matches desired ubatchsize
                        ready[name] = tensor
                    elif tensor < u_bwd: # residual
                        not_ready[name] = tensor
                    else:
                        raise ValueError
                elif isinstance(split,list):
                    # tmp_tensor, match_flag = [], []
                    # for s in split:
                    #     tensor = s[j]
                    #     tmp_tensor.append(tensor)
                    #     if tensor.size(0) == u_bwd: # 0-dim matches desired ubatchsize
                    #         match_flag.append(True)
                    #     elif tensor.size(0) < u_bwd: # residual
                    #         match_flag.append(False)
                    #     else:
                    #         raise ValueError
                    # if match_flag == [True]*len(match_flag):
                    #     ready[name] = tmp_tensor
                    # elif match_flag == [False]*len(match_flag):
                    #     not_ready[name] = tmp_tensor
                    # else:
                    #     raise ValueError
                    tmp1, tmp2 = [], []
                    for s in split:
                        tensor = s[j]
                        if tensor.size(0) == u_bwd: # 0-dim matches desired ubatchsize
                            tmp1.append(tensor)
                        elif tensor.size(0) < u_bwd: # residual
                            tmp2.append(tensor)
                        else:
                            raise ValueError
                    if tmp1 != []:
                        ready[name] = tmp1
                    elif tmp2 != []:
                        not_ready[name] = tmp2
                    else:
                        raise ValueError
                else:
                    raise ValueError
            # è‹¥readyå­—å…¸ä¸ä¸ºç©ºï¼Œè¯´æ˜å½“å‰åˆ†å‰²å‡ºæ¥çš„tensorå—çš„å¤§å°ç­‰äºåå‘ä¼ æ’­æ—¶microbatchçš„å¤§å°ï¼Œå°†è¯¥å­—å…¸æ·»åŠ åˆ°convertedåˆ—è¡¨ä¸­ï¼Œ
            # å¹¶å°†è¯¥layer_idçš„è®¡æ•°+1ï¼Œè¡¨ç¤ºæ‰§è¡Œäº†ä¸€æ¬¡microbatchå¤§å°çš„è½¬æ¢ã€‚è‹¥æ‰€æœ‰çš„microbatchå·²ç»è½¬æ¢å®Œäº†ï¼Œå°†è¯¥å±‚çš„è®¡æ•°ç½®0ï¼Œä»¥ä¾¿
            # ä¸‹ä¸€æ¬¡iterationæ­£ç¡®æ‰§è¡Œ
            if ready:
                assert list(ready.keys()) == list(named_tensors.keys()), "{} vs. {}".format(list(ready.keys()), list(named_tensors.keys()))
                # å°†è¯¥å­—å…¸æ·»åŠ åˆ°convertedåˆ—è¡¨ä¸­
                converted.append(ready)
                # å°†è¯¥layer_idçš„è®¡æ•°+1ï¼Œè¡¨ç¤ºæ‰§è¡Œäº†ä¸€æ¬¡microbatchå¤§å°çš„è½¬æ¢
                self.cnt_converted_ubatch[layer_id] += 1
                cnt = self.cnt_converted_ubatch[layer_id]
                # è‹¥è¿˜æ²¡æ‰§è¡Œåˆ°æœ€åä¸€æ¬¡è½¬æ¢ï¼ˆæ€»æ¬¡æ•°ä¸ºBWD microbatchsizeçš„åˆ—è¡¨ï¼Œå³åå‘è¦æ‰§è¡Œå‡ ä¸ªmicrobatchï¼‰ï¼Œ
                # ubwdï¼Œå³åå‘çš„microbatch sizeï¼Œå–self.ubatchszs_bwd[cnt]ï¼Œå®é™…åªè¦ä¸æ˜¯æœ€åä¸€ä¸ªï¼Œå€¼éƒ½æ˜¯ä¸€æ ·çš„
                if cnt < len(self.ubatchszs_bwd): # not last ubatch yet
                    u_bwd = self.ubatchszs_bwd[cnt]
                    # print("[UBatchSizeConverter] rank{}: converted L{}'s {} ubatches".format(self.rank,layer_id,cnt))
                
                # æ‰€æœ‰çš„microbatchå·²ç»è½¬æ¢å®Œäº†ï¼Œå°†è¯¥å±‚çš„è®¡æ•°ç½®0ï¼Œä»¥ä¾¿ä¸‹ä¸€æ¬¡iterationæ­£ç¡®æ‰§è¡Œ
                else: # last ubatch done (of this iteration)
                    u_bwd = -1 # prevent keep looping
                    self.cnt_converted_ubatch[layer_id] = 0 # reset for next iteration
                    assert not layer_id in self.residual, "no more residual left"
                    # print("[UBatchSizeConverter] rank{}: converted L{}'s All {} ubatches".format(self.rank,layer_id,cnt))
            # è‹¥not_readyå­—å…¸ä¸ä¸ºç©ºï¼Œè¯´æ˜æœ€åä¸€ä¸ªtensorå—çš„å¤§å°å°äºåå‘ä¼ æ’­æ—¶microbatchçš„å¤§å°ï¼Œä¸èƒ½ç›´æ¥ç”¨
            # ç›´æ¥å°†å…¶ä¿å­˜åˆ°self.residualè¿™ä¸ªå‰©ä½™æ•°æ®å­—å…¸ä¸­
            elif not_ready:
                assert j == list(num_split)[0]-1, "residual must be the last split"
                assert list(not_ready.keys()) == list(named_tensors.keys())
                self.residual[layer_id] = not_ready
            else:
                raise ValueError
        # clean up
        del named_split
        
        # æœ€ç»ˆè¿”å›æ‰€æœ‰ç¬¦åˆæ ‡å‡†ï¼Œå³å¤§å°ä¸åå‘Microbatchsizeç›¸åŒçš„tesnorå—çš„åˆ—è¡¨
        return converted
                
    # åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šæ‹¼æ¥tensorï¼Œå¹¶æ”¾åˆ°pinned memoryä¸­
    def _concat_tensors(self, tensors):
        for t in tensors:
            # assert isinstance(t, (torch.Tensor,Variable))
            # assert not t.is_cuda and not t.requires_grad
            assert t.ndim > 0, "scalar tensor cannot be concat'ed"
        # dim=0 must be ubatchsize
        if self.pin_memory:
            # åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šæ‹¼æ¥tensorï¼Œå¹¶æ”¾åˆ°pinned memoryä¸­
            return torch.cat(tensors, dim=0).pin_memory() # create new memory # inherit tensor's device
        else:
            return torch.cat(tensors, dim=0)

    # åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šï¼Œå°†tensoræ‹†åˆ†ä¸º split_size ä¸ªï¼Œå¹¶å°†è¿™äº›tensorä»¥åˆ—è¡¨å½¢å¼è¿”å›
    def _split_tensor(self, t, split_size):
        # assert isinstance(t, (torch.Tensor,Variable))
        # assert not t.is_cuda and not t.requires_grad
        assert t.ndim > 0, "scalar tensor cannot be split'ed"
        # dim=0 must be ubatchsize
        return torch.split(t, split_size, dim=0) # share the same underlying memory # inherit tensor's device
        # tensor will be split into equally sized chunks (if possible). 
        # Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.
    
    def _concat_const_ubatchsizes(self, Cs):
        return int(sum(Cs))

    def _split_const_ubatchsize(self, C, U):
        assert isinstance(C, int) and isinstance(U, int)
        if C >= U:
            if C % U == 0:
                ubatchszs = [U] * int(C/U)
            else:
                ubatchszs = [U] * int(C/U) + [ C%U ]
            assert sum(ubatchszs) == C
        else: # not sufficient to split
            ubatchszs = [C]
        return tuple(ubatchszs)
    
    # inputï¼šlayer_id
    # input2ï¼šcpu_named_tensors
    # å°†layer_idå’Œinput2ï¼šcpu_named_tensorsåŠ å…¥åˆ° input_queue é˜Ÿåˆ—ä¸­ï¼Œè¿™æ„å‘³ç€UBatchSizeConverterå®ä¾‹çš„çº¿ç¨‹
    # å°†å¼€å§‹æ‰§è¡Œtensorå¤§å°çš„è½¬æ¢ï¼Œè€Œåå°†convertå¥½çš„tensoråˆ—è¡¨åŠ å…¥åˆ°MSGstashXçš„send_ditcå­—å…¸ä¸­ï¼Œè¿™ä¹Ÿæ„å‘³ç€
    # MSGstashXå®ä¾‹çš„çº¿ç¨‹å°†å¼€å§‹æ‰§è¡Œå‘ç›®æ ‡rankçš„å‘é€ä»»åŠ¡
    def isend(self, input, input2=None, is_convert=True):
        ''' 
        Call by upstream thread. Nonblocking send.
        Argument: 
            input, input2 = 
                [ [ layer_id, named_tensors ] ] -- a pack at an ubatch
                or [ layer_id, named_tensors ]  -- a layer at an ubatch
                or layer_id, named_tensors      -- a layer at an ubatch
            is_convert = whether to convert this ubatch
        '''
        # è¯¥å‚æ•°ä¸ºfalseï¼Œæ‰§è¡Œelse
        if self.pack_ordering:
            assert isinstance(input, list) and isinstance(input[0], list) and len(input[0])==2
            self.input_queue.add([input, is_convert])
        # 
        else:
            if input2 is None:
                assert isinstance(input, list) and len(input)==2
                self.input_queue.add( input+is_convert ) 
            else:
                self.input_queue.add([input, input2, is_convert]) 
