# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from __future__ import division, print_function
import argparse
import os
import sys
import json
import numpy as np
import gc
from collections import OrderedDict as ODict
from copy import deepcopy

import torch
from torch.autograd import Variable
from torch.utils.data import DataLoader, RandomSampler
import torch.distributed as dist
mp = torch.multiprocessing.get_context('spawn') # for GPU usage

def _assert_assumption(CONFIGS):
    # GPU only
    assert torch.cuda.is_available()
    # FP32 only
    torch.set_default_tensor_type('torch.FloatTensor')
    # Optimizer offload
    assert CONFIGS["opt_offld"]
    # double buffer need equal size
    if CONFIGS['mode'] == 'vPP':
        assert len(set(CONFIGS['ubatchszs_fwd'])) == 1
        assert len(set(CONFIGS['ubatchszs_bwd'])) == 1
    elif CONFIGS['mode'] == 'vDP': 
        for ubatchszs_fwd, ubatchszs_bwd in zip(CONFIGS['ubatchszs_fwd'], CONFIGS['ubatchszs_bwd']):
            assert len(set(ubatchszs_fwd)) == 1
            assert len(set(ubatchszs_bwd)) == 1
    else:
        raise ValueError
    # a single BWD task should have equal ubatchszs_fwd and ubatchszs_bwd per GPU
    if CONFIGS["pack_fwd"] == []: 
        assert CONFIGS["u_fwd"] == CONFIGS["u_bwd"]
        if CONFIGS['mode'] == 'vPP':
            assert CONFIGS['ubatchszs_fwd'] == CONFIGS['ubatchszs_bwd']
        elif CONFIGS['mode'] == 'vDP': 
            for ubatchszs_fwd, ubatchszs_bwd in zip(CONFIGS['ubatchszs_fwd'], CONFIGS['ubatchszs_bwd']):
                assert ubatchszs_fwd == ubatchszs_bwd
        else:
            raise ValueError

def worker_func(*pargs, **kwargs): # per process
    # 1.workerï¼šharmonyåŸç‰ˆä»£ç 
    # 2.worker_moniterï¼šç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
    from worker import Worker
    w = Worker(*pargs, **kwargs)
    # w.run_initial_iteration()
    w.run_training_loop()
    w.finish()
        
def run(args, real_dataset, create_model, create_optimizer, get_train_steps=None, get_lr_sched=None, compute_loss=None, save_model=None): # main process
    
    import seeding
    seeding.seed(args.seed, args.seed_cudnn)
    
    """ Initialize Harmony. """
    # å³è£…ç€åˆ‡åˆ†æˆå±‚çš„ä»£ç è·¯å¾„
    module_path = os.path.join(args.module_dir, args.module_name)
    assert os.path.exists(module_path)
    assert os.path.basename(module_path) not in ["prof", "sched"], "no base_dir in module_path"
    
    # read profiles
    from prof_data_struct import ConstMeta, TensorMeta, XMeta, TMeta, load_prof_data_struct
    prof = ODict()
    print("........args.profile_fnames: ",args.profile_fnames)
    # ['prof_XMETA', 'prof_TMETA']
    for name in args.profile_fnames:
        key = name.split("prof_")[-1]
        # åŠ è½½ä¿å­˜åœ¨pickleæ–‡ä»¶ä¸­çš„æ•°æ®ç»“æ„
        prof[key] = load_prof_data_struct(module_path, name + args.suffix, base_dir="my_prof", verbose=True)
    
    # read schedule
    from task_data_struct import Medium, vTask, unserialize_scheduled
    # åœ¨è¯¥ä¾‹å­ä¸­ï¼Œè¿™ä¸ªå‚æ•°å°±æ˜¯ç©º
    if args.schedule_dir == "":
        args.schedule_dir = module_path
    # ä»æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ä¸­ååºåˆ—åŒ–ï¼ˆunserializeï¼‰è°ƒåº¦ï¼ˆscheduleï¼‰æ•°æ®
    rTASKS, CONFIGS = unserialize_scheduled(args.schedule_dir, args.schedule_fname + args.suffix, base_dir="my_sched", verbose=False)
    _assert_assumption(CONFIGS)
    
    """ Initialize data. """
    if args.synthetic_data:
        args.num_epochs = 1
        assert args.num_iters is not None
        args.num_train_steps = args.num_iters
        print('----- Training Info -----')
        print("  num epoches = %d" % args.num_epochs)
        print("  num iterations per epoch = %d" % (args.num_iters))
        print("  num optimization steps = %d" % (args.num_train_steps))

    # æœ¬ä¾‹å­ä¼šæ‰§è¡Œelseçš„é€»è¾‘
    # æ”¶é›†æˆ–è®¡ç®—è®­ç»ƒçš„ä¿¡æ¯å¹¶æ‰“å°å‡ºæ¥ï¼Œä¸€æ¬¡epochçš„Iterationæ¬¡æ•°ã€epochæ¬¡æ•°ã€æ€»iterationæ¬¡æ•°
    else:
        # data_loader: ç”¨äºè¿­ä»£æ•°æ®é›†çš„DataLoaderå®ä¾‹
        data_loader, examples, _, _, _, _, _ = real_dataset(args, CONFIGS["D"], data_workers=0)
        print(f"........len(data_loader):{len(data_loader)}")
        if get_train_steps is not None: # "bert_thomwolf"
            args.num_train_steps = get_train_steps(args, examples, CONFIGS["D"])
        else:
            # è®­ç»ƒçš„stepæ•°ï¼šminibatchçš„æ•°é‡ Ã— epochæ¬¡æ•°
            args.num_train_steps = len(data_loader) * args.num_epochs
        # è®¾ç½®ä¸€ä¸ªepochè¿­ä»£çš„æ¬¡æ•°
        if args.num_iters is None:
            args.num_iters = len(data_loader) # num_minibatch
        else:
            # è‹¥æ‰‹åŠ¨è®¾ç½®äº†ä¸€ä¸ªepochè¿­ä»£çš„æ¬¡æ•°ï¼Œè¿™é‡Œè¦åœ¨æ‰‹åŠ¨è®¾ç½®çš„çš„æ¬¡æ•°å’Œminibatchçš„æ•°é‡é—´å–æœ€å°å€¼
            args.num_iters = min(args.num_iters, len(data_loader))
        print('----- Training Info -----')
        print("  num epoches = %d" % args.num_epochs)
        print("  num minibatches per epoch = %d" % len(data_loader))
        print("  num iterations per epoch = %d" % (args.num_iters))
        print("  num optimization steps = %d" % (args.num_train_steps))
        del data_loader
    
    # æµ‹è¯•æ€§èƒ½çš„å·¥å…·
    if args.nvprof:
        assert args.num_epochs == 1, "num_epochs must be 1 during nvprof"
        if args.nvprof_iter == "first":
            args.nvprof_iter = { "start" : 0, "end" : 0 }
        elif args.nvprof_iter == "last":
            args.nvprof_iter = { "start" : args.num_iters - 1, "end" : args.num_iters - 1 }
        elif args.nvprof_iter == "all":
            args.nvprof_iter = { "start" : 0, "end" : args.num_iters - 1 } 
        else:
            raise ValueError

    """ Initialize model. """
    from utils import PrintCPUMem
    pcm = PrintCPUMem()
    pcm.print("before creating model")
    # æ ¹æ®argsåŠ¨æ€åœ°åˆ›å»ºä¸€ä¸ªæ¨¡å‹å¯¹è±¡ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å‚æ•°
    model = create_model(args)
    pcm.print("model created")

    # ç¡®ä¿æ¨¡å‹çš„æ‰€æœ‰å‚æ•°éƒ½ä¸åœ¨ GPU ä¸Š
    for vlayer, _, _ in model:
        if len(list(vlayer.parameters())) != 0:
            for param in vlayer.parameters():
                # å¦‚æœå‚æ•°åœ¨ GPU ä¸Šï¼Œæ–­è¨€å°†ä¼šè§¦å‘å¼‚å¸¸ï¼Œç»ˆæ­¢ç¨‹åºçš„æ‰§è¡Œ
                assert not param.is_cuda

    for m in model[0][0].modules():
        for key, param in m._parameters.items():
            if param is not None:
                print(f"layer0 key,{param.numel()} ,{param.numel()*4/1024/1024}, {param.shape}")
        for key, buf in m._buffers.items():
            if buf is not None:
                print(f"layer0 buffer:{key}, {buf.numel()},{buf.shape}")

    total_size=0
    for name,param in model[0][0].named_parameters():
        print(name, param.numel())
        total_size+=param.numel()
    print(f"layer0: {total_size*4/1024/1024}")

    print()
    global_param_idx = 0
    for m in model[1][0].modules():
        print(f"xxxxxx: {m}")
        for key, param in m._parameters.items():
            if param is not None:
                print(f"key:{key},{global_param_idx}, {param},{param.numel()} ,{param.numel()*4/1024/1024}, {param.shape}")
                global_param_idx+=1
                
        for key, buf in m._buffers.items():
            if buf is not None:
                print(f"buffer:{key},{global_param_idx},{buf}, {buf.numel()},{buf.shape}")
                global_param_idx+=1
                

    total_size=0
    for name,param in model[1][0].named_parameters():
        print(name, param.numel())
        total_size+=param.numel()
    print(total_size*4/1024/1024)

    print("æ‰“å°transformer layerçš„modulesçœ‹çœ‹")
    for m in model[1][0].modules():
        print(f"xxxxxx: {m}")

    print("å€’æ•°ç¬¬äºŒå±‚ï¼š")
    global_param_idx = 0
    for m in model[-3][0].modules():
        for key, param in m._parameters.items():
            if param is not None:
                print(f"å€’æ•°ç¬¬äºŒå±‚ key,{param.numel()} ,{param.numel()*4/1024/1024}, {param.shape}")
                global_param_idx+=1
                print(global_param_idx)
        for key, buf in m._buffers.items():
            if buf is not None:
                print(f"å€’æ•°ç¬¬äºŒå±‚ buffer:{key}, {buf.numel()},{buf.shape}")
                global_param_idx+=1
                print(global_param_idx)

    for name,param in model[-3][0].named_parameters():
        print(name, param.numel())

    print("æœ€åä¸€å±‚ï¼š")
    global_param_idx = 0
    for m in model[-2][0].modules():
        for key, param in m._parameters.items():
            if param is not None:
                print(f"last key,{param.numel()} ,{param.numel()*4/1024/1024}, {param.shape}")
                global_param_idx+=1
                print(global_param_idx)
        for key, buf in m._buffers.items():
            if buf is not None:
                print(f"last buffer:{key}, {buf.numel()},{buf.shape}")
                global_param_idx+=1
                print(global_param_idx)

    total_size=0
    for name,param in model[-2][0].named_parameters():
        print(name, param.numel())
        total_size+=param.numel()
    print(f"last layerå†…å­˜å ç”¨: {total_size*4/1024/1024}")

    # exit(0)
    
    # initialize empty model on CPU
    # åˆ›å»ºä¸€ä¸ªç©ºmodelï¼ŒğŸ“Œè¿™ä¸ªmodelåœ¨åé¢å°±æ˜¯GPUä¸Šçš„æ¨¡å‹ç‰ˆæœ¬
    # æ˜¾ç„¶ï¼Œåˆ é™¤äº†æ‰€æœ‰ä¸œè¥¿çš„modelè¿˜åœ¨cpuä¸Š
    from local_model_gpu import delete_param_grad_buf
    empty_model = []
    for vlayer, _, _ in model:
        with torch.no_grad():
            vlayer_copy = deepcopy(vlayer)
        # é€’å½’åœ°åˆ é™¤æ¨¡å—ï¼ˆåŒ…æ‹¬å­æ¨¡å—ï¼‰ä¸­çš„æ‰€æœ‰å‚æ•°ã€æ¢¯åº¦å’Œç¼“å†²åŒº
        delete_param_grad_buf(vlayer_copy)
        empty_model.append(vlayer_copy)
    
    # initialize shared model on CPU  
    # modelä¸­çš„æ¯ä¸€å±‚éƒ½æ”¾è¿›å…±äº«å†…å­˜äº†
    for vlayer, _1, _2 in model:
        print("vlayerçš„ç±»å‹æ˜¯", type(vlayer),", ç±»åæ˜¯:", vlayer.__class__.__name__)
        print(_1,_2)
        vlayer.share_memory() # move parameter into shared memory    
    pcm.print("shared model created")

    """ Initialize optimizer. """
    # æ ¹æ®æ¨¡å‹çš„æ¯ä¸ªå±‚çš„å‚æ•°æƒ…å†µåˆ›å»ºç›¸åº”çš„ä¼˜åŒ–å™¨ï¼Œå¹¶è¿”å›ä¸€ä¸ªè£…ç€æ‰€æœ‰å±‚çš„ä¼˜åŒ–å™¨çš„åˆ—è¡¨
    optimizer = create_optimizer(args, model)
    pcm.print("optimizer created")
    
    # initialize shared optimizer on CPU
    from shared_optim_cpu import SharedOptimCPU
    shared_model = model # model is already shared
    shared_optimizer = [] # wrapper object for optimizer
    # å°†æ‰€æœ‰å±‚çš„ä¼˜åŒ–å™¨çŠ¶æ€å…¨éƒ¨æ”¾å…¥å…±äº«å†…å­˜
    for id, ((vlayer, _, _), optim) in enumerate(zip(shared_model, optimizer)): 
        # åˆ›å»ºSharedOptimCPUçš„å®ä¾‹ï¼Œå…¶æˆå‘˜ä¸ºæ”¾è¿›å…±äº«å†…å­˜çš„vlayerå’Œå…¶å¯¹åº”çš„ä¼˜åŒ–å™¨ï¼Œä¸”è¯¥ä¼˜åŒ–å™¨çš„ä¼˜åŒ–å™¨çŠ¶æ€ä¹Ÿå…¨éƒ¨åœ¨å…±äº«å†…å­˜ä¸­ã€‚
        # é€»è¾‘ï¼šå°†å‚æ•°çš„æ¢¯åº¦åˆå§‹åŒ–ä¸ºä¸å‚æ•°æ•°æ®å½¢çŠ¶ç›¸åŒçš„é›¶å¼ é‡ï¼Œè€Œåè°ƒç”¨step()å¼ºåˆ¶åˆå§‹åŒ–ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè€ŒåæŠŠä¼˜åŒ–å™¨çŠ¶æ€å…¨éƒ¨æ”¾å…¥å…±äº«å†…å­˜
        # 1.å°†vlayerå‚æ•°çš„æ¢¯åº¦åˆå§‹åŒ–ä¸ºä¸å‚æ•°æ•°æ®å½¢çŠ¶ç›¸åŒçš„å…¨é›¶å¼ é‡
        # 2.å¼ºåˆ¶åˆå§‹åŒ–ä¼˜åŒ–å™¨çŠ¶æ€
        # 3.å°†ä¼˜åŒ–å™¨çŠ¶æ€æ”¾å…¥å…±äº«å†…å­˜ï¼ˆç”šè‡³è¿stepæ•°ä¹Ÿæ”¾è¿›å»äº†ï¼‰
        # 4.å°†vlayerå‚æ•°çš„æ¢¯åº¦ç½®ä¸ºNone
        shared_optimizer.append(SharedOptimCPU(vlayer, optim, id))
    pcm.print("shared optimizer created")

    """ Initialize distributed training. """ 
    gc.collect(); torch.cuda.empty_cache() 
    assert torch.cuda.memory_reserved() == 0, "fork process begins w/ alloc = {} B".format(torch.cuda.memory_reserved()) 
    
    processes = []
    # è‹¥å‚æ•°ä¸­è®¾ç½®äº†ç»‘å®šgpuåˆ°cpu
    # è¯¥å‚æ•°é»˜è®¤ä¸ºfalseï¼Œä¸æ‰§è¡Œ
    print(f"æ˜¯å¦ä½¿ç”¨å‚æ•° arg.numa_bind:{args.numa_bind}")
    if args.numa_bind:
        from utils import NumaBinder
        numa_binder = NumaBinder(args.numa_bind_config)
    for rank in range(CONFIGS["N"]):
        # targetä¸ºè¯¥è¿›ç¨‹è¦è¿è¡Œçš„å‡½æ•°ï¼Œargsä¸ºtargetå‡½æ•°çš„è¾“å…¥å‚æ•°
        p = mp.Process(target=worker_func, 
                        args=(args, real_dataset, shared_model, shared_optimizer, empty_model, get_lr_sched, compute_loss, save_model, prof['XMETA'], prof['TMETA'], rTASKS, CONFIGS, rank),
                        name="rank%d"%rank)
        # NOTE: this moves parameter from pinned memory to shared memory
        p.start()
        processes.append(p)
        if args.numa_bind:
            numa_binder.bind(p, rank)
        
    if args.nvprof:
        from viewer.probe_cpu import ProbeCPU
        probe_cpu = ProbeCPU(pids=[p.pid for p in processes], 
                            ranks=[rank for rank in range(CONFIGS["N"])])
        probe_cpu.run(processes[0])
        print("--- rank -1: Done ---")
        print("--- all pids = (%s) ---"% " ".join("%d"%pid for pid in list([os.getpid()]+[p.pid for p in processes])) )

    for p in processes:
        p.join()
    print("--- all workers joined successfully. ---")
